## Основные понятия

Рассмотрим подробнее понятия интеллектуального агента и проблемной среды. Они дают общее представление о принципах работы систем ИИ. Тип проблемной среды и интеллектуального агента определяет, какие методы ИИ подойдут для решения конкретной задачи.

### Интеллектуальный агент

#### Понятие агента

Теорию интеллектуальных агентов детально проработали Стюарт Рассел и Питер Норвиг в книге 1995 года "Искусственный интеллект. Современный подход" (Artificial Intelligence: A Modern Approach). Она считается стандартным учебником по ИИ для университетских программ.

Рассел и Норвиг дают следующее определение термина **агент**:

*Агентом является любой объект, который воспринимает свою окружающую среду через датчики и воздействует на эту среду через исполнительные механизмы.*

Агентом может быть человек, животное, робот или компьютерная программа. У каждого агента свои датчики и исполнительные механизмы. В этой книге мы рассмотрим только компьютерные программы в роли агентов.

В случае программы датчиками являются механизмы получения информации. Эта информация может храниться или передаваться разным оборудованием. Вот несколько примеров:

* Устройства хранения данных (например, жёсткий диск).
* Компьютерная сеть.
* Устройства ввода данных.

Кроме оборудования программа может получать информацию от операционной системы (ОС) или другой программы.

Исполнительными механизмами программы могут быть любые средства передачи информации. Программа может записывать данные на устройства хранения или передавать их по сети. Также можно выводить данные с помощью устройств вывода. Примеры таких устройств — монитор и принтер. Кроме этого программа может передавать данные ОС или другой программе через механизмы ОС (например, механизм передачи сообщений).

Для решения задачи агенту нужно какое-то время. В течении этого времени он наблюдает окружающую среду и действует в ней. Чтобы описать процесс наблюдения, нам понадобится два новых понятия: восприятие и последовательность актов восприятия.

**Восприятием** называется набор входных данных, поступающий на датчики агента в любой момент времени. **Последовательность актов восприятия** — это полный набор всех данных, которые когда-либо поучил агент.

Теперь рассмотрим действия агента. Полностью описать его поведение можно с помощью математической функции. Она отображает последовательности актов восприятия на действия агента. Эта функция называется **функцией агента**.

Функция агента представляет собой внешнее описание. Она однозначно определяет, какое действие выберет агент в ответ на любую возможную последовательности актов восприятия. При этом функция ничего не говорит о внутреннем устройстве агента.

Внутреннее устройство агента определяется его программой. **Программа агента** — это реализация функции агента конкретными методами ИИ в виде программы, работающей на физическом устройстве.

#### Понятие рациональности

До сих пор мы говорили об агентах вообще. Некоторые из них могут совершать беспорядочные действия. Другие — действовать рационально. Что означает термин рациональность?

**Рациональность** применительно к агентам означает правильное поведение. Тогда **рациональный агент** — это агент, функция которого определяет правильное действие для каждой возможной последовательности актов восприятия. Другими словами, в любой возможной ситуации рациональный агент поступает правильно.

Теперь возникает вопрос: что такое правильное действие и как его отличить от неправильного? Чтобы агент смог оценить свои действия в процессе обучения и работы, его разработчик задаёт **показатели производительности**. Это критерии для объективной оценки успешности действий агента. В случае агента для ведения стратегических игр показателем производительности будет его выигрыш.

Рассмотрим пример рационального агента для игры в шахматы. Его показателем производительности может быть количество взятых фигур оппонента. Тогда агент будет выбирать агрессивную стратегию и размениваться с противником при каждой возможности. Такие действия нельзя назвать хорошей игрой. Поэтому вместо количества взятых фигур, показателем производительности должно быть количество выигранных партий.

Общая рекомендация по выбору показателей производительности звучит так:

*Выбирать показатели производительности из расчёта того, чего надо достичь в данной среде. Не надо предполагать, как должен вести себя агент, чтобы это достичь.*

Все рассмотренные нами понятия позволяют дать полное определение рациональному агенту. Рассел и Норвиг формулируют это определение так:

*Для каждой возможной последовательности актов восприятия рациональный агент должен выбирать действие, которое максимизирует его показатель производительности. При выборе действия учитываются факты из последовательности актов восприятия и любые знания, которыми обладает агент.*

### Проблемная среда

Рациональные агенты нужны для решения какой-то конкретной задачи. Чтобы описать эту задачу, введём понятие **проблемной средой** (task environment).

Проблемная среда включает в себя следующие факторы:

* Датчики агента.
* Исполнительные механизмы агента.
* Показатели производительности.
* Среда.

Таким образом проблемная среда полностью описывает задачу, решаемую агентом.

Вариантов проблемных сред очень много. Однако, программа агентов для похожих сред будет работать по одинаковому принципу. Все возможные среды удобно классифицировать по некоторым признаки. Тогда намного проще будет определять требования к агентам, которые должны в них работать.

Таблица 2-5 демонстрирует признаки классификации проблемных сред.

{caption: "Таблица 2-5. Классификация проблемных сред", width: "100%"}
| Признак | Тип среды |
| --- | --- |
| Знание состояния | **Полностью наблюдаемая** |
|  | **Частично наблюдаемая** |
|  | |
| Элемент случайности | **Детерминированная** |
|  | **Стохастическая** |
|  | **Стратегическая** |
|  | |
| Акты восприятия агента | **Эпизодическая** |
|  | **Последовательная** |
|  | |
| Характер изменений | **Статическая** |
|  | **Динамическая** |
|  | **Полудинамическая** |
|  | |
| Количество различимых состояний | **Дискретная** |
|  | **Непрерывная** |
|  | |
| Количество агентов | **Одноагентная** |
|  | **Мультиагентная** |
|  | |
| Знание правил | **Известная** |
|  | **Неизвестная** |
|  | |
| Вычислительная сложность | **Простая** |
|  | **Сложная** |

Сравните классификацию проблемных сред и стратегических игр из таблицы 1-1. Вы найдёте в них совпадения.

Рассмотрим подробнее каждый тип предметной среды.

#### Знание состояния среды

Датчики агента могут давать полную информацию о состоянии среды в каждый момент времени. В таком случае проблемная среда называется **полностью наблюдаемой**. Пример такой среды — шахматная доска. Агент может наблюдать расположение всех фигур.

Игра с совершенной информацией является полностью наблюдаемой средой.

В **частично наблюдаемой** среде агент не может получить полную информацию о происходящем через свои датчики. Любая игра с несовершенной информацией является частично наблюдаемой средой. Например, в карточной игре агент знает только карты у себя на руках. Карты других игроков ему неизвестны.

#### Элемент случайности

Проблемные среды отличаются по характеру происходящих в них изменений. Если следующее состояние среды однозначно определяется её текущим состоянием и действием агента, она называется **детерминированной**.

Если на следующее состояние среды влияет какое-то случайное событие, она называется **стохастической**. В терминологии теории игр в стохастической среде присутствует внешняя неопределённость. Поэтому игра с несовершенной информацией является стохастической средой.

#### Акты восприятия агента

Датчики агента могут воспринимать окружающую среду двумя способами. В первом случае агент получает состояния среды одно за другим. Каждое следующее состояния не зависит от предыдущего и совершённых раньше действий агента. В этом случае среда называется **эпизодической**.

Пример эпизодической среды — набор фотографий для распознавания образов. Каждая следующая фотография выбирается случайно. Этот выбор не зависит от успешности распознавания прошлых фотографий.

Второй вариант восприятия среды — непрерывное наблюдение за её изменениями. В этом случае каждое действие агента влияет на все последующие состояния среды. В этом случае говорят о **последовательной** среде. Пример — игра в шахматы.

#### Характер изменений среды

Среда может вести себя по-разному в тот момент, когда агент выбирает очередное действие. Если среда меняется и агент вынужден постоянно за ней наблюдать, она называется **динамической**. Это значит, что пока агент вычисляет своё следующее действие, ситуация может измениться. Тогда от него потребуется другое действие, чем для прошлого состояния среды.

Некоторые игры с параллельными ходами представляют собой динамическую среду. Например, игра в футбол. В ней участники обоих команд постоянно перемещаются. Следовательно, среда постоянно меняется.

**Статическая** среда в отличие от динамической меняется только после действия агента. Поэтому агент может получить информацию о текущем состоянии среды и больше его не проверять до выполнения следующего действия.

Игры с последовательным порядком ходов являются статическими средами. Пример — шахматы.

Может быть так, что за время выбора агентом следующего действия меняется не среда, а его показатели производительности. В этом случае среда называется **полудинамической**.

#### Количество различимых состояний среды

Среды могу отличаться тем, как в них различаются следующие факторы:

* Состояние среды.
* Время.
* Восприятие агента.
* Действия агента.

Если любой из этих факторов меняется непрерывно, среда называется **непрерывной**.

Например, в футболе время течёт без остановок. Кроме того, расположение каждого игрока на поле представляет собой некоторое непрерывное значение. Эти значения меняются во времени плавно. Поэтому игра в футбол будет непрерывной средой.

Если у каждого из четырёх факторов есть конечное число состояний, говорят что среда **дискретная**.

Пример дискретной среды — игра в шахматы. В ней число возможных состояний доски огромно, но конечно. Время для каждого игрока останавливается после совершения им хода. Состояние доски меняется только после очередного хода. Сами игроки действуют по очереди.

#### Количество агентов

Если в среде действует только один агент, она называется **одноагентной**. Если агентов большего одного — это **мультиагентная** среда.

Некоторые стохастические одноагентные среды похожи на мультиагентные. Например, в одноагентной среде есть объект, который выполняет случайные действия. Возникает вопрос: следует ли рассматривать этот объект в роли другого агента?

Чтобы отличить объект от другого объекта, надо проанализировать его поведение. Объект считается другим агентом если:

* У него есть показатели производительности.
* Эти показатели зависят от действий нашего агента.
* Объект стремится максимизировать свои показатели производительности.

В **конкурентной** мультиагентной среде показатели производительности агентов обратно пропорциональны. Это значит, что увеличение показателей одного агента уменьшает показатели другого. Любая антагонистическая игра с нулевой суммой является конкурентной средой.

В **кооперативной** мультиагентной среде показатели производительности всех агентов растут и уменьшаются одновременно. Некоторые из игр с ненулевой суммой могут быть кооперативными мультиагентными средами. Пример такой игры — [ультиматум](https://ru.wikipedia.org/wiki/Ультиматум_(игра)). В ней два участника должны договориться о разделении суммы денег. Если им не удалось договориться, никто ничего не получает.

#### Знание правил среды

Перед тем, как начать действовать, агент может знать последствие любого своего действия. В этом случае говорят, что среда **известна** агенту.

Если агент не знает к чему приведёт любое из его возможных действий, то среда называется **неизвестной**.

Вот пример. Рассмотрим агента для игры в шахматы. Если он знает правила игры, ценность и ходы фигур, то среда для него будет известной. Если агент не знает фигуры или правил игры, ему сначала придётся их изучить. В этом случае среда для него будет неизвестной.

Знание правил среды не связано с информацией о её состоянии. В примере с шахматами среда полностью наблюдаема, даже когда агент не знает правил игры. Противоположная ситуация — карточная игра. Агент может знать её правила, но среда частично наблюдаема. Причина в том, что агент не знает карты на руках остальных игроков.

#### Вычислительная сложность

Две дискретные среды могут отличаться количеством возможных состояний. Если все состояния среды можно в принципе обработать с помощью современного компьютера, будем называть эту среду **простой**. Если для обработки всех состояний среды недостаточно памяти или производительности современного компьютера, будем называть такую среду **сложной**.

Простые и сложные среды отличаются не только количеством состояний. У агента в простой среде относительно мало вариантов возможных действий.

Даже если у агента в простой среде много вариантов действий, большинство из них приведут к одинаковым состояниям среды. Тогда с точки зрения агента действия с одинаковым результатом будут одинаковы. Нет никакого смысла их различать.

В сложной среде у агента много вариантов действий. Это связано с многочисленными состояниями среды. Когда таких состояний много, есть много способов повлиять на среду. Каждое из этих воздействий приведёт к разному результату. Следовательно, агент должен различать эти воздействия.

### Классификация интеллектуальных агентов

Функцию агента можно представить в виде таблицы. Эта таблица задаёт соответствия между каждой возможной последовательностью актов восприятия и действием агента.

Для примера в роли агента рассмотрим компьютерную программу. Она следит за местом на жёстком диске. Когда место заканчивается, программа очищает каталог с временными файлами.

Функция нашего агента определяет, когда надо очищать каталог с временными файлами. Представим эту функцию в виде таблицы 2-6.

{caption: "Таблица 2-6. Функция программы очистки диска", width: "100%"}
| Последовательность актов восприятия | Действие |
| --- | --- |
| Место есть | Ждать |
|  | |
| Места нет | Очистить каталог |
|  | |
| Место есть, Место есть | Ждать |
|  | |
| Место есть, Места нет | Очистить каталог |
|  | |
| Место есть, Место есть, Место есть | Ждать |
|  | |
| Место есть, Место есть, Места нет | Очистить каталог |
|  | |
| ... | ... |

Размер этой таблицы зависит от времени работы программы. Число строк равно числу актов восприятия.

Для примера предположим, что программа проверяет диск каждую секунду. Тогда чтобы программа проработала один день, ей нужна таблица с `60 * 60 * 24 = 86400` строкам.

Рассчитаем, сколько нужно памяти компьютера для хранения таблицы нашей программы. Для кодирования одного состояния среды достаточно одного бита. Если бит равен 0, место на диске есть. В противном случае — места нет. Для кодирования действия программы тоже достаточно одного бита:

* 0 — ждать.
* 1 — очистить каталог.

Чтобы сохранить только столбец с последовательностью актов восприятия, нам нужно `(86400 * 86400 / 2) + 86400 / 2 = 3732523200` бит. Добавим к этому 86400 бит, чтобы сохранить по одному действию для каждой строки таблицы. Получим 3732609600 бит или 466576200 байт. Это примерно равно 466 мегабайтам.

Итак, чтобы сохранить функцию элементарного агента в виде таблицы, нам нужно 466 мегабайт памяти. Мы могли бы написать программу с такой таблицей. Эта программа находит строку таблицы, которая соответствует текущей последовательности актов восприятия. Затем она совершает действие из второго столбца.

Решение задачи с помощью таблицы далеко не оптимально. Представьте, что наш агент должен проработать не один день, а неделю. Тогда размер его программы вырастет до `466 * 7 = 3262` мегабайт.

Задача ИИ заключается в том, чтобы перевести функцию агента из формы таблицы в более компактный формат.

Рассел и Норвиг классифицируют агентов по их внутреннему устройству. Таблица 2-7 демонстрирует эту классификацию. Для каждого класса агентов приводится наиболее сложный тип среды, в которой он может работать.

{caption: "Таблица 2-7. Классификация интеллектуальных агентов", width: "100%"}
| Класс | Механизм работы | Тип проблемной среды |
| --- | --- | --- |
| Простой рефлексный агент | Таблица правил | Полностью наблюдаемая |
|  | | Эпизодическая |
|  | | Статическая |
|  | | Дискретная |
|  | | Одноагентная |
|  | | Известная |
|  | | Простая |
|  | | |
| Рефлексный агент с внутренним состоянием | Таблица правил | Частично наблюдаемая |
|  | | Последовательная |
|  | | Статическая |
|  | | Дискретная |
|  | | Одноагентная |
|  | | Известная |
|  | | Простая |
|  | | |
| Рефлексный агент, основанный на модели | Таблица правил | Частично наблюдаемая |
|  | | Последовательная |
|  | | Динамическая |
|  | | Дискретная |
|  | | Мультиагентная |
|  | | Известная |
|  | | Простая |
|  | | |
| Агент, основанный на цели | Поиск и планирование | Полностью наблюдаемая |
|  | | Последовательная |
|  | | Статическая |
|  | | Дискретная |
|  | | Мультиагентная |
|  | | Известная |
|  | | Простая |
|  | | |
| Агент, основанный на полезности | Поиск и планирование | Частично наблюдаемая |
|  | | Последовательная |
|  | | Динамическая |
|  | | Дискретная |
|  | | Мультиагентная |
|  | | Известная |
|  | | Сложная |
|  | | |
| Обучающийся агент | Машинное обучение | Частично наблюдаемая |
|  | | Последовательная |
|  | | Динамическая |
|  | | Непрерывная |
|  | | Мультиагентная |
|  | | Неизвестная |
|  | | Сложная |

#### Простой рефлексный агент

Самый примитивный вид агента называется **простым рефлексным**. Такой агент выбирает своё действие на основе текущего состояния среды. При этом он игнорирует всю предыдущую последовательность актов восприятия и все свои прошлые действия.

Предположим, что наша программа очистки диска — это простой рефлексный агент. Тогда её функцию можно представить в виде таблицы правил 2-8.

{caption: "Таблица 2-8. Таблица правил для программы очистки диска", width: "100%"}
| Текущее состояние среды | Действие |
| --- | --- |
| Место есть | Ждать |
|  | |
| Места нет | Очистить каталог |

Теперь чтобы наш агент проработал один день, ему не нужна таблица размером 466 мегабайт. Вместо этого ему достаточно таблицы 2-7. Она занимает в памяти компьютера 4 бита. Размер этой таблицы не зависит от времени работы агента.

Таблицу правил 2-7 можно записать в виде программы на [**псевдокоде**](https://ru.wikipedia.org/wiki/Псевдокод_(язык_описания_алгоритмов)). Листинга 2-1 приводит эту программу.

{caption: "Листинг 2-1. Псевдокод программы очистки диска", line-numbers: true}
```
if места_нет then
    очистить_каталог
else
    ждать
```

Простые рефлексные агенты легко реализовать. Они работают максимально быстро и занимают минимум памяти. Но область их применения ограничена. Такие агенты работаю только в среде со следующими свойствами:

* Полностью наблюдаемая
* Эпизодическая
* Статическая
* Одноагентная
* Известная
* Простая

Если среда частично наблюдаемая, рефлексный агент будет совершать ошибки. Причина в том, что агент не может получить полную информацию о состоянии такой среды через свои датчики. Недостаток информации приводит к тому, что два разных состояния среды выглядят для агента одинаково. В этом случае он может выбрать "неправильное" состояние и соответствующее ему действие. Другими словами, агент путает строки в своей таблице правил.

Простой рефлексный агент не может определить состояние последовательной среды. Причина в том, что он не следит за её прошлыми состояниями и своими предыдущими действиями. Однако, именно они определяют текущее состояние среды. Результат будет тем же, что и для частично наблюдаемой среды: агент неправильно выбирает свои действия.

Рефлексный агент не может работать в динамической среде. Проблема в том, что он не прогнозирует изменения среды в ближайшем будущем. Однако, пока агент выполняет своё действие, среда меняется. В результате на момент совершения этого действия состояние среды может требовать совсем другого поведения.

В мультиагентной среде у рефлексного агента возникнут проблемы. Он может наблюдать только текущее состояние среды. У него нет возможности просчитать будущие действия других агентов и их последствия. Однако, эти действия влияют на показатели производительности нашего агента. Следовательно, он должен выбирать свои действия с учётом поведения других агентов, но не может этого сделать.

Предположим, что среда неизвестна рефлексному агенту. Это означает, что таблица его правил либо пуста, либо заполнена не полностью. У агента нет возможности самостоятельно заполнить свою таблицу правил, т.е. обучиться.

Рефлексный агент не может работать в сложной среде. Эти среды отличаются огромным количеством возможных состояний. Кроме того у агента много вариантов действий в такой среде. Из-за этого таблица правил агента будет слишком большой и не поместится в память любого современного компьютера.

#### Рефлексный агент с внутренним состоянием

Следующий по сложности тип агента — это **рефлексный агент с внутренним состоянием**. В его программе решена проблема работы в следующих средах:

* Частично наблюдаемые
* Последовательные

Агент такого типа хранит **внутреннее состояние**. Оно определяется предыдущими актами восприятия и действиями агента.

С помощью внутреннего состояния агент может отслеживать изменения в последовательной среде. Благодаря этому, он точно знает, какой строке из таблицы правил соответствует текущее состояние среды. Поэтому агент выбирает правильное действие.

Агент с внутренним состоянием может работать в частично наблюдаемых средах. Однако, его действия не всегда будут правильными.

Рассмотрим пример. Предположим,что датчики агента получили информацию о части среды, которая впоследствии оказалась скрыта. Агент запоминает эту информацию в своём внутреннем состоянии. Теперь он получает полное состояние среды из своего текущего восприятия и внутреннего состояния. Восприятие даст информацию о видимой части среды, а внутреннее состояние — о скрытой. В этом случае агент выбирает правильное действие из таблицы правил.

Возможны ситуации, когда скрытая часть среды остаётся недоступна агенту. Например, он не наблюдал её ни в одном из предыдущих актов восприятия. Именно это происходит в карточных играх, когда игроки не знают карт друг друга.

Другая ситуация — агент получил информацию о скрытой части среды, но состояние этой части изменилось. Например, агент видел карты другого игрока. Но на следующий ход игрок вытянул другие карты.

В общем случае агент с внутренним состоянием предполагает состояние частично наблюдаемой среды. Исходя из этого предположения, он выбирает своё действие. Если предположение оказалось ложным, действие агента будет неподходящим.

#### Рефлексный агент, основанный на модели

Чтобы действовать правильно в динамической среде, программа агента должна иметь **модель среды**. Эта модель описывает следующие правила:

1. Как изменяется среда с течением времени независимо от действий агента?

2. Как любое возможное действие агента изменяет среду?

**Рефлексный агент, основанный на модели**, использует модель среды для прогнозирования её изменений. В этом случае он может работать в динамической среде. Для выбора действия агент учитывает текущее восприятие и прогноз о ближайших изменениях в среде. Так он выбирает верную строку в таблице правил и соответствующее ей действие.

Модель позволяет агенту правильно действовать и в некоторых мультиагентных средах. Предположим, что среда простая и у каждого агента мало вариантов действий. Тогда модель даёт прогноз о всех изменениях среды в ближайшем будущем. Учитывая эту информацию, основанный на модели агент выберет правильное действие.

Если мультиагентная среда сложная, просчитать её возможные состояние нельзя из-за комбинаторного взрыва. Чтобы ограничить пространство поиска состояний среды, нужны эвристики. Но у основанного на модели агента эвристик нет. Поэтому в такой среде он работать не сможет.

#### Агент, основанный на цели

Все рефлексные агенты работают по таблице правил. Эта таблица может представлять собой код с условными операторами или структуру данных в памяти компьютера. **Агенты, основанные на цели** используют принципиально другой способ принятия решений.

Агент, знающий свою цель, способен анализировать результаты своих действий. Если какая-то последовательность действий приводит к поставленной цели, агенту следует её выполнить. Таким образом решение задачи сводится к **поиску**.

Для перебора всех возможных действий агента и их результатов используются **алгоритмы поиска**. Обычно действия и их результаты представляются в виде дерева или графа. Каждый узел дерева — это состояние среды. К этому состоянию приводят действия агента, которые соответствуют входящим в этот узел ветвям. Исходящие из узла ветви — это возможные действия агента в данном состоянии среды.

Алгоритм поиска обходит дерево состояний среды и действий агента. Известная агенту цель соответствует определённому состоянию среды. Когда алгоритм поиска находит узел с таким состоянием, задача считается решённой. Агенту остаётся только выполнить последовательность действий, которая ведёт из текущего узла дерева к найденному.

Какие преимущества даёт агенту замена таблицы правил на алгоритм поиска? Прежде всего дерево состояний среды и действий агента — это более компактный формат хранения данных, чем таблица правил. Благодаря компактному формату, агент может работать в более сложных средах.

Ограничением сложности среды для агента, основанного на цели, является память компьютера. До тех пор, пока дерево поиска целиком помещается в память, компьютер сможет его обработать. В этом случае целевое состояние среды будет найдено.

Если дерево поиска не поместилось в память, агент не сможет найти целевое состояние. У агента, основанного на цели, нет возможности делить и анализировать дерево поиска по частям.

Использование алгоритма поиска даёт также преимущество при работе в мультиагентных средах. Дерево поиска включает действия всех агентов. Благодаря этому, легко учесть их влияние и рассчитать лучшую последовательность действий для агента, основанного на цели.

#### Агент, основанный на полезности

Главная проблема агента, основанного на цели, заключается в работе со сложными средами. **Агенты, основанные на полезности**, решают эту проблему. Такой агент может рассматривать текущее и будущие состояния среды с точки зрения их желательности. Для такой оценки используется понятие **полезности**.

Полезность означает предпочтение агентом тех или иных состояний среды. Часто чем выше полезность, тем выше показатели производительности агента.

Количественную оценку полезности даёт **функция полезности**. Эта функция отображает состояния среды (или последовательность её состояний) на вещественные числа. Эти числа означают степень удовлетворённости агента от состояния среды. Для правильно спроектированного агента большее значение функции полезности соответствует лучшим показателям производительности агента.

Что даёт агенту функция полезности? С помощью этой функции агент способен оценить последовательность своих действий даже, если она не приводит к решению задачи. В этом случае у агента появляется возможность делить и анализировать дерево поиска по частям. Таким образом агент способен работать в сложных средах, дерево поиска по которым не помещается в память компьютера.

Может возникнуть вопрос: чем отличаются показатели производительности от функции полезности? Эти понятия означают разные вещи.

Показатели производительности — это внешняя оценка поведения агента его разработчиком. Эти показатели нужны во время создания, тестирования и обучении агента. Также они позволяют сравнить агентов c разными программами. Показатели производительности определяют, делает ли агент то, что должен.

Функция полезности — это внутрення оценка агентом своих действий. При принятии решений агент может ориентироваться на функцию полезности, но не на показатели производительности. Функция полезности определяет насколько желательны для агента те или иные состояния среды.

Агент, основанный на полезности, успешно действует в любых частично наблюдаемых средах. Даже если часть среды постоянно остаётся скрытой от агента, он выберет наилучшее возможное действие в текущей ситуации. В отличие от рефлексного агента, он не путает состояния среды. Это предотвращает заведомо неправильные действия.

Функция полезности позволяет агенту действовать в сложных мультиагентных средах. Она служит эвристикой при расчёте возможных действий других агентов и будущих состояний среды. Благодаря эвристике, удаётся сократить пространство поиска и предотвратить комбинаторный взрыв.

#### Обучающийся агент

Предположим, что агент выбирает свои действия на основе таблицы правил или поиска. В этом случае он не сможет действовать в неизвестной среде. Перед началом работы, агент должен узнать о правилах изменения среды. Другими словами, он должен заполнить свою таблицу правил или определить пространство поиска. Такой процесс называется **обучением**. Способный к обучению агент называется **обучающимся**.

Кроме работы в неизвестной среде обучение решает ещё одну задачу. Используя обучение, агент улучшает свои показатели производительности по сравнению с изначальными. Рассмотрим пример.

Разработчик построил агента и задал ему таблицу правил для решения задачи в какой-то среде. Эти правила основаны на личном опыте разработчика и его представлении о задаче. При этом существуют правила, которые дают лучшие показатели производительности агента. Если агент обучаемый, он самостоятельно найдёт эти лучшие правила.

Если среда сложная с множеством состояний и возможных действий, составить таблицу правил не получится из-за её огромного размера. В этом случае обучающийся агент может самостоятельно изучить правила среды. Так агент приходит к решению поставленной задачи, даже если его разработчик не знает способа её решить.

Программа обучающегося агента принципиально отличается от программ агентов других типов. Кроме механизма для выбора действия на основе восприятия, она имеет **механизм обучения**.

Существует несколько подходов к реализации механизма обучения. Эти подходы разрабатываются в рамках направления ИИ под названием [**машинное обучение**](https://en.wikipedia.org/wiki/Machine_learning).

Механизм обучения состоит из двух компонентов:

* **Алгоритм обучения**
* **Обучаемая модель**

Сегодня наиболее популярны следующие алгоритмы обучения:

* [**Обучение с учителем**](https://ru.wikipedia.org/wiki/Обучение_с_учителем) (supervised learning). Агент наблюдает за примерами входных и выходных данных ожидаемой функции агента. Входные данные представляют собой последовательность актов восприятия, а выходные — действие. По этим примерам агент строит свою программу.

* [**Обучение без учителя**](https://ru.wikipedia.org/wiki/Обучение_без_учителя) (unsupervised learning). Агент обучается только на входных данных функции агента. Он не получает примеры ожидаемых выходных данных. Другими словами агент решает сам, какое действие соответствует каждой последовательности актов восприятия.

* [**Обучение с подкреплением**](https://ru.wikipedia.org/wiki/Обучение_с_подкреплением) (reinforcement learning). Агент взаимодействует со средой или её моделью. Успешные действия агента поощряются, а неуспешные наказываются. Система поощрений является частью среды или её модели. Агент строит свою программу так, чтобы выполнять успешные действия.

Чаще всего применяются следующие обучаемые модели:

* **Нейронная сеть** (artificial neural network) состоит их искусственных нейронов. Нейроны соединяются направленными связями. Через эти связи по сети распространяются сигналы. Каждая связь имеет числовой вес, который определяет силу и знак связи. Веса входных связей нейрона определяют, какие сигналы приведут к его активации. Нейрон после активации посылает выходной сигнал. В процессе обучения агент подбирает правильные числовые веса для всех связей в сети.

* [**Дерево решений**](https://ru.wikipedia.org/wiki/Обучение_дерева_решений) (decision tree learning). В результате обучения агент строит дерево решений. Построенное дерево реализует функцию агента: получает на вход последовательность актов восприятия и на их основе выбирает подходящее действие. Чтобы выбрать действие, над входными данными выполняется серия проверок. Каждый узел дерева соответствует условию проверки, а исходящие из него ветви — результатам проверки.

* [**Машина поддерживающих векторов**](https://ru.wikipedia.org/wiki/Метод_опорных_векторов) (support vector machine или SVM) представляет собой модель и набор алгоритмов для работы с ней. Машина обучается с учителем на примерах. Для каждого примера указано, к какому из двух классов он относится. После обучения машина определяет класс полученного на вход примера. SVM решает задачи **линейной** и **нелинейной классификации**.

I> [Линейная классификация](https://ru.wikipedia.org/wiki/Линейный_классификатор) сводится к разделению множества точек в пространстве признаков одной гиперплоскостью. **Гиперплоскость** имеет на одну размерность меньше, чем объемлющее её пространство. Например, гиперплоскость для двумерного пространства — это прямая.

* [**Байесовская сеть**](https://ru.wikipedia.org/wiki/Байесовская_сеть) (Bayesian network) представляет собой направленный граф. Его вершины — это переменные произвольного типа (признаки, теории и т.д.). Граф отображает [условные зависимости](https://en.wikipedia.org/wiki/Conditional_dependence) между переменными. Пример использования сети: вычисление вероятности той или иной болезни у пациента по её симптомам.

* [**Генетический алгоритм**](https://ru.wikipedia.org/wiki/Генетический_алгоритм) (genetic algorithm) — это модель, которая имитирует процесс естественного отбора. В процессе обучения такой модели применяются методы мутации, скрещивания и отбора. Эти методы производят новые версии модели, которые обладают нужными характеристиками.

Таблица 2-9 демонстрирует какие алгоритмы обучения применяются для разных моделей. Кроме того в таблице указан класс модели: deep learning или shallow learning.

{caption: "Таблица 2-9. Соответствие алгоритмов обучения и моделей", width: "100%"}
| Модель | Класс модели | Алгоритм обучения |
| --- | --- | --- |
| Однослойная нейронная сеть | Shallow learning | Обучение с учителем |
|  | | |
| Глубокая нейронная сеть (DNN) | Deep learning | Обучение с учителем |
|  | | Обучение без учителя |
|  | | Обучение с подкреплением |
|  | | |
| Глубокая сеть доверия (DBN) | Deep learning | Обучение с учителем |
|  | | Обучение без учителя |
|  | | |
| Рекуррентная нейронная сеть (RNN) | Deep learning | Обучение с учителем |
|  | | Обучение без учителя |
|  | | Обучение с подкреплением |
|  | | |
| Свёрточная нейронная сеть (CNN) | Deep learning | Обучение с учителем |
|  | | |
| Дерево решений | Shallow learning | Обучение с учителем |
|  | | |
| Машина поддерживающих векторов | Shallow learning | Обучение с учителем |
|  | | |
| Байесовская сеть | Shallow learning | Обучение с учителем |
|  | | |
| Генетический алгоритм | Shallow learning | Обучение с учителем |

Используя современные технологии машинного обучения, агент может справиться со средами самой высокой сложности. Именно поэтому обучающиеся системы ИИ считаются сегодня наиболее перспективными.