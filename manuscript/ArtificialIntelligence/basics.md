## Основные понятия

Рассмотрим подробнее понятия интеллектуального агента и проблемной среды. Они дают общее представление о принципах работы систем ИИ. Тип проблемной среды и интеллектуального агента определяет, какие методы ИИ подойдут для решения конкретной задачи.

### Интеллектуальный агент

#### Понятие агента

Теорию интеллектуальный агентов детально проработали Стюарт Рассел и Питер Норвиг в книге 1995 года "Искусственный интеллект. Современный подход" (Artificial Intelligence: A Modern Approach). Она считается стандартным учебником по ИИ для университетских программ.

Рассел и Норвиг дают следующее определение термина "агент":

*Агентом является любой объект, который воспринимает свою окружающую среду через датчики и воздействует на эту среду через исполнительные механизмы.*

Агентом может быть человек, животное, робот или компьютерная программа. У каждого агента свои "датчики" и "исполнительные механизмы". В этой книге мы рассмотрим только компьютерные программы в роли агентов.

В случае программы "датчиками" являются механизмы получения информации. Эта информация может храниться или передаваться разным оборудованием. Вот несколько примеров:

* Устройства хранения данных (например, жёсткий диск).
* Компьютерная сеть.
* Устройства ввода данных.

Кроме оборудования программа может получать информацию от операционной системы (ОС) или другой программы.

"Исполнительными механизмами" программы могут быть любые средства передачи информации. Программа может записывать данные на устройства хранения или передавать их по сети. Также можно выводить данные с помощью устройств вывода. Примеры таких устройств — монитор и принтер. Кроме этого программа может передавать данные ОС или другой программе через механизмы ОС (например, механизм передачи сообщений).

Для решения задачи агенту нужно какое-то время. В течении этого времени он наблюдает окружающую среду и действует в ней. Чтобы описать процесс наблюдения, нам понадобится два новых понятия: восприятие и последовательность актов восприятия.

**Восприятием** называется набор входных данных, поступающий на "датчики" агента в любой момент времени. **Последовательность актов восприятия** — это полный набор всех данных, которые когда-либо поучил агент.

Теперь рассмотрим действия агента. Полностью описать его поведение можно с помощью математической функции. Она однозначно определяет, какое действие выберет агент в ответ на любую возможную последовательности актов восприятия. Эта функция называется
**функцией агента**.

Функция агента представляет собой внешнее описание. Она говорит о поведении агента, но не о его внутреннем устройстве.

Внутреннее устройство агента определяется его программой. **Программа агента** — это реализация функции агента конкретными методами ИИ в виде программы, работающей на физическом устройстве.

#### Понятие рациональности

До сих пор мы говорили об агентах вообще. Некоторые из них могут совершать беспорядочные действия. Другие — действовать рационально. Что означает термин рациональность?

**Рациональность** применительно к агентам означает правильное поведение. Тогда **рациональный агент** — это агент, функция которого определяет правильное действие для каждой возможной последовательности актов восприятия. Другими словами, в любой возможной ситуации рациональный агент поступает правильно.

Теперь возникает вопрос: что такое "правильное действие" и как его отличить от неправильного? Чтобы агент смог оценить свои действия в процессе обучения и работы, его разработчик задаёт **показатели производительности**. Эти показатели объективно оценивают успешность действий агента. В случае агента для ведения стратегических игр показателем производительности будет его выигрыш.

Рассмотрим пример рационального агента для игры в шахматы. Его показателем производительности может быть количество срубленных фигур. Тогда агент будет выбирать агрессивную стратегию и размениваться с противником при каждой возможности. Такие действия нельзя назвать хорошей игрой. Поэтому вместо количества срубленных фигур, показателем производительности должно быть количество выигранных партий.

Общая рекомендация по выбору показателей производительности звучит так:

*Выбирать показатели производительности из расчёта того, чего надо достичь в данной среде. Не надо предполагать, как должен вести себя агент, чтобы это достичь.*

Все рассмотренные нами понятия позволяют дать полное определение рациональному агенту. Рассел и Норвиг формулируют это определение так:

*Для каждой возможной последовательности актов восприятия рациональный агент должен выбирать действие, которое максимизирует его показатель производительности. При выборе действия учитываются факты из последовательности актов восприятия и любые знания, которыми обладает агент.*

### Проблемная среда

Рациональные агенты нужны для решения какой-то конкретной задачи. Чтобы описать эту задачу, введём понятие **проблемной средой** (task environment).

Проблемная среда включает в себя следующие факторы:

* Датчики агента.
* Исполнительные механизмы агента.
* Показатели производительности.
* Среда.

Таким образом проблемная среда полностью описывает задачу, решаемую агентом.

Вариантов проблемных сред очень много. Однако, программа агентов для похожих сред будет работать по одинаковому принципу. Все возможные среды удобно классифицировать по некоторым признаки. Тогда намного проще будет определять требования к агентам, которые должны в них работать.

Таблица 2-5 демонстрирует признаки классификации проблемных сред.

{caption: "Таблица 2-5. Классификация проблемных сред", width: "100%"}
| Признак | Тип среды |
| --- | --- |
| Знание состояния | **Полностью наблюдаемая** |
|  | **Частично наблюдаемая** |
|  | |
| Элемент случайности | **Детерминированная** |
|  | **Стохастическая** |
|  | **Стратегическая** |
|  | |
| Акты восприятия | **Эпизодическая** |
|  | **Последовательная** |
|  | |
| Характер изменений | **Статическая** |
|  | **Динамическая** |
|  | **Полудинамическая** |
|  | |
| Количество различимых состояний | **Дискретная** |
|  | **Непрерывная** |
|  | |
| Количество агентов | **Одноагентная** |
|  | **Мультиагентная** |
|  | |
| Знание правил | **Известная** |
|  | **Неизвестная** |

Сравните классификацию проблемных сред и стратегических игр из таблицы 1-1. Вы найдёте в них совпадения.

Рассмотрим подробнее каждый тип предметной среды.

#### Знание состояния среды

Датчики агента могут давать полную информацию о состоянии среды в каждый момент времени. В таком случае проблемная среда называется **полностью наблюдаемой**. Пример такой среды — шахматная доска. Агент может наблюдать расположение всех фигур.

Игра с совершенной информацией является полностью наблюдаемой средой.

В **частично наблюдаемой** среде агент не может получить полную информацию о происходящем через свои датчики. Любая игра с несовершенной информацией является частично наблюдаемой средой. Например, в карточной игре агент знает только карты у себя на руках. Карты других игроков ему неизвестны.

#### Элемент случайности

Проблемные среды отличаются по характеру происходящих в них изменений. Если следующее состояние среды однозначно определяется её текущим состоянием и действием агента, она называется **детерминированной**.

Если на следующее состояние среды влияет какое-то случайное событие, она называется **стохастической**. В терминологии теории игр в стохастической среде присутствует внешняя неопределённость. Поэтому игра с несовершенной информацией является стохастической средой.

Допустим, что проблемная среда детерминированная и в ней действуют несколько агентов. Тогда следующее состояние среды зависит от действий всех агентов. В этом случае её называют **стратегической**.

#### Акты восприятия агента

Датчики агента могут воспринимать окружающую среду двумя способами. В первом случае агент получает состояния среды одно за другим. Каждое следующее состояния не зависит от предыдущего и совершённых раньше действий агента. В этом случае среда называется **эпизодической**.

Пример эпизодической среды — набор фотографий для распознавания образов. Каждая следующая фотография выбирается случайно. Этот выбор не зависит от успешности распознавания прошлых фотографий.

Второй вариант восприятия среды — непрерывное наблюдение за её изменениями. В этом случае текущее состояние среды определяется её прошлым состоянием и совершёнными ранее действиями агента. Такая среда называется **последовательной**. Пример — игра в шахматы.

#### Характер изменений среды

Среда может вести себя по-разному в тот момент, когда агент выбирает очередное действие. Если среда меняется и агент вынужден постоянно за ней наблюдать, она называется **динамической**. Это значит, что пока агент вычисляет своё следующее действие, ситуация может измениться. Тогда от него потребуется другое действие, чем для прошлого состояния среды.

Некоторые игры с параллельными ходами представляют собой динамическую среду. Например, игра в футбол. В ней участники обоих команд постоянно перемещаются. Следовательно, среда постоянно меняется.

**Статическая** среда в отличие от динамической меняется только после действия агента. Поэтому агент может получить информацию о текущем состоянии среды и больше его не проверять до выполнения следующего действия.

Игры с последовательным порядком ходов являются статическими средами. Пример — шахматы.

Может быть так, что за время выбора агентом следующего действия меняется не среда, а его показатели производительности. В этом случае среда называется **полудинамической**.

#### Количество различимых состояний среды

Среды могу отличаться тем, как в них различаются следующие факторы:

* Состояние среды.
* Время.
* Восприятие агента.
* Действия агента.

Если любой из этих факторов меняется непрерывно, среда называется **непрерывной**.

Например, в футболе время течёт без остановок. Кроме того, расположение каждого игрока на поле представляет собой некоторое непрерывное значение. Эти значения меняются во времени плавно. Поэтому игра в футбол будет непрерывной средой.

Если у каждого из четырёх факторов есть конечное число состояний, говорят что среда **дискретная**.

Пример дискретной среды — игра в шахматы. В ней число возможных состояний доски огромно, но конечно. Время для каждого игрока останавливается после совершения им хода. Состояние доски меняется только после очередного хода. Сами игроки действуют по очереди.

#### Количество агентов

Если в среде действует только один агент, она называется **одноагентной**. Если агентов большего одного — это **мультиагентная** среда.

Некоторые стохастические одноагентные среды похожи на мультиагентные. Например, в одноагентной среде есть объект, который выполняет случайные действия. Возникает вопрос: следует ли рассматривать этот объект в роли другого агента?

Чтобы отличить объект от другого объекта, надо проанализировать его поведение. Объект считается другим агентом если:

* У него есть показатели производительности.
* Эти показатели зависят от действий нашего агента.
* Объект стремится максимизировать свои показатели производительности.

В **конкурентной** мультиагентной среде показатели производительности агентов обратно пропорциональны. Это значит, что увеличение показателей одного агента уменьшает показатели другого. Любая антагонистическая игра с нулевой суммой является конкурентной средой.

В **кооперативной** мультиагентной среде показатели производительности всех агентов растут и уменьшаются одновременно. Некоторые из игр с ненулевой суммой могут быть кооперативными мультиагентными средами. Пример такой игры — [ультиматум](https://ru.wikipedia.org/wiki/Ультиматум_(игра)). В ней два участника должны договориться о разделении суммы денег. Если им не удалось договориться, никто ничего не получает.

#### Знание правил среды

Перед тем, как начать действовать, агент может знать последствие любого своего действия. В этом случае говорят, что среда **известна** агенту.

Если агент не знает к чему приведёт любое из его возможных действий, то среда называется **неизвестной**.

Вот пример. Рассмотрим агента для игры в шахматы. Если он знает правила игры, ценность и ходы фигур, то среда для него будет известной. Если агент не знает фигуры или правил игры, ему сначала придётся их изучить. В этом случае среда для него будет неизвестной.

Знание правил среды не связано с информацией о её состоянии. В примере с шахматами среда полностью наблюдаема, даже когда агент не знает правил игры. Противоположная ситуация — карточная игра. Агент может знать её правила, но среда частично наблюдаема. Причина в том, что агент не знает карты на руках остальных игроков.

### Классификация интеллектуальных агентов

Функцию агента можно представить в виде таблицы. Эта таблица задаёт соответствия между каждой возможной последовательностью актов восприятия и действием агента.

Для примера в роли агента рассмотрим компьютерную программу. Она следит за местом на жёстком диске. Когда место заканчивается, программа очищает каталог с временными файлами.

Функция нашего агента определяет, когда надо очищать каталог с временными файлами. Представим эту функцию в виде таблицы 2-6.

{caption: "Таблица 2-6. Функция программы очистки диска", width: "100%"}
| Последовательность актов восприятия | Действие |
| --- | --- |
| Место есть | Ждать |
|  | |
| Места нет | Очистить каталог |
|  | |
| Место есть, Место есть | Ждать |
|  | |
| Место есть, Места нет | Очистить каталог |
|  | |
| Место есть, Место есть, Место есть | Ждать |
|  | |
| Место есть, Место есть, Места нет | Очистить каталог |
|  | |
| ... | ... |

Размер этой таблицы зависит от времени работы программы. Число строк равно числу актов восприятия.

Для примера предположим, что программа проверяет диск каждую секунду. Тогда чтобы программа проработала один день, ей нужна таблица с `60 * 60 * 24 = 86400` строкам.

Рассчитаем, сколько нужно памяти компьютера для хранения таблицы нашей программы. Для кодирования одного состояния среды достаточно одного бита. Если бит равен 0, место на диске есть. В противном случае — места нет. Для кодирования действия программы тоже достаточно одного бита:

* 0 — ждать.
* 1 — очистить каталог.

Чтобы сохранить только столбец с последовательностью актов восприятия, нам нужно `(86400 * 86400 / 2) + 86400 / 2 = 3732523200` бит. Добавим к этому 86400 бит, чтобы сохранить по одному действию для каждой строки таблицы. Получим 3732609600 бит или 466576200 байт. Это примерно равно 466 мегабайтам.

Итак, чтобы сохранить функцию элементарного агента в виде таблицы, нам нужно 466 мегабайт памяти. Мы могли бы написать программу с такой таблицей. Эта программа находит строку таблицы, которая соответствует текущей последовательности актов восприятия. Затем она совершает действие из второго столбца.

Решение задачи с помощью таблицы далеко не оптимально. Представьте, что наш агент должен проработать не один день, а неделю. Тогда размер его программы вырастет до `466 * 7 = 3262` мегабайт.

Задача ИИ заключается в том, чтобы перевести функцию агента из формы таблицы в более компактный формат.

Классифицируем агенты по их внутреннему устройству следующим образом:

* Простой рефлексный агент.
* Рефлексный агент с внутренним состоянием.
* Рефлексный агент, основанный на модели.
* Агент, основанный на полезности.
* Обучающийся агент.

Эта классификация основана на идеях Рассела и Норвига с небольшими изменениями. Класс **основанных на цели агентов** не актуален для стратегических игр. Поэтому мы его не рассматриваем.

#### Простой рефлексный агент

Самый примитивный вид агента называется **простым рефлексным**. Такой агент выбирает своё действие на основе текущего состояния среды. При этом он игнорирует всю предыдущую последовательность актов восприятия и все свои прошлые действия.

Предположим, что наша программа очистки диска — это простой рефлексный агент. Тогда её функцию можно представить в виде таблицы правил 2-7.

{caption: "Таблица 2-7. Таблица правил для программы очистки диска", width: "100%"}
| Текущее состояние среды | Действие |
| --- | --- |
| Место есть | Ждать |
|  | |
| Места нет | Очистить каталог |

Теперь чтобы наш агент проработал один день, ему не нужна таблица размером 466 мегабайт. Вместо этого ему достаточно таблицы 2-7. Она занимает в памяти компьютера 4 бита. Размер этой таблицы не зависит от времени работы агента.

Таблицу правил 2-7 можно записать в виде программы на [**псевдокоде**](https://ru.wikipedia.org/wiki/Псевдокод_(язык_описания_алгоритмов)). Листинга 2-1 приводит эту программу.

{caption: "Листинг 2-1. Псевдокод программы очистки диска", line-numbers: true}
```
if места_нет then
    очистить_каталог
else
    ждать
```

Простые рефлексные агенты легко реализовать. Они работают максимально быстро и занимают минимум памяти. Но область их применения ограничена. Такие агенты работаю только в среде со следующими свойствами:

* Полностью наблюдаемая.
* Эпизодическая.
* Статическая.
* Одноагентная.
* Известная.

Если среда частично наблюдаемая, рефлексный агент будет совершать ошибки. Причина в том, что агент не может получить полную информацию о состоянии такой среды через свои датчики. Недостаток информации приводит к тому, что два разных состояния среды выглядят для агента одинаково. В этом случае он может выбрать "неправильное" состояние и соответствующее ему действие. Другими словами, агент путает строки в своей таблице правил.

Простой рефлексный агент не может определить состояние последовательной среды. Причина в том, что он не следит за её прошлыми состояниями и своими предыдущими действиями. Однако, именно они определяют текущее состояние среды. Результат будет тем же, что и для частично наблюдаемой среды: агент неправильно выбирает свои действия.

Рефлексный агент не может работать в динамической среде. Проблема в том, что он не прогнозирует изменения среды в ближайшем будущем. Однако, пока агент выполняет своё действие, среда меняется. В результате на момент совершения этого действия состояние среды может требовать совсем другого поведения.

В мультиагентной среде у рефлексного агента возникнут проблемы. Он может наблюдать только текущее состояние среды. У него нет возможности просчитать будущие действия других агентов и их последствия. Однако, эти действия влияют на показатели производительности нашего агента. Следовательно, он должен выбирать свои действия с учётом поведения других агентов, но не может этого сделать.

Предположим, что среда неизвестна рефлексному агенту. Это означает, что таблица его правил либо пуста, либо заполнена не полностью. У агента нет возможности самостоятельно заполнить свою таблицу правил, т.е. обучиться.

#### Рефлексный агент с внутренним состоянием

Следующий по сложности тип агента — это **рефлексный агент с внутренним состоянием**. В его программе решена проблема работы в следующих средах:

* Частично наблюдаемые.
* Последовательные.

Агент такого типа хранит **внутреннее состояние**. Оно определяется предыдущими актами восприятия и действиями агента.

С помощью внутреннего состояния агент может отслеживать изменения в последовательной среде. Благодаря этому, он точно знает, какой строке из таблицы правил соответствует текущее состояние среды. Поэтому агент выбирает правильное действие.

Агент с внутренним состоянием может работать в частично наблюдаемых средах. Однако, его действия не всегда будут правильными.

Рассмотрим пример. Предположим,что датчики агента получили информацию о части среды, которая впоследствии оказалась скрыта. Агент запоминает эту информацию в своём внутреннем состоянии. Теперь он получает полное состояние среды из своего текущего восприятия и внутреннего состояния. Восприятие даст информацию о видимой части среды, а внутреннее состояние — о скрытой. В этом случае агент выбирает правильное действие из таблицы правил.

Возможны ситуации, когда скрытая часть среды остаётся недоступна агенту. Например, он не наблюдал её ни в одном из предыдущих актов восприятия. Именно это происходит в карточных играх, когда игроки не знают карт друг друга.

Другая ситуация — агент получил информацию о скрытой части среды, но состояние этой части изменилось. Например, агент видел карты другого игрока. Но на следующий ход игрок вытянул другие карты.

В общем случае агент с внутренним состоянием предполагает состояние частично наблюдаемой среды. Исходя из этого предположения, он выбирает своё действие. Если предположение оказалось ложным, действие агента будет неподходящим.

#### Рефлексный агент, основанный на модели

Чтобы действовать правильно в динамической среде, программа агента должна иметь **модель среды**. Эта модель описывает следующие правила:

1. Как изменяется среда с течением времени независимо от действий агента?

2. Как любое возможное действие агента изменяет среду?

**Рефлексный агент, основанный на модели**, использует модель среды для прогнозирования её изменений. В этом случае он может работать в динамической среде. Для выбора действия агент учитывает текущее восприятие и прогноз о ближайших изменениях в среде. Так он выбирает верную строку в таблице правил и соответствующее ей действие.

Модель позволяет агенту правильно действовать и в некоторых мультиагентных средах. Предположим, что среда простая и у каждого агента мало возможных действий. Тогда модель даёт прогноз о всех изменениях среды в ближайшем будущем. Учитывая эту информацию, основанный на модели агент выберет правильное действие.

Если мультиагентная среда сложная, просчитать её возможные состояние нельзя из-за комбинаторного взрыва. Чтобы ограничить пространство поиска состояний среды, нужны эвристики. Но у основанного на модели агента эвристик нет. Поэтому в такой среде он работать не сможет.

#### Агент, основанный на полезности

Все рефлексные агенты работают по таблице правил. Эта таблица может представлять собой код с условными операторами или структуру данных в памяти компьютера. **Основанные на полезности агенты** используют принципиально другой способ принятия решений.

Агент может рассматривать текущие и будущие состояния среды с точки зрения их желательности. Для оценки этих состояний используют понятие **полезность**. Полезность означает предпочтение агентом тех или иных состояний среды. Часто чем выше полезность, тем выше показатели производительности агента.

Количественную оценку полезности даёт **функция полезности**. Эта функция отображает состояние среды (или последовательность состояний) на вещественное число. Это число означает степень удовлетворённости агента. Для правильно спроектированного агента большее значение функции полезности соответствует лучшим показателям производительности агента.

Что даёт агенту функция полезности? Эта функция позволяет агенту оценить успешность своих будущих действий. Используя эту оценку, агент может перебрать все возможные действия, чтобы выбрать из них наилучшее. Для такого перебора используются **алгоритмы поиска**. Именно эти алгоритмы заменяют таблицу правил для агентов, основанных на полезности.

Кроме выбора текущего действия агент может строить последовательность действий, которая приведёт его к наилучшему результату. Этот процесс называется **планированием**.

Может возникнуть вопрос: чем отличаются показатели производительности от функции полезности? Эти понятия означают разные вещи.

Показатели производительности — это внешняя оценка поведения агента его разработчиком. Эти показатели нужны во время создания, тестирования и обучении агента. Также они позволяют сравнить агентов c разными программами. Показатели производительности определяют, делает ли агент то, что должен.

Функция полезности — это внутрення оценка агентом своих действий. При принятии решений агент может ориентироваться на функцию полезности, но не на показатели производительности. Функция полезности определяет насколько желательны для агента те или иные состояния среды.

Агент, основанный на полезности, успешно действует в любых частично наблюдаемых средах. Даже если часть среды постоянно остаётся скрытой от агента, он выберет наилучшее возможное действие в текущей ситуации. В отличие от рефлексного агента, он не путает состояния среды. Это предотвращает заведомо неправильные действия.

Функция полезности позволяет агенту действовать в сложных мультиагентных средах. Она служит эвристикой при расчёте возможных действий других агентов и будущих состояний среды. Благодаря эвристике удаётся сократить пространство поиска и предотвратить комбинаторный взрыв.

Поиск возможных состояний среды может не дать конечного состояния, которое соответствует решению задачи. В этом случае агент может оценить найденные промежуточные состояния с помощью функции полезности. Вероятно, что состояния с большей полезностью окажутся ближе к искомому решению.

#### Обучающийся агент
