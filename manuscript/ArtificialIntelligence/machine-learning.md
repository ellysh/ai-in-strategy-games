## 2.6 Машинное обучение

Мы рассмотрели методы формальной логики и алгоритмы поиска, которые применяются в программах интеллектуальных агентов. Теперь познакомимся с подходами, разработанными в рамках машинного обучения. В качестве примера рассмотрим несколько простых моделей класса shallow learning.

### 2.6.1 Механизм обучения

Начнём с основных понятий, которые применяются в машинном обучении.

**Механизм обучения** это набор методов, который автоматически создаёт программу интеллектуального агента. Любой механизм обучения состоит из двух компонентов:

* **Алгоритм обучения**
* [**Обучаемая модель**](https://wiki.loginom.ru/articles/taught-model.html)

Чтобы воспользоваться механизмом обучения, нужен [**обучающий набор данных**](https://wiki.loginom.ru/articles/training-set.html) (training set). Этот набор состоит из [**обучающих примеров**](https://wiki.loginom.ru/articles/training-sample.html) (training sample).

Алгоритм обучения получает на вход обучающий набор данных. Далее алгоритм находит в них закономерности и строит обучаемую модель. Эта модель учитывает все найденные закономерности.

Для разных типов обучаемых моделей применяются разные алгоритмы обучения. Алгоритм обучения определят требования к обучающему набору данных.

По принципу работы алгоритмы обучения делятся на три класса:

* [**Обучение с учителем**](https://ru.wikipedia.org/wiki/Обучение_с_учителем) или контролируемое (supervised learning)

* [**Обучение без учителя**](https://ru.wikipedia.org/wiki/Обучение_без_учителя) или неконтролируемое (unsupervised learning)

* [**Обучение с подкреплением**](https://ru.wikipedia.org/wiki/Обучение_с_подкреплением) (reinforcement learning)

Каждый класс представляет собой отдельную **парадигму**. Парадигма — это система предположений, концепций и практических приёмов для решения какой-то задачи.

Один и тот же алгоритм обучения можно представить в разных формах:

* С помощью математических формул.
* В виде псевдокода.
* В виде запускаемой программы.
* В виде программной библиотеки.

Форму представления выбирают исходя из задачи. Первые две формы чаще применяют в образовательных целях. Последние две — на практике.

Алгоритм обучения после обработки обучающего набора данных строит обучаемую модель. Она хранит результаты процесса обучения. Модель состоит из правил и наборов данных, которые необходимы для прогнозирования. По-сути модель — это программа. Она имеет данные и машинные инструкции для их обработки. Инструкции используют данные, чтобы выполнять прогнозы для новых наборов данных. Эти наборы представляют собой восприятие агента в процессе его работы.

Обучаемая модель состоит из двух компонентов:

* **Данные модели**
* **Алгоритм для прогнозирования**

Цель машинного обучения — автоматически получить обучаемую модель. Алгоритм обучения используется как средство достижения этой цели. Он влияет на качество обучаемой модели. При этом конечная программа агента никак не использует алгоритм обучения.

### 2.6.2 Алгоритмы обучения

Теперь познакомимся с парадигмами алгоритмов обучения.

Таблица 2-18 демонстрирует парадигмы обучения и решаемые с их помощью задачи.

{caption: "Таблица 2-18. Парадигмы алгоритмов обучения", width: "100%"}
| Парадигма обучения | Задачи |
| --- | --- |
| Обучение с учителем | Регрессия |
|  | Классификация |
|  | Структурный вывод |
|  | |
| Обучение без учителя | Кластеризация |
|  | Обнаружение аномалий |
|  | Сокращение размерности |
|  | Поиск ассоциативных правил |
|  | |
| Обучение с подкреплением | Машинный перевод |
|  | Задача управления |

Рассмотрим некоторые из этих задач подробнее для демонстрации парадигм обучения на примерах.

#### 2.6.2.1 Обучение с учителем

Алгоритмы обучения с учителем принимают на вход **размеченные данные**. В терминологии интеллектуальных агентов размеченный обучающий пример означает следующее:

> Обучающий пример содержит акт восприятия агента и ожидаемое действие на него.

В этой же терминологии алгоритм обучения работает так:

> Алгоритм принимает на вход размеченные данные. Эти данные содержат примеры работы желаемой функции агента. В результате обучения алгоритм строит модель. Она реализует функцию агента, поведение которой близко к желаемой.

Теперь перейдём к терминологии, принятой в машинном обучении. Она более точна, но в то же время сложнее для понимания. Для начала дадим определение размеченному обучающему примеру:

> Обучающий пример представляет собой пару векторов X = (x~1~, x~2~,..., x~n~) и Y = (y~1~, y~2~,..., y~n~). X – это вектор входных значений, а Y – вектор выходных значений. Вектор Y должна вернуть обучаемая модель при получении на вход вектора X.

I> **Вектором** в контексте машинного обучения называют упорядоченный набор чисел.

Теперь рассмотрим общий алгоритм обучения с учителем. Алгоритм передаёт на вход модели вектор входных значений X очередного примера из обучающего набора. Модель в ответ на эти входные значения возвращает вектор выходных значений Y'.

Далее алгоритм обучения сравнивает вектора Y из обучающего примера и Y', полученный от модели. Если эти вектора не совпадают, это называется [**ошибкой обучения**](https://wiki.loginom.ru/articles/training-error.html) (training error). При возникновении такой ошибки алгоритм обучения автоматически изменяет параметры модели. Если вектора Y и Y' совпали, алгоритм обучения переходит к следующему примеру.

Когда обучаемая модель готова, её работу можно оценить с помощью размеченного [**тестового набора данных**](https://wiki.loginom.ru/articles/test-set.html) (test set). Эти данные не входят в обучающий набор, поэтому модель ещё никогда с ними не встречалась. Для каждого **тестового примера** модель возвращает вектор выходных значений Y'. Если он не совпадает с вектором Y из примера, это называется [**ошибкой обобщения**](https://wiki.loginom.ru/articles/generalization-error.html) (generalization error). При высокой частоте таких ошибок, говорят, что **производительность** или **точность** модели низка.

Чтобы улучшить производительность модели, есть целый [ряд методов](https://machinelearningmastery.com/improve-deep-learning-performance/). Некоторые из них меняют конфигурацию модели. В результате она подгоняется под обучающий набор данных. Чрезмерная подгонка приводит к проблеме [**переобучения**](https://wiki.loginom.ru/articles/overtraining.html) (overtraining или overfitting). Суть проблемы в том, что модель хорошо работает на обучающем наборе данных, но часто ошибается на тестовых и реальных данных. В этом случае говорят о плохой [**обобщающей способности модели**](https://wiki.loginom.ru/articles/generalization-ability.html) (generalization ability). Обобщающая способность модели — это эффективность её работы с новыми данными, с которыми она не встречалась в процессе обучения. Проверка модели на тестовом наборе данных позволяет обнаружить проблему переобучения.

Обобщающая способность даёт лишь примерную оценку того, как модель будет справляться с реальной задачей. Более точно это определяет **предсказательная способность модели**. Предсказательной способностью модели называется её эффективность при решении задачи, для которой она была разработан. Например, эта эффективность может измеряться как соотношение числа правильных и ложных выводов модели при распознавании образов.

Чтобы оценить предсказательную способность модели, её проверяют на [**валидационном наборе данных**](https://wiki.loginom.ru/articles/validation-set.html) (validation set). Примеры этого набора не входят ни в обучающий, ни в тестовый наборы. Такая проверка модели называется [**валидацией**](https://help.loginom.ru/userguide/processors/validation.html) (validation).

Обучение с учителем эффективно для решения следующих задач:

* **Классификация** — определение категории, к которой принадлежит некоторый объект.

* **Регрессия** — предсказание числового значения по входным данным.

* **Структурный вывод** — порождение вектора выходных значений, между элементами которого существуют важные связи.

Рассмотрим подробнее первые две задачи и примеры к ним. Это даст нам общее представление об алгоритмах и моделях обучения с учителем.

##### 2.6.2.1.1 Классификация

Задача классификации — определить класс объекта по его параметры. Параметры объекта в терминологии машинного обучения называются **признаками**. Например, рассмотрим объект автомобиль. Его признаки такие: габариты, мощность двигателя, расход топлива и т.д.

Для обучения модели, которая решает задачу классификации, нужен специальный формат набора обучающих данных. В каждом примере вектор входных значений X представляет собой все проверяемые признаки конкретного объекта. Выходной вектор Y состоит из единственного скалярного значения y. Оно определяет номер класса, к которому следует отнести объект из примера. Этот номер класса называется **меткой**.

При решении задачи классификации обучаемая модель возвращает дискретные значения. Обычно это номер класса, к которому относится объект, признаки которого модель получила на вход.

Каноничным примером для демонстрации задачи классификации считается набор данных под названием [**ирисы Фишера**](https://ru.wikipedia.org/wiki/Ирисы_Фишера). С его помощью английский статистик и биолог Рональд Фишер продемонстрировал разработанный им метод статистического анализа в 1936 году.

Набор данных ирисы Фишера представляет собой таблицу со следующей информацией:

1. Длина чашелистника (sepal) цветка.
2. Ширина чашелистника (sepal) цветка.
3. Длина лепестка (petal) цветка.
4. Ширина лепестка (petal) цветка.
5. Вид ириса.

Всего в таблице 150 записей для трёх видов ириса. Для каждого вида есть по 50 записей с разными разными параметрами цветка.

Для простоты мы рассмотрим только два параметра цветка: длина и ширина чашелистника. По этим признакам модель должна определить, относится ли растение к виду Iris setosa или к другому виду.

В ходе обучения на наборе данных, алгоритм обучения строит модель. Простейшая модель для классификации представляет собой формулу для расчёта выходного значения y по вектору входных значений X. Модели такого типа называются [**линейными классификаторами**](http://www.machinelearning.ru/wiki/index.php?title=Линейный_классификатор).

Формула линейного классификатора в общем виде выглядит так:
{width: "25%"}
![](images/ArtificialIntelligence/linear-classifier-formula.png)

В этой формуле используются следующие обозначения:

* y — выходное значение модели
* f — пороговая функция
* w~i~ — вес признака под номером i
* x~i~ — значение признака под номером i

**Пороговая функция** f отображает все возможные суммы (w~i~ * x~i~) на два дискретных значения. Обычно это значения 0 и 1. Они соответствуют двум классам объектов. Допустим, что в нашем случае значение 0 соответствует виду Iris setosa, а 1 — другим видам ирисов.

**Вес признаков** (feature weight) w~i~ — это некоторые коэффициенты, которые подбирает алгоритм обучения. Благодаря правильно подобранным весам, пороговая функция может однозначно различать классы объектов.

Теперь вернёмся к нашему примеру с ирисами. Мы рассматриваем только два признака: ширина и длина чашелистника цветка. Поэтому формулу линейного классификатора можно упростить до такого вида:
{width: "35%"}
![](images/ArtificialIntelligence/iris-classifier-formula.png)

Обозначения в формуле следующие:

* y — выходное значение модели
* f — пороговая функция
* w~1~ — вес признака ширина чашелистника
* x~1~ — ширина чашелистника
* w~2~ — вес признака длина чашелистника
* x~2~ — длина чашелистника

Возникает вопрос: какая именно пороговая функция f используется в формуле? Эта функция зависит от конкретной обучаемой модели, которую мы решим использовать. Вот некоторые из моделей, которые применяют на практике для линейной классификации:

* [**Метод опорных векторов**](https://ru.wikipedia.org/wiki/Метод_опорных_векторов) (support vector machine или SVM).

* [**Логистическая регрессия**](https://ru.wikipedia.org/wiki/Логистическая_регрессия) или логит-модель (logit model).

* [Нейронная сеть перцептрон](https://www.mathematik.uni-muenchen.de/~deckert/teaching/SS18/sec-steps.html).

Модель линейного классификатора можно наглядно представить в виде графика. Этот график строится на **пространстве признаков**. Размерность пространства совпадает с количеством проверяемых признаков объекта.

В нашем случае признаков два, поэтому график строится в двумерном пространстве. Его демонстрирует иллюстрация 2-18.

{caption: "Иллюстрация 2-18. Модель для классификации ирисов", height: "50%", width: "100%"}
![Классификация ирисов](images/ArtificialIntelligence/fisher-iris-classification.png)

На горизонтальной оси откладывается длина чашелистника в сантиметрах для каждого примера из обучающего набора данных. На вертикальной оси — ширина чашелистника. Фиолетовые точки в левой верхней части графика обозначают ирисы вида Iris setosa. Зелёные и жёлтые точки справ внизу — это ирисы видов Iris virginica и Iris versicolor. Синяя пунктирная линия соответствует границе между классами, которую построил алгоритм обучения.

Наклон линии границы между классами определяют веса w~1~ и w~2~, которые подобрал алгоритм обучения.

Рассмотрим, как работает обученная и проверенная модель. На вход она получает вектор X. Элементы вектора: ширина и длина чашелистника ириса, вид которого надо определить. Модель подставляет полученные признаки x~1~ и x~2~ в формулу и вычисляет значение y. Если y равно 0, то проверяемый ирис относится к виду Iris setosa. Если y равно 1, то мы столкнулись с ирисом какого-то другого вида.

Чтобы продемонстрировать работу модели, обратимся к графику на иллюстрации 2-18. Предположим, что модель получила входные данные: ширина и длина чашелистника неизвестного ириса. Отложим на горизонтальной и вертикальной оси значения этих признаков. Если точка оказалась выше границы классов, проверяемый ирис относится к виду Iris setosa. В противном случае, этот ирис относится к другому виду.

Остаётся открытым один вопрос: как именно алгоритм обучения подбирает веса w~1~ и w~2~ в нашем примере с ирисами? Для моделей линейных классификаторов часто применятся [**стохастический градиентный спуск**](https://ru.wikipedia.org/wiki/Стохастический_градиентный_спуск).

Python скрипт для классификации ирисов Фишера приводится в приложении 5.1.1 и на [Github](https://github.com/ellysh/ai-in-strategy-games/blob/master/manuscript/resources/code/ArtificialIntelligence/linear-classifier.py).

##### 2.6.2.1.2 Регрессия

Задача регрессии — это прогноз значений некоторой **зависимой переменной** y от вектора **независимых переменных** X. Связь между ними математическая: изменение значений независимых переменных систематически меняет значение зависимой. Независимые переменные называются **предикторами** или **регрессорами**, а зависимые — **критериальными**.

Рассмотрим обучающий набор данных для модели, решающей задачу регрессии. Пример состоит из вектора независимых переменных X и единственного зависимого от них значения y. В отличие от задачи классификации y является вещественным числом. В этом случае говорят, что модель возвращает непрерывные значения.

Вот пример задачи регрессии. Нам нужно оценить стоимость автомобиля Volkswagen Golf в зависимости от его пробега. В этом случае мы учитываем единственный признак объекта — пробег в километрах.

Чтобы обучить модель, нам нужен набор данных. Каждый пример этого набора представляет собой пару значений: цена и пробег. Пробег будет единственным элементом вектора X независимых переменных. Цена будет зависимой переменной y.

Предположим, что мы проверили рынок поддержанных автомобилей и нашли несколько предложений Volkswagen Golf. Цены и пробег этих машин мы заносим в таблицу. Далее переводим эти данные в формат, подходящий для алгоритма обучения. Алгоритм обрабатывает эти данные и строит обучаемую модель.

Полученная модель принимает на вход пробег автомобиля. По этому признаку она предсказывает примерную стоимость автомобиля.

Формула модели линейной регрессии в общем виде выглядит так:
{width: "30%"}
![](images/ArtificialIntelligence/linear-regressor-formula.png)

В ней используются следующие обозначения:

* y — выходное значение модели
* w~i~ — вес признака под номером i
* x~i~ — значение признака под номером i
* ε — случайная ошибка модели

Эта модель в отличие от классификатора не использует пороговую функцию f. Вместо этого, она представляет собой указанную выше формулу с правильно подобранными весами w~i~.

Вернёмся к нашей задаче определения цены на поддержанные Volkswagen Golf. Мы рассматриваем только один признак машины — её пробег. Поэтому в формуле модели остаётся только одна независимая переменная x~1~ и её вес w~1~. Таким образом формула принимает следующий вид:
{width: "25%"}
![](images/ArtificialIntelligence/cost-regressor-formula.png)

Здесь используются следующие обозначения:

* y — выходное значение модели
* w~1~ — вес признака пробег автомобиля
* x~1~ — пробег автомобиля в километрах
* ε — случайная ошибка модели.

Модель линейной регрессии можно представить в виде графика. В отличии от линейного классификатора к пространству признаков надо добавить ещё одно измерение — значение зависимой переменной y. Поскольку в нашем примере используется один признак, пространство графика будет двумерным. Первое измерение — это пробег автомобиля в километрах x~1~, а второе — цена y.

Иллюстрация 2-19 демонстрирует модель линейной регрессии для определения цены Volkswagen Golf.

{caption: "Иллюстрация 2-19. Модель для вычисления стоимости автомобиля", height: "50%", width: "100%"}
![Стоимость автомобиля](images/ArtificialIntelligence/car-cost-regression.png)

На горизонтальной оси отображается пробег автомобиля в километрах. На вертикальной оси — его цена. Чёрные точки обозначают тестовый набор данных. Это машины с разным пробегом и их актуальные цены. Мы получили их из анализа рынка поддержанных машин. Синяя линия отображает зависимость между пробегом и ценой автомобиля. Это и есть наша модель, которую построил алгоритм обучения.

Вес w~1~ определяет наклон линии на иллюстрации 2-19.

Рассмотрим, как работает наша модель. На вход она получает вектор независимых переменных X. Он состоит из единственного значения x~1~ — пробег автомобиля, цену которого надо определить. Модель подставляет это значение x~1~ в формулу и вычисляет y. Значение y — это искомая цена автомобиля.

Обратимся к графику на иллюстрации 2-19, чтобы представить работу модели наглядно. Найдём цену автомобиля, исходя из его пробега. Для этого отложим величину пробега на горизонтальной оси. Например, это 2000 км. Такому пробегу соответствует одна точка на синей линии. По вертикальной оси этой точке соответствует цена 20000$. Таким образом мы нашли ожидаемую цену автомобиля.

Чтобы подобрать веса w~i~ в формуле модели, на практике часто применяется [**метод наименьших квадратов**](http://www.machinelearning.ru/wiki/index.php?title=Метод_наименьших_квадратов) (МНК). Он является алгоритмом обучения модели.

Python скрипт для линейной регрессии цен на автомобили приводится в приложении 5.1.2 и на [Github](https://github.com/ellysh/ai-in-strategy-games/blob/master/manuscript/resources/code/ArtificialIntelligence/linear-regression.py).

#### 2.6.2.2 Обучение без учителя

Обучение с учителем хорошо работает, когда есть большой объём размеченных данных. Однако подготавливать такие данные — трудоёмкий и дорогой процесс. Обычно это приходится делать вручную. Из-за сложной подготовки наборы обучающих данных получаются небольшими. Поэтому обучить на них удаётся только относительно простые модели. Для сложных моделей требуется больше данных.

Обученная модель нужна для решения какой-то прикладной задачи. Чем сложнее задача, тем сложнее должна быть модель. Если модель проще чем функция, которую она должна реализовать, возникает проблема [**недообучения**](https://wiki.loginom.ru/articles/underfitting.html) (underfitting). В случае недообучения обобщающая способность модели оказывается неприемлемо низкой.

Обучение без учителя решает проблему трудоёмкой подготовки обучающих данных. Такие алгоритмы обучения принимают на вход **неразмеченные данные**. Каждый обучающий пример таких данных содержит только вектор входных значений X = (x~1~, x~2~,..., x~n~).

Может возникнуть вопрос: как алгоритм обучения поймёт, что делать с неразмеченными данными? Обучение без учителя решает совершенно иные задачи, чем обучение с учителем.

Цели обучения без учителя следующие:

* Найти повторяющиеся закономерности в данных.
* Сгруппировать примеры по их сходству.
* Представить набор данных в сжатом формате.

Алгоритм обучения строится с учётом этих целей. Он работают не так, как алгоритм обучения с учителем.

Обучение без учителя решает следующие задачи:

* **Кластеризация** — объединение объектов в непересекающиеся группы на основе близости значений их признаков.

* **Обнаружение аномалий** — поиск редких элементов или наблюдений, которые значительно отклоняются от остальных данных и не соответствуют представлению о нормальном поведении.

* [**Сокращение размерности**](https://wiki.loginom.ru/articles/data-reduction.html) — преобразование данных в более удобную форму для их анализа и интерпретации.

* [**Поиск ассоциативных правил**](https://en.wikipedia.org/wiki/Association_rule_learning) — обнаружение неявных закономерностей в больших наборах данных.

Рассмотрим алгоритмы и модели обучения без учителя на примере кластеризации.

##### 2.6.2.2.1 Кластеризация

Задача кластеризации заключается в том, чтобы разбить множество объектов на группы. В одну группу попадают объекты со сходными признаками.

Обучающие алгоритмы кластеризации работают с неразмеченными данными. Поэтому у разработчика нет возможности явно задать группы объектов. Алгоритм должен определить их сам в зависимости от признаков объектов.

Важное условие кластеризации: полученные группы объектов не должны пересекаться. Другими словами, каждый объект должен относится только к одной группе.

I> **Кластером** называется набор объектов, схожих по какому-то признаку.

Обучающий пример из набора данных для решения задачи кластеризации состоит из вектора входных значений X. В этом примере нет вектора выходных значений Y, которые ожидаются от модели. Алгоритм обучения выводит его самостоятельно для каждого примера и закладывает полученную зависимость вектора Y от X в модель.

Рассмотрим задачу кластеризации на примере. Предположим, что онлайн магазину нужно сгруппировать пользователей по интересом. Это позволит показывать каждому пользователю только интересную ему рекламу о новых товарах.

У магазина есть статистика о том, сколько раз каждый пользователь просматривал разные категории товаров. Для простоты предположим, что в магазине есть только две категории товаров: велозапчасти и автозапчасти.

Алгоритм обучения должен подготовить модель, которая относит пользователя к некоторому классу. В зависимости от этого класса магазин выбирает, какую рекламу показывать пользователю. Варианты могут быть следующие:

* Показывать рекламу только велозапчастей.
* Показывать рекламу только автозапчастей.
* Показывать рекламу и велозапчастей, и автозапчастей.
* Не показывать рекламу велозапчастей и автозапчастей.

Идея в том, чтобы пользователи получали рекламу только интересной им категории товаров. Другими словами, пользователи с большим числом просмотров велозапчастей должны получать рекламу новых велозапчастей. Аналогично для пользователей, предпочитающих автозапчасти, — они получают рекламу автозапчастей. Рекламу товаров обоих категорий получают те, кому интересны эти обе категории.

Применим алгоритм обучения для задачи кластеризации под названием [**метод k-средних**](https://en.wikipedia.org/wiki/K-means_clustering) (k-means). Среди прочих параметров этому алгоритму нужно указать число ожидаемых кластеров. В нашем примере это число равно четырём. Именно столько вариантов показа рекламы есть у онлайн магазина.

Метод k-средних в ходе обучения находит [**геометрический центр**](https://ru.wikipedia.org/wiki/Барицентр) (centroid) каждого кластера на пространстве признаков. Геометрический центр множества точек представляет собой среднее арифметическое координат этих точек. То есть координата геометрического центра по каждой оси равна сумме координат всех точек по этой оси, разделённая на количество точек.

Найденные геометрические центры кластеров являются обучаемой моделью. Готовая модель может определить к какому кластеру относится каждый из объектов в обучающем наборе данных. Для этого она вычисляет расстояние от объекта до центра каждого кластера на пространстве признаков. Ближайший к объекту центр определит, к какому кластеру его отнести.

Модель кластеризации также может определить кластер незнакомого ей объекта. Однако, такое использование модели нетипично. Правильнее будет добавить новый объект в обучающий набор и перезапустить алгоритм обучения.

Для наглядности представим обученную модель для нашего примера с онлайн магазином на графике. У каждого пользователя мы учитываем только два признака: посещения двух категорий товаров. Поэтому пространство признаков на графике будет двумерным. Его демонстрирует иллюстрация 2-20.

{caption: "Иллюстрация 2-20. Модель для кластеризации пользователей онлайн магазина", height: "50%", width: "100%"}
![Кластеризация пользователей](images/ArtificialIntelligence/online-shop-clustering.png)

Горизонтальная ось показывает количество посещений пользователями категории товаров автозапчасти. Вертикальная ось — количество посещений категории велозапчасти. Большие серые круги обозначают центры кластеров, которые нашел алгоритм k-средних. Ближайшие к этим центрам точки попали в соответствующий кластер.

Что означает график модели в контексте нашей задачи? Алгоритм k-средних выделил следующие четыре кластера пользователей:

* Жёлтые точки — пользователи с небольшим числом посещений обоих категорий товаров. Эти категории им не особенно интересны. Поэтому показывать рекламу им не нужно.

* Синие точки — пользователи, которые интересуются автозапчастями. Им нужно показывать рекламу новинок этой категории товаров.

* Фиолетовые точки — пользователи, которые интересуются велозапчастями. Им нужно показывать рекламу новинок этой категории товаров.

* Зелёные точки — пользователи, которые интересуются обоими категориями товаров. Им нужно показывать рекламу новинок из обоих категорий.

Благодаря нашей модели, онлайн магазин может учесть предпочтения каждого постоянного посетителя и показать интересную ему рекламу.

Если у магазина появился новый посетитель, по нему сначала надо собрать статистику: какие категории товаров он чаще посещает. Затем нового посетителя надо добавить в обучающий набор данных и повторить алгоритм k-средних. Так обученная модель учтёт предпочтения нового пользователя.

Python скрипт для кластеризации методом k-средних  случайно сгенерированного набора данных приведён в приложении 5.1.3 и на [Github](https://github.com/ellysh/ai-in-strategy-games/blob/master/manuscript/resources/code/ArtificialIntelligence/k-mean-clustering.py).

#### 2.6.2.3 Обучение с подкреплением

Мы рассмотрели парадигмы обучения с учителем и без. В них алгоритм обучения получает на вход подготовленные данные. В парадигме обучения с подкреплением таких данных нет. В 
этом её принципиальное отличие.

В обучении с подкреплением применяют терминологию теории интеллектуальных агентов. Она поможет нам понять основные идеи этой парадигмы обучения.

Алгоритмы обучения с учителем и без учителя создают модели, которые работают в эпизодических средах. Обучение происходит на несвязанных между собой примерах входных данных. Готовые модели обрабатывают такие же несвязанные новые данные. В этом случае прошлые решения агента никак не влияют на его последующие решения. Поэтому агент не отслеживает долгосрочные последствия своих действий. Как следствие, он не может эффективно работать в последовательной среде. В такой среде действия, успешные в краткосрочной перспективе, могут привести к негативным результатам в будущем. Чтобы учесть такие долгосрочные последствия, нужна иная парадигма обучения. Именно для таких задач применяется обучение с подкреплением.

Алгоритм обучения с подкреплением строит функцию агента, которая выбирает действия для различных наблюдаемых состояний среды. Выполняя эти действия, агент максимизирует числовой **сигнал вознаграждения** (reward signal). В процессе обучения агент не получает инструкций о том, какие действия следует выполнять. Вместо этого он должен самостоятельно пробовать разные действия и искать среди них те, которые дают максимальный сигнал вознаграждения.

Обучение с подкреплением отличают от других парадигм обучения две особенности:

1. Поиск [методом проб и ошибок](https://en.wikipedia.org/wiki/Trial_and_error) (trial-an-error).

2. Отсроченное по времени вознаграждение.

Главные компоненты обучения с подкреплением — это агент и среда. Каждое действие агента меняет состояние среды. В ответ на это, среда посылает сигнал вознаграждения. Величина этого сигнала позволяет агенту отличить успешные действия от неуспешных.

Кроме агента и среды в обучении с подкреплением есть четыре второстепенных элемента:

* **Стратегия** (policy) означает функцию агента в терминологии обучения с подкреплением. Эта функция определяет действие агента, как ответ на конкретное наблюдаемое состояние среды. Функция может представлять собой как простую таблицу соответствий состояние-действие, так и более сложный вычислительный процесс (например, алгоритм поиска). Поведение агента полностью определяется его функцией.

* **Сигнал вознаграждения** (reward signal) определяет цель обучения с подкреплением. Среда посылает агенту этот сигнал в ответ на каждое выполненное им действие. Вознаграждение представляет собой число. За успешные действия агент получает большее вознаграждение, чем за менее успешные. Главная цель агента — максимизировать своё суммарное вознаграждение в долгосрочной перспективе. Ориентируясь на вознаграждение, агент меняет свою стратегию. Таким образом он избегает действий с низким вознаграждением.

* **Функция полезности** (value function) для указанного состояния среды возвращает общую сумму вознаграждения. Эту сумму агент потенциально получит в будущем, если начнёт действовать с указанного состояния среды. Функция полезности определяет долгосрочную перспективу действий агента. Она учитывает вознаграждения от будущих состояний среды, которые станут доступны из указанного состояния. Это отличает функцию полезности от вознаграждения. Вознаграждение определяет желательность только текущего состояния среды без учёта доступных из него состояний.

* **Модель среды** (model) имитирует поведение среды. Используя модель, агент прогнозирует результат своих действий. Вот пример того, как работает модель. Она получает на вход текущее состояние среды и выбранное агентом действие. Исходя из этого, модель вычисляет следующее состояние среды и ожидаемый сигнал вознаграждения за выполненное действие. Благодаря модели, агент может планировать свои действия до их исполнения.

Алгоритмы обучения с подкреплением делятся на два типа в зависимости от использования модели среды:

1. **Основанные на модели** (model-based)
2. **Без модели** (model-free)

I> Обратите внимание на различие между терминами "модель среды" и "обучаемая модель". Последнюю называют "агентом" в терминологии обучения с подкреплением.

Перед тем, как обратиться к примеру обучения с подкреплением, разберёмся с двумя вопросами.

Первый вопрос: что означает термин "подкрепление"? Этот термин ввёл российский физиолог Иван Петрович Павлов в начале XX века. Подкреплением он назвал стимул, который усиливает модель поведения животного. Психологи расширили термин "подкрепление". Они обратили внимание на то, что модель поведения можно не только усиливать, но и ослаблять. Важная особенность "подкрепления" в том, что после устранения стимула, животное сохраняет выученное им поведение.

Теперь вернёмся к машинному обучению. Агент всегда действует в какой-то среде. Эта среда некоторым образом реагирует на каждое совершённое действие. Такую реакцию в численном виде представляет функция полезности. Для успешного действия функция вернёт большее значение, чем для менее успешного. Агент воспринимает это значение как поощрение или наказание. Он стремится повторять успешные действия и избегать неудачные. Такое воздействие среды на агента и называется "подкреплением".

Подкрепление бывает двух типов:

* **Положительное подкрепление** увеличивает склонность агента к успешному поведению.

* **Отрицательное подкрепление** уменьшает склонность агента совершать неудачные действия. Таким образом поведение агента в целом становится успешнее, поскольку он избегает состояния среды с низкими показателями функции полезности.

Второй вопрос: что означает поиск методом проб и ошибок в машинном обучении? Прежде всего это врождённый метод мышления человека и животных. Он заключается в повторении различных действий до тех пор, пока задача не будет решена. Например, именно этот метод использует мышь, которая ищет выход из лабиринта. Она пробует первый возможный путь. Если он заканчивается тупиком, мышь возвращается назад и пробует следующий вариант пути.

В обучении с подкреплением поведение агента напоминает поведение мыши в лабиринте. Изначально он ничего не знает о среде. Он пробует первое случайно выбранное действие. Если в ответ на него среда посылает положительный сигнал вознаграждения, агент запоминает это действие как успешное. Если сигнал вознаграждения отрицательный — агент будет избегать этого действия в дальнейшем.

После ряда попыток агент составляет список известных состояний среды и успешных действий в них. Функция полезности связывает текущее состояние среды и будущее вознаграждение. Используя эту функцию, агент может строить последовательности действий. Некоторые последовательности будут более успешными чем другие. Следуя самой успешной из них, агент решает поставленную задачу.

У метода проб и ошибок есть ряд особенностей. Допустим, что решается сложная задача. Её решение состоит из нескольких шагов, причем каждый шаг можно выполнить разными способами. В этом случае метод не найдёт эффективное решение. Вместо этого он укажет на первое сработавшее решение. Это означает, что сложную задачу агент, скорее всего, будет решать неоптимальным образом.

Применяя метод проб и ошибок, агент вынужден искать компромисс между исследованием и эксплуатированием (exploration and exploitation). Суть компромисса заключается в следующем: чтобы максимизировать вознаграждение, агенту лучше повторять уже знакомые успешные действия. Однако, чтобы их найти, надо пробовать новые ещё неизвестные действия. Получается конфликт. С одной стороны, агент должен эксплуатировать знания, которые у него уже есть. Это позволит ему максимизировать вознаграждение в краткосрочной перспективе. С другой стороны, агент вынужден исследовать новые действия, чтобы постараться улучшить вознаграждение в будущем.

[**Проблема многорукого бандита**](https://en.wikipedia.org/wiki/Multi-armed_bandit) хорошо демонстрирует компромисс исследования и эксплуатировании. Одноруким бандитом называется [игровой автомат](https://ru.wikipedia.org/wiki/Слот-машина). Пользователь должен бросить монету и дёрнуть за рычаг. После этого вращается барабан, который показывает комбинацию символов. Некоторые комбинации являются выигрышными.

В проблеме многорукого бандита у игрока есть набор игровых автоматов. У каждого автомата своё соотношение затрат к выигрышу. Изначально игрок не знает этих соотношений, но может их выяснить после нескольких попыток игры на автомате.

Задача игрока — максимизировать свой выигрыш, имея на руках ограниченное количество монет. Для этого он должен решить для себя следующее:

1. На каких автоматах ему играть?
2. Сколько раз играть на каждом автомате и в какой последовательности?
3. Стоит ли продолжать играть на текущем автомате или перейти к следующему?

Найти точное решение проблемы многорукого бандита пока никому не удалось. Есть только ряд приблизительных решений. Они дают не наилучший, но приемлемый результат.

##### 2.6.2.3.1 Подходы к обучению

Чтобы реализовать обучение с подкреплением на практике, применяют следующие подходы:

1. **Подход, основанный на стратегии** (policy-based) явно строит стратегию агента. Она хранится в памяти на протяжении всего процесса обучения агента и его применения для решения задачи. Некоторые policy-based алгоритмы обучения не используют функцию полезности. Другие применяют её для повышения точности финальной стратегии. Policy-based алгоритмы обучения могут построить следующие стратегии:

   * **Детерминированная стратегия** — предписывает агенту выполнять одни и те же действия в каждом состоянии среды.

   * **Вероятностная (стохастическая) стратегия** — агент выбирает случайно одно из нескольких возможных действий в каждом состоянии среды.

2. **Подход, основанный на полезности** (value-based) не строит стратегию агента явно. Вместо этого он находит функцию полезности. Из неё неявно выводится стратегия агента: в каждом состоянии среды он выбирает действие с максимальной полезность.

3. [**Подход, основанный на модели**](https://ai.stackexchange.com/questions/4456/whats-the-difference-between-model-free-and-model-based-reinforcement-learning) (model-based) — агент в процессе обучения и последующей работы использует **модель среды** для прогнозирования результатов своих действий. Моделью среды называется механизм, который воспроизводит ответную реакцию среды на действия агента. Другими словами модель сообщает, какие состояния станут доступны после совершения конкретного действия и какие вознаграждения (reward signal) агент в них получит.

Рассмотрим подходы обучения с подкреплением на примере. Допустим, что мы создаём агента для игры в крестики-нолики. Этот агент будет соревноваться с несовершенным игроком. Такой игрок допускает ошибки и иногда выбирает не лучший из возможных ходов.

Для решения задачи применим обучение с подкреплением. Наш агент будет многократно играть с оппонентом. То каким образом агент сформирует свою финальную стратегию, будет зависеть от выбранного нами подхода.

Для примера выберем подход, основанный на стратегии (policy-based). В качестве метода применим эволюционный алгоритм. Перед запуском алгоритма надо определить, какую именно стратегию мы ищем. Для этого достаточно указать вероятность победы агента над оппонентом, если он будет играть по найденной стратегии. Предположим, что нас устроит стратегия с вероятностью победы 90%.

Эволюционный алгоритм генерирует набор случайных стратегий. В нашем случае стратегия — это набор правил, которые определяют следующий ход агента для любого возможного состояния поля игры крестики-нолики. Самые первые стратегии, сгенерированные алгоритмом, называются **первым поколением**.

Перед запуском алгоритма полезно указать максимальное число поколений, за которое он должен найти интересующую нас стратегию. Если найти её невозможно в принципе, алгоритм будет продолжаться без конца. Будет лучше прервать его и скорректировать, либо выбрать другой метод обучения с подкреплением.

Предположим, что мы указали все необходимые входные данные и запустили алгоритм обучения. В ходе его выполнения агент многократно играет с оппонентом, используя первую стратегию из списка сгенерированных. В результате получается три числа: количество побед, поражений и ничьих. Основываясь на этом результате рассчитывается вероятность победы агента при использовании этой стратегии. Закончив с ней, агент переходит ко второй стратегии и снова многократно играет с оппонентом. Этот процесс повторяется для всех стратегий первого поколения.

На следующем этапе эволюционный алгоритм отбирает несколько стратегий с максимальной вероятностью победы. Каждая из них модифицируется случайным образом. В результате получается набор стратегий второго поколения. В него входят самые успешные стратегии первого поколения и их модификации.

Агент снова многократно играет с оппонентом, используя стратегии второго поколения. Так отбираются наиболее успешные из них. Этот процесс повторяется снова и снова. Алгоритм обучения завершается, когда находит стратегию с вероятностью выигрыша 90%, либо достигает максимально допустимого числа поколений.

Эволюционные алгоритмы плохо справляются со сложными задачами. Кроме того они требуют значительных вычислительных ресурсов и времени. Сегодня более популярны две группы методов, которые можно отнести к (policy-based):

1. [Policy gradient](https://jonathan-hui.medium.com/rl-policy-gradients-explained-9b13b688b146) — алгоритмы обучения этой группы последовательно приближают текущую стратегию к оптимальной. Для этого они увеличивают вероятность выбора вариантов стратегии, которые дают высокое итоговое вознаграждение. Одновременно с этим вероятность выбора вариантов стратегий с низким вознаграждением понижается. Примеры policy gradient методов:

   * [REINFORCE](https://julien-vitay.net/deeprl/PolicyGradient.html#sec:reinforce)

   * [Trust Region Policy Optimization](https://julien-vitay.net/deeprl/NaturalGradient.html#sec:trust-region-policy-optimization-trpo) (TRPO)

   * [Proximal Policy Optimization](https://julien-vitay.net/deeprl/NaturalGradient.html#sec:proximal-policy-optimization-ppo) (PPO)

* [Actor–critic](https://julien-vitay.net/deeprl/ActorCritic.html#sec:advantage-actor-critic-methods) — алгоритмы обучения этой группы приближают к оптимальной текущую стратегию и функцию полезности одновременно. Алгоритмы состоят из двух компонентов: actor (действующий объект) и critic (critic). Actor выполняет несколько действий, согласно выбранной стратегии. Critic вычисляет полезность состояний среды, которые стали доступны после действий actor. В зависимости от результатов critic, actor улучшает свою стратегию. После выполнения действий actor и расчёта фактических вознаграждений в новых состояниях среды, critic улучшает свою функцию полезности. Примеры actor–critic методов:

   * [Advantage actor-critic](https://julien-vitay.net/deeprl/ActorCritic.html#sec:advantage-actor-critic-a2c) (A2C)

   * [Asynchronous advantage actor-critic](https://julien-vitay.net/deeprl/ActorCritic.html#sec:asynchronous-advantage-actor-critic-a3c) (A3C)

   * [Soft actor-critic](https://spinningup.openai.com/en/latest/algorithms/sac.html) (SAC)

Подход, основанный на полезности (value-based), работает иначе. В этом подходе алгоритм обучения строит таблицу всех возможных состояний игрового поля или позиций. Для каждой позиции алгоритм подбирает некоторое число. Оно оценивает вероятность выигрыша агента, если он продолжит игру из соответствующего состояния поля.

Числовые оценки позиций представляют собой **полезность** (value). Большая полезность означает, более выгодное для агента состояние игрового поля. Построенная алгоритмом обучения таблица позиций и их оценок соответствует функции полезности (value function). Конкретная позиция — это пример входного параметра функции, на который она возвращает соответствующую оценку.

Предположим, что наш агент играет всегда крестиками. Тогда любая позиция с тремя крестиками в ряд или по диагонали имеет полезность равную 1. В таком состоянии игрового поля агент уже одержал победу. Аналогично любая позиция с тремя ноликами в ряд или по диагонали имеет полезность равную 0. Здесь агент уже проиграл.

Перед запуском алгоритма обучения мы выставляем полезность выигрышных позиций в 1, а проигрышных — в 0. Для всех остальных состояний поля устанавливаем полезность 0.5. Это число означает неизвестную нам позицию, в которой агент может добиться победы в 50% случаев. Задача алгоритма обучения — скорректировать оценки этих неизвестных состояний.

Алгоритм обучения проводит многократные игры между агентом и его оппонентом. Агент выбирает свои ходы исходя из таблицы позиций и их оценок. В большинстве случаев он выбирает **жадный** (greedy) ход, который приводит к позиции с максимальной полезностью. Изредка агент отклоняется от этого правила и выбирает ход случайно. В этом случае ход называется **исследовательским** (exploratory). Таким образом агент получает возможность проверить позиции, которые обычно он избегает. Иногда они приводят к неизвестным ранее состояниям поля с высокой полезностью.

В ходе игр между агентом и оппонентом алгоритм обучения корректирует полезность позиций в которых оказывается агент. Это происходит следующим образом:

1. Полезность текущего состояния сохраняется.

2. Агент выполняет жадный ход и оказывается в новом состоянии.

3. Полезность прошлого состояния увеличивается или уменьшается на небольшое значение так, чтобы приблизиться к полезности нового состояния.

Многократно повторяя эти шаги, алгоритм обучения находит пути из начального состояния поля ко всем возможным выигрышным позициям. После окончания обучения агенту достаточно в любой позиции выбирать жадный ход. Такие ходы в итоге приведут его к выигрышу.

Рассмотренный нами метод называется [обучением с временной разницей](http://www.scholarpedia.org/article/Temporal_difference_learning) (temporal difference или TD learning). Семейство подобных методов называется [Q-learning](https://en.wikipedia.org/wiki/Q-learning).

Кроме Q-learning сегодня популярны следующие основанные на полезности (value-based) методы:

* [State-Action-Reward-State-Action](https://en.wikipedia.org/wiki/State–action–reward–state–action) (SARSA) — в отличие от Q-learning оценивает полезность не жадных ходов, а тех которые соответствуют наилучшей найденной в данный момент стратегией.

* Deep Q Network (DQN) — использует нейронную сеть для нахождения функции полезности. Подходит для обучения агента действовать в дискретной среде.

* Deep Deterministic Policy Gradient (DDPG) — использует две нейронные сети. Одна из них выбирает действия, а вторая даёт им оценку.

Эта [статья](https://habr.com/ru/post/561746/) приводит их краткое описание.