## 2.6 Машинное обучение

Мы рассмотрели методы формальной логики и алгоритмы поиска, которые применяются в программах интеллектуальных агентов. Это относительно простые способы их реализации.

Теперь мы познакомимся с подходами, разработанными в рамках машинного обучения. Они применяются в программах обучающихся агентов.

Машинное обучение — очень сложная тема. Чтобы в ней разобраться, нужна хорошая техническая подготовка. Мы познакомимся с основными принципами машинного обучения на примере простейших моделей класса shallow learning. Их достаточно легко понять без специальных знаний.

Более сложные модели мы рассмотрим в следующей главе, посвящённой шахматным программам.

### 2.6.1 Механизм обучения

Начнём с основных терминов, которые применяются в машинном обучении.

**Механизм обучения** — это набор методов, который автоматически создаёт программу интеллектуального агента. Любой механизм обучения состоит из двух компонентов:

* **Алгоритм обучения**
* [**Обучаемая модель**](https://wiki.loginom.ru/articles/taught-model.html)

Чтобы воспользоваться механизмом обучения, нужен [**обучающий набор данных**](https://wiki.loginom.ru/articles/training-set.html) (training set). Этот набор состоит из [**обучающих примеров**](https://wiki.loginom.ru/articles/training-sample.html) (training sample).

Алгоритм обучения получает на вход обучающий набор данных. У алгоритма две основные задачи:

* Найти закономерности в обучающем наборе данных.

* Построить обучаемую модель, которая учитывает все найденные закономерности.

Есть несколько типов обучаемых моделей. Для каждого типа применяется свой алгоритм обучения. Алгоритм обучения определяет характеристики обучающего набора данных. Эту зависимость демонстрирует иллюстрация 2-16.

{caption: "Иллюстрация 2-16. Компоненты машинного обучения", height: "30%"}
![Компоненты машинного обучения](images/ArtificialIntelligence/machine-learning.png)

По принципу работы алгоритмы обучения делятся на три класса:

* [**Обучение с учителем**](https://ru.wikipedia.org/wiki/Обучение_с_учителем) или контролируемое (supervised learning)

* [**Обучение без учителя**](https://ru.wikipedia.org/wiki/Обучение_без_учителя) или неконтролируемое (unsupervised learning)

* [**Обучение с подкреплением**](https://ru.wikipedia.org/wiki/Обучение_с_подкреплением) (reinforcement learning)

Каждый класс представляет собой отдельную парадигму. **Парадигма** — это система предположений, концепций и практических приёмов для решения какой-то задачи.

Один и тот же алгоритм обучения можно представить в разных формах:

* В виде математических формул
* В виде псевдокода
* В виде запускаемой программы
* В виде программной библиотеки.

Форму представления выбирают исходя из задачи. Первые две формы чаще применяют в образовательных целях. Последние две — на практике.

Обучаемая модель хранит результаты процесса обучения. Она состоит из правил и наборов данных, которые необходимы для прогнозирования. По сути, модель — это программа. Она имеет данные и машинные инструкции для их обработки. Инструкции выполняют действия над данными. В результате этих действий модель даёт прогноз для новых наборов данных. Эти наборы представляют собой восприятие агента в процессе работы.

Обучаемая модель состоит из двух компонентов:

* **Данные модели**
* **Алгоритм для прогнозирования**

Цель машинного обучения — автоматически построить обучаемую модель. Алгоритм обучения — только средство для достижения этой цели. Он важен, поскольку влияет на качество обучаемой модели. Но конечная программа агента никак не использует алгоритм обучения.

### 2.6.2 Алгоритмы обучения

Познакомимся с парадигмами машинного обучения. Их демонстрирует таблица 2-20. В таблице приводятся также задачи, решаемые в рамках каждой парадигмы.

{caption: "Таблица 2-20. Парадигмы алгоритмов обучения", width: "100%"}
| Парадигма обучения | Задачи |
| --- | --- |
| Обучение с учителем | Регрессия |
|  | Классификация |
|  | Структурный вывод |
|  | |
| Обучение без учителя | Кластеризация |
|  | Обнаружение аномалий |
|  | Сокращение размерности |
|  | Поиск ассоциативных правил |
|  | |
| Обучение с подкреплением | Машинный перевод |
|  | Задача управления |

Рассмотрим несколько типичных задач в качестве примеров. С их помощью разберём каждую парадигму обучения.

#### 2.6.2.1 Обучение с учителем

Алгоритмы обучения с учителем принимают на вход **размеченные данные**. В терминологии интеллектуальных агентов **размеченный обучающий пример** означает следующее:

> Обучающий пример содержит акт восприятия агента и ожидаемое действие на него.

В этой же терминологии алгоритм обучения работает так:

> Алгоритм принимает на вход размеченные данные. Эти данные содержат примеры работы желаемой функции агента. В результате обучения алгоритм строит модель. Она реализует функцию агента, поведение которой близко к желаемой.

Теперь перейдём к терминологии, принятой в машинном обучении. Она более точна, но в то же время сложнее для понимания. Для начала дадим определение размеченному обучающему примеру:

> Обучающий пример представляет собой пару векторов X = (x~1~, x~2~,..., x~n~) и Y = (y~1~, y~2~,..., y~n~). X – это вектор входных значений, а Y – вектор выходных значений. Обучаемая модель должна вернуть вектор Y при получении на вход вектора X.

I> [**Вектором**](https://ru.wikipedia.org/wiki/Вектор_(математика)#Вектор_как_последовательность) в машинном обучении называют упорядоченный набор чисел.

Рассмотрим алгоритм обучения с учителем в общем виде. Его шаги следующие:

1. Алгоритм передаёт на вход модели вектор входных значений X очередного примера из обучающего набора.

2. В ответ на входные значения модель возвращает вектор выходных значений Y'.

3. Алгоритм обучения сравнивает вектора Y из обучающего примера и Y', полученный от модели.

4. Если вектора Y и Y' не совпадают, это называется [**ошибкой обучения**](https://wiki.loginom.ru/articles/training-error.html) (training error). Чтобы её скорректировать, алгоритм обучения автоматически изменяет параметры модели.

5. Если вектора Y и Y' совпали, алгоритм обучения переходит к следующему примеру из обучающего набора.

При обучении модели могут возникнуть проблемы. Вот некоторые из них:

1. Низкая производительность модели (model performance)

2. Переобучение (overfitting)

3. Недообучение (underfitting)

4. Плохая обобщающая способность модели (generalization ability).

Рассмотрим их по порядку.

Когда обучаемая модель готова, её работу оценивают с помощью размеченного [**тестового набора данных**](https://wiki.loginom.ru/articles/test-set.html) (test set). Эти данные не входят в обучающий набор. Поэтому модель ещё никогда с ними не встречалась. Для каждого **тестового примера** модель возвращает вектор выходных значений Y'. Если он не совпадает с вектором Y из примера, это называется [**ошибкой обобщения**](https://wiki.loginom.ru/articles/generalization-error.html) (generalization error). При высокой частоте таких ошибок говорят, что **производительность** (performance) или **точность** (accuracy) модели низка.

I> Производительность модели — это более широкий термин, чем точность. Он включает в себя различные показатели для оценки того, насколько хорошо модель справляется с конкретной задачей.

Низкая производительность модели — это сигнал о том, что процесс обучения завершился неудачно. Это может произойти по ряду причин. Самая распространённая из них называется проблемой недообучения. [**Недообучение**](https://wiki.loginom.ru/articles/underfitting.html) (underfitting) возникает, когда модель оказалась слишком простой. Она не способна отследить закономерности в обучающем наборе данных.

Причины недообучения могут быть следующие:

1. Обучающий набор данных слишком мал.

2. Конфигурация модели не соответствует задаче.

3. Выбран неподходящий алгоритма обучения.

4. Малое число итераций алгоритма обучения.

5. Шум в обучающем наборе данных.

I> **Шумом** в обучающих данных называются случайные изменения. Они не отражают основные закономерности, для изучения которых предназначена модель.

Чтобы решить проблему недообучения, есть [ряд методов](https://machinelearningmastery.com/improve-deep-learning-performance/). Подходящий метод выбирается исходя из причины проблемы. В результате исправления может поменяться конфигурация модели. Так модель подгоняется под обучающий набор данных. Чрезмерная подгонка приводит к переобучению. Проблема [**переобучения**](https://wiki.loginom.ru/articles/overtraining.html) (overfitting или overtraining) возникает, когда модель слишком хорошо изучает обучающий набор данных. Она начинает реагировать на шум и случайные отклонения в данных вместо основных закономерностей. Как следствие, модель хорошо работает на обучающем наборе данных, но часто ошибается на тестовых и реальных данных.

Переобучение — это одна из возможных причин плохой обобщающей способности модели. [**Обобщающая способность**](https://wiki.loginom.ru/articles/generalization-ability.html) (generalization ability) — это эффективность работы модели с новыми данными, с которыми она не встречалась в процессе обучения. Чтобы работать с ними, модель должна применять закономерности, извлечённые из обучающего набора данных. Низкая обобщающая способность говорит о том, что алгоритм обучения не смог извлечь эти закономерности.

Обобщающая способность даёт лишь примерную оценку того, как модель будет справляться с реальной задачей. Более точно это определяет её предсказательная способность. **Предсказательной способностью** (predictive ability) называется эффективность модели при решении задачи, для которой она была разработана. Для измерения этой эффективности есть много показателей. Какие из них применять, зависит от конкретной модели и решаемой ею задачи.

Чтобы оценить предсказательную способность модели, её проверяют на [**валидационном наборе данных**](https://wiki.loginom.ru/articles/validation-set.html) (validation set). Примеры этого набора не входят ни в обучающий, ни в тестовый набор. Такая проверка модели называется [**валидацией**](https://help.loginom.ru/userguide/processors/validation.html) (validation).

Обучение с учителем эффективно для решения следующих задач:

* **Классификация** (classification) — определение категории, к которой принадлежит некоторый объект.

* **Регрессия** (regression) — предсказание числового значения по входным данным.

* **Структурный вывод** (structured prediction) — порождение вектора выходных значений, между элементами которого существуют связи и зависимости.

Рассмотрим подробнее первые две задачи на примерах. Они дадут общее представление об алгоритмах обучения с учителем и построенных ими моделях.

##### 2.6.2.1.1 Классификация

Задача классификации — определить класс объекта по его параметрам. Параметры объекта в терминологии машинного обучения называются **признаками**. Например, объект автомобиль имеет следующие признаки: габариты, мощность двигателя, расход топлива и т.д.

Иллюстрация 2-16 демонстрирует зависимость характеристик обучающего набора данных от класса алгоритма обучения и типа модели. Эта зависимость выполняется для задачи классификации. Чтобы построить подходящую модель, нужен специальный формат обучающих данных. Каждый пример такого набора состоит из двух векторов:

1. Вектор входных значений X содержит все проверяемые признаки конкретного объекта: x~1~, x~2~,..., x~n~.

2. Вектор Y содержит единственное выходное значение y. Это номер класса, к которому относится объект из примера. Номер класса также называется **меткой**.

Обученная модель работает по следующему принципу. Она получает на вход вектор признаков объекта X. На выходе модель возвращает вектор Y'. Он содержит единственное дискретное значение y' с номером класса объекта.

Для демонстрации задачи классификации есть каноничный пример. Это набор данных под названием [**ирисы Фишера**](https://ru.wikipedia.org/wiki/Ирисы_Фишера). Его составил английский статистик и биолог Рональд Фишер. С помощью набора учёный продемонстрировал свой новый метод статистического анализа в 1936 году.

Набор данных Фишера содержит информацию о цветках различных видов ирисов. Он представляет собой таблицу со следующей информацией:

1. Длина чашелистика (sepal) цветка.
2. Ширина чашелистика (sepal) цветка.
3. Длина лепестка (petal) цветка.
4. Ширина лепестка (petal) цветка.
5. Вид ириса.

Таблица содержит примеры для трёх видов ирисов. Для каждого вида есть 50 примеров с разными параметрами цветков. Всего в таблице 150 записей.

Мы построим модель для определения вида ириса по параметрам цветка. Для простоты будем учитывать только два параметра: длина и ширина чашелистика. Наша модель будет определять, относится ли растение к виду Iris setosa или нет.

Проходя по набору данных, алгоритм обучения строит модель для классификации. В простейшем случае это формула для расчёта выходного значения y по вектору входных значений X. Модели такого типа называются [**линейными классификаторами**](http://www.machinelearning.ru/wiki/index.php?title=Линейный_классификатор) (linear classifier).

Формула линейного классификатора в общем виде выглядит так:
{height: "10%"}
![](images/ArtificialIntelligence/linear-classifier-formula.png)

Обозначения в формуле следующие:

* y — выходное значение модели
* f — пороговая функция
* w~i~ — вес признака под номером i
* x~i~ — значение признака под номером i.

**Пороговая функция** f отображает все возможные суммы (w~i~ * x~i~) на два дискретных значения. Допустим, что в нашем случае это значения 0 и 1. Они соответствуют двум классам объектов:

* 0 — вид Iris setosa.
* 1 — другой вид ириса.

**Вес признаков** (feature weight) w~i~ — это коэффициенты, которые подбирает алгоритм обучения. Если веса подобраны правильно, пороговая функция верно определит класс объекта по заданным признакам.

В примере с ирисами мы учитываем только два признака: ширину и длину чашелистика цветка. Поэтому формулу линейного классификатора можно упростить:
{height: "5%"}
![](images/ArtificialIntelligence/iris-classifier-formula.png)

Обозначения в формуле следующие:

* y — выходное значение модели.
* f — пороговая функция.
* w~1~ — вес признака длина чашелистика.
* x~1~ — длина чашелистика.
* w~2~ — вес признака ширина чашелистика.
* x~2~ — ширина чашелистика.

Возникает вопрос: какую именно пороговую функцию f использует линейный классификатор? Эта функция зависит от выбора обучаемой модели.

На практике для классификации применяют следующие модели:

* [**Логистическая регрессия**](https://ru.wikipedia.org/wiki/Логистическая_регрессия) (logistic regression) или логит-модель (logit model).

* [**Метод опорных векторов**](https://ru.wikipedia.org/wiki/Метод_опорных_векторов) (support vector machine или SVM).

* [Нейронная сеть](https://www.mathematik.uni-muenchen.de/~deckert/teaching/SS18/sec-steps.html).

Это самые популярные модели. Кроме них есть и другие.

Мы применим модель логистической регрессии. В этом случае пороговая функция представляет собой **сигмоид** (sigmoid). Его формула выглядит так:
{height: "10%"}
![](images/ArtificialIntelligence/sigmoid-formula.png)

Обозначения в формуле следующие:

* x — входное значение функции.
* y — выходное значение функции.
* e — константа [основание натурального логарифма](https://ru.wikipedia.org/wiki/E_(число)).

Иллюстрация 2-17 демонстрирует график функции сигмоид.

{caption: "Иллюстрация 2-17. Функция активации сигмоид", height: "40%"}
![Функция активации сигмоид](images/ArtificialIntelligence/sigmoid-graph.png)

При x равном 0 график пересекает ось y в точке 0,5. Чем входное значение x больше, тем результат функции y ближе к 1. Чем значение x меньше, тем y ближе к 0.

Результат работы линейного классификатора представляют в виде графика на **пространстве признаков**. Размерность пространства совпадает с количеством проверяемых признаков объекта. График изображает границу принятия решений. [**Граница принятия решений**](https://en.wikipedia.org/wiki/Decision_boundary) (decision boundary) — это гиперповерхность, разделяющая пространство признаков на области. Каждая область соответствует одному классу объектов.

I> **Гиперповерхность** — это геометрический объект, который делит пространство любой размерности на две области.

Граница принятия решений соответствует графику формулы линейного классификатора. Это формула уже обученной модели. Поэтому в ней используются реальные веса признаков, которые подобрал алгоритм обучения.

В нашем примере проверяются только два признака. Поэтому граница принятия решений строится в двумерном пространстве. Её демонстрирует иллюстрация 2-18.

{caption: "Иллюстрация 2-18. Граница принятия решений линейного классификатора", height: "50%", width: "100%"}
![Классификация ирисов](images/ArtificialIntelligence/fisher-iris-classification.png)

Каждая запись из таблицы ирисы Фишера соответствует точке на графике. Горизонтальная ось определяет длину чашелистика цветка в сантиметрах. Вертикальная ось — ширину чашелистика в сантиметрах. Таким образом, цветок с длиной чашелистика 5 см и шириной 2 см соответствует точке на графике с координатой (5, 2).

В левой верхней части графика находятся фиолетовые точки. Они соответствуют цветкам ириса вида Iris setosa. Зелёные и жёлтые точки справа внизу — это ирисы видов Iris virginica и Iris versicolor. Синяя пунктирная линия отделяет цветки вида Iris setosa от остальных. Это и есть граница принятия решений. Её наклон определяют веса w~1~ и w~2~, которые подобрал алгоритм обучения.

Рассмотрим, как работает обученная модель. На вход она получает вектор X. У него два элемента:

* x~1~ длина чашелистика цветка
* x~2~ ширина чашелистика цветка.

По этим признакам надо определить вид растения. Модель подставляет x~1~ и x~2~ в формулу линейного классификатора и вычисляет значение y. Если y равен 0, то проверяемый ирис относится к виду Iris setosa. Если y равен 1, это ирис какого-то другого вида.

График на иллюстрации 2-18 наглядно демонстрирует работу модели. Допустим, модель получила входной вектор X. Отложим на горизонтальной оси значение признака x~2~, а на вертикальной оси — x~1~. Получим точку в пространстве. Она соответствует цветку некоторого ириса, который надо классифицировать. Если точка оказалась выше границы принятия решений, растение относится к виду Iris setosa. В противном случае, это другой вид ириса.

Остаётся открытым ещё один вопрос. Как именно алгоритм обучения подбирает веса w~1~ и w~2~? Для моделей линейных классификаторов применятся алгоритм с названием [**стохастический градиентный спуск**](https://ru.wikipedia.org/wiki/Стохастический_градиентный_спуск) (stochastic gradient descent или SGD). Он реализован в нескольких библиотеках с открытым исходным кодом:

* [scikit-learn](https://scikit-learn.org/stable/modules/sgd.html)

* [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/experimental/SGD)

На практике применяют одну из этих готовых реализаций.

Приложение 5.1.1 подробно описывает Python скрипт для классификации ирисов Фишера. Скрипт также доступен на [Github странице проекта](https://github.com/ellysh/ai-in-strategy-games/blob/master/manuscript/resources/code/ArtificialIntelligence/linear-classifier.py).

##### 2.6.2.1.2 Регрессия

Регрессионный анализ — это статистический метод, который применяется в машинном обучении. Он моделирует взаимосвязи между **зависимой переменной** и несколькими **независимыми переменными**. Зависимая переменная называется **результатом** (outcome) или **ответом** (response). Зависимые переменные называются **предикторами** (predictors) или **признаками** (features).

Основная цель регрессионного анализа — понять природу и значимость взаимосвязи между переменными, а также сделать прогноз. Регрессионный анализ занимается только математическими зависимостями, а не причинно-следственными. Это значит, что он не предполагает логического вывода в каком-либо виде.

Допустим, что некоторая модель должна решать задачу регрессии. Чтобы её создать, надо подготовить обучающий набор данных. Каждый его пример состоит из двух векторов:

1. Вектор входных значений X содержит набор независимых переменных: x~1~, x~2~,..., x~n~.

2. Вектор Y содержит единственное значение зависимой переменной y. В отличие от задачи классификации y является вещественным числом. В этом случае говорят, что модель возвращает **непрерывные значения**.

Обученная модель работает по тому же принципу, что и линейный классификатор. На вход она получает вектор независимых переменных X. На выходе модель возвращает вектор Y'. Единственное значение в нём — это зависимая переменная y'.

Рассмотрим задачу регрессии на примере. Нам нужно оценить цену автомобиля Volkswagen Golf в зависимости от его пробега в километрах. Цена будет зависимой переменной, а пробег — независимой. Другими словами, пробег — это единственный признак, который мы учитываем.

Чтобы обучить модель, нам нужен набор данных. Каждый пример набора представляет собой пару значений: цена и пробег. Пробег x будет единственным элементом входного вектора X. Цена y — элемент выходного вектора Y'.

Предположим, что мы проверили рынок поддержанных автомобилей и нашли несколько предложений Volkswagen Golf. Цены и пробег этих машин мы заносим в таблицу. Далее переводим эти данные в формат, подходящий для алгоритма обучения. Алгоритм обрабатывает эти данные и строит обучаемую модель. Она принимает на вход пробег автомобиля и предсказывает его ожидаемую цену.

В качестве модели применим линейную регрессию. **Линейная регрессия** (linear regression) моделирует линейную зависимость между результатом и признаками. Для этого [**линейное уравнение**](https://ru.wikipedia.org/wiki/Линейное_уравнение) (или уравнение прямой) подгоняется под наблюдаемые данные. Цель модели — найти наиболее подходящую линию, которая минимизирует разницу между прогнозируемыми и фактическими значениями зависимой переменной. Эта линия называется **линией регрессии** (regression line).

Линейная регрессия бывает трёх видов:

1. Если модель учитывает только один признак, она называется **простой линейной регрессией**(simple linear regression).

2. Если модель учитывает несколько признаков, она называется **множественной линейной регрессией** (multiple linear regression).

3. Если модель предсказывает не одну зависимую переменную, а несколько, она называется **многомерной линейной регрессией** (multivariate linear regression).

Наш пример — это случай простой линейной регрессии.

Формула линейной регрессии в общем виде выглядит так:
{height: "10%"}
![](images/ArtificialIntelligence/linear-regressor-formula.png)

Обозначения в формуле следующие:

* y — выходное значение модели.
* w~i~ — вес признака под номером i.
* x~i~ — значение признака под номером i.
* ε — случайная ошибка модели.

Эта модель в отличие от линейного классификатора не использует пороговую функцию f. Выходное значение y рассчитывается по приведённой формуле с правильно подобранными весами w~i~.

В нашей задаче мы учитываем только один признак машины — её пробег. Поэтому в формуле остаётся только один признак x~1~ и её вес w~1~. Таким образом она принимает следующий вид:
{height: "5%"}
![](images/ArtificialIntelligence/cost-regressor-formula.png)

Здесь используются следующие обозначения:

* y — выходное значение модели.
* w~1~ — вес признака пробег автомобиля.
* x~1~ — пробег автомобиля в километрах.
* ε — случайная ошибка модели.

Результат работы линейной регрессии можно представить в виде графика. В отличие от линейного классификатора к пространству признаков надо добавить ещё одно измерение — значение зависимой переменной y. Для нашего примера пространство графика будет двумерным, поскольку мы учитываем только один признак. Первое измерение — это пробег автомобиля в километрах x~1~, а второе — его цена y.

Иллюстрация 2-19 демонстрирует линию регрессии. По ней обученная модель прогнозирует цены на автомобили.

{caption: "Иллюстрация 2-19. Линия регрессии для прогноза цен на автомобили", height: "50%", width: "100%"}
![Цены автомобили](images/ArtificialIntelligence/car-cost-regression.png)

На горизонтальной оси отображается пробег автомобиля в километрах. На вертикальной оси — его цена в долларах. Чёрные точки обозначают примеры из тестового набора данных. Исходные данные по ценам машин и их пробегу мы делим на обучающий и тестовые наборы. Примеры обучающего набора на графике не приводятся. Синяя линия отображает зависимость между пробегом и ценой автомобиля. Это и есть линия регрессии.

Вопрос — от чего зависят наклон и смещение линии регрессии по вертикальной оси? Вес признака пробег автомобиля w~1~ определяет наклон линии. Её смещение определяет случайная ошибка модели ε.

Рассмотрим, как работает обученная модель. Исходные данные для неё — это пробег некоторого автомобиля в километрах. Представляем эти данные в виде вектора X с единственным элементом x~1~. Далее передаём их на вход модели. Она подставляет значение x~1~ в формулу линейной регрессии и вычисляет результат y. Это и будет прогнозируемой ценой автомобиля с указанным пробегом.

Обратимся к графику на иллюстрации 2-19, чтобы представить работу модели наглядно. Спрогнозируем цену некоторого автомобиля по его пробегу. Для этого отложим величину пробега на горизонтальной оси. Пусть для примера он равен 2000 км. Этому значению соответствует только одна точка на синей линии. По вертикальной оси эта точка находится на уровне цены 20000$. Таким образом мы нашли прогнозируемую цену автомобиля.

Чтобы подобрать веса w~i~ в формуле линейной регрессии, на практике обычно применяют алгоритм обучения с названием [**метод наименьших квадратов**](http://www.machinelearning.ru/wiki/index.php?title=Метод_наименьших_квадратов) или МНК (least squares). Он реализован в нескольких библиотеках с открытым исходным кодом:

* [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)

* [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/linalg/lstsq)

* [PyTorch](https://pytorch.org/docs/stable/generated/torch.linalg.lstsq.html)

Приложение 5.1.2 подробно описывает Python скрипт для прогнозирования цен на автомобили. Скрипт также доступен на [Github странице проекта](https://github.com/ellysh/ai-in-strategy-games/blob/master/manuscript/resources/code/ArtificialIntelligence/linear-regression.py).

#### 2.6.2.2 Обучение без учителя

Обучение с учителем хорошо работает, когда есть большой объём размеченных данных. Но подготовить такие данные — трудоёмкий и дорогой процесс. Обычно разметка выполняется вручную человеком. Поэтому набор обучающих данных получается небольшим. На нём удаётся построить только относительно простую модель. Сложная модель требуют намного больше данных.

Обученная модель решает некоторую прикладную задачу. Чем сложнее задача, тем сложнее должна быть модель. Модель может оказаться намного проще чем функция, которую она должна аппроксимировать. В этом случае возникает проблема недообучения. Тогда обобщающая способность модели оказывается неприемлемо низкой.

I> **Аппроксимация** — это замена сложного объекта более простыми. При этом важные аспекты заменяющего и исходного объекта похожи. **Аппроксимация функции** означает замену исходной функции на более простую. При этом её поведение похоже на исходную.

Обучение без учителя решает проблему трудоёмкой подготовки обучающих данных. Алгоритмы этой парадигмы принимают на вход **неразмеченный набор данных**. Каждый обучающий пример в нём содержит только вектор входных значений X = (x~1~, x~2~,..., x~n~).

Возникает вопрос: как алгоритм обучения понимает, что надо делать с неразмеченными данными? Построенные в рамках этой парадигмы модели работают иначе, чем при обучении с учителем. Они решают совершенно другие практические задачи. Алгоритм обучения для создания этих моделей тоже отличается.

Обучение без учителя решает следующие задачи:

* **Кластеризация** (clustering) — объединение объектов в непересекающиеся группы на основе близости их признаков.

* **Обнаружение аномалий** (anomaly detection) — поиск редких элементов или наблюдений, которые значительно отклоняются от остальных данных и не соответствуют представлению о нормальном поведении.

* [**Понижение размерности**](https://wiki.loginom.ru/articles/data-reduction.html) (data reduction) — преобразование данных в более удобную форму для их анализа и интерпретации.

* [**Поиск ассоциативных правил**](https://en.wikipedia.org/wiki/Association_rule_learning) (association rule learning) — обнаружение неявных закономерностей в больших наборах данных.

* **Подготовка обучающих наборов для обучения с учителем** — это автоматизация ручной разметки данных.

Рассмотрим алгоритмы и модели обучения без учителя на примере кластеризации.

##### 2.6.2.2.1 Кластеризация

Задача кластеризации заключается в том, чтобы разбить множество объектов на группы. В каждую попадают объекты со сходными признаками. Есть важное условие: полученные группы не должны пересекаться. Другими словами, каждый объект должен относится только к одной группе или кластеру.

I> **Кластером** (cluster) называется набор объектов, схожих по какому-то признаку.

Задачу кластеризации решают модели, обученные без учителя. Это означает, что правило группирования объектов заранее неизвестно или не задаётся. Алгоритм обучения должен сам вывести это правило и заложить его в модель. Для этого он ориентируется на закономерности во входных данных.

Формат обучающего набора данных отличается для алгоритмов обучения с учителем и без. В случае обучения без учителя каждый пример набора состоит только из вектора входных значений X. Элементы вектора x~1~, x~2~,..., x~n~ — это признаки некоторого объекта, который надо отнести к той или иной группе. В обучающем примере нет вектора выходных значений Y, которые ожидаются от модели.

Рассмотрим задачу кластеризации на примере. Предположим, что онлайн магазину нужно сгруппировать пользователей по интересам. Тогда для каждого пользователя можно будет показывать таргетированную рекламу новых товаров.

Магазин ведёт такую статистику: сколько раз каждый пользователь просматривал какие категории товаров. Для простоты предположим, что в магазине есть только две категории: велозапчасти и автозапчасти.

Алгоритм обучения должен построить модель, которая относит пользователя к некоторому классу. В зависимости от класса сайт магазина выбирает, какую рекламу показывать пользователю. Варианты могут быть следующие:

* Показывать рекламу только велозапчастей.
* Показывать рекламу только автозапчастей.
* Показывать рекламу и велозапчастей, и автозапчастей.
* Не показывать рекламу велозапчастей и автозапчастей.

Идея в том, чтобы пользователи получали рекламу только интересной им категории товаров. Пользователи, которые просматривали велозапчасти, должны получать рекламу новых велозапчастей. То же самое для пользователей, которые когда-то выбирали автозапчасти, — они получат рекламу новых автозапчастей. Рекламу товаров обеих категорий получат те, кому они обе интересны. Новые пользователи сайта без собранной по ним статистики таргетированную рекламу не получат.

Применим алгоритм обучения для задачи кластеризации под названием [**метод k-средних**](https://en.wikipedia.org/wiki/K-means_clustering) (k-means). Среди прочих параметров этому алгоритму нужно указать число ожидаемых кластеров. В нашем примере это число равно четырём. Именно на столько групп мы собираемся разделить всех пользователей.

Метод k-средних находит геометрический центр каждого кластера на пространстве признаков. [**Геометрический центр**](https://ru.wikipedia.org/wiki/Барицентр) (centroid) множества точек — это среднее арифметическое их координат. То есть координата геометрического центра по каждой оси равна сумме координат всех точек по этой оси, разделённая на количество точек.

Метод k-средних не строит модель для прогнозов, как это делают алгоритмы обучения с учителем. Вместо этого он организует уже имеющиеся данные. Если в исходном множестве появляется новый объект, его надо добавить в обучающий набор данных и перезапустить алгоритм обучения сначала.

Представим результат работы метода k-средних для нашей задачи на графике. У каждого пользователя мы учитываем только два признака: посещения двух категорий товаров. Поэтому пространство признаков на графике будет двумерным. Его демонстрирует иллюстрация 2-20.

{caption: "Иллюстрация 2-20. Результат кластеризации пользователей онлайн магазина", height: "50%", width: "100%"}
![Кластеризация пользователей](images/ArtificialIntelligence/online-shop-clustering.png)

Горизонтальная ось показывает количество посещений категории товаров автозапчасти. Вертикальная ось — количество посещений категории велозапчасти. Серые круги обозначают центры кластеров, которые нашёл метод k-средних. Ближайшие к этим центрам точки попали в соответствующий кластер. Относящиеся к одному кластеру точки имеют один цвет.

Метод k-средних разбил всех пользователей онлайн магазина на четыре кластера:

* Жёлтые точки — пользователи с небольшим числом посещений обеих категорий товаров. Чтобы судить об их предпочтениях, собрано недостаточно статистики.

* Синие точки — пользователи, которые интересуются автозапчастями. Им нужно показывать рекламу новинок этой категории товаров.

* Фиолетовые точки — пользователи, которые интересуются велозапчастями. Им нужно показывать рекламу новинок этой категории товаров.

* Зелёные точки — пользователи, которые интересуются обеими категориями товаров. Им нужно показывать рекламу всех новинок онлайн магазина.

Благодаря кластеризации, онлайн магазин может учесть предпочтения каждого постоянного пользователя. Для него сайт будет показывать только актуальную рекламу.

Если у магазина появится новый пользователь, по нему надо собрать статистику: какие категории товаров он чаще посещает. Затем эти данные надо добавить в обучающий набор и повторить алгоритм k-средних. Тогда предпочтения пользователя попадут в результат кластеризации. На основании этого результата алгоритм сайта магазина выберет подходящую рекламу.

Приложение 5.1.3 подробно описывает Python скрипт для кластеризации методом k-средних. В этом примере обучающий набор данных сгенерирован случайным образом. Скрипт также доступен на [Github странице проекта](https://github.com/ellysh/ai-in-strategy-games/blob/master/manuscript/resources/code/ArtificialIntelligence/k-mean-clustering.py).

#### 2.6.2.3 Обучение с подкреплением

Мы рассмотрели алгоритмы обучения с учителем и без. Они получают на вход обучающий набор данных в том или ином формате. В парадигме обучения с подкреплением таких данных нет. В этом её принципиальное отличие.

Что означает термин "подкрепление"? Его ввёл российский физиолог Иван Петрович Павлов в начале XX века. Подкреплением он назвал стимул, который усиливает модель поведения у животных. Психологи расширили термин "подкрепление". Они обратили внимание на то, что модель поведения можно не только усиливать, но и ослаблять. Важная особенность подкрепления в том, что после устранения стимула животное сохраняет выученное им поведение. В машинном обучении "подкрепление" означает использование сигналов обратной связи. Они направляют процесс обучения.

В обучении с подкреплением применяют терминологию теории интеллектуальных агентов. Эта парадигма обучения работает с агентами и процессами принятия решений в интерактивных и динамических средах. Теория интеллектуальных агентов предлагает подходящую основу для моделирования таких систем и процессов.

Начнём с принципиального отличия обучения с подкреплением от других парадигм. Рассмотрим задачи этих парадигм с точки зрения теории интеллектуальных агентов.

Алгоритмы обучения с учителем и без учителя создают модели, которые работают в эпизодических средах. Эти алгоритмы получают на вход несвязанные между собой примеры данных. Готовый агент обрабатывает такие же несвязанные примеры новых данных. В этом случае его прошлые решения никак не влияют на последующие. Агенту не надо учитывать долгосрочные последствия своих действий. Поэтому алгоритм обучения просто игнорирует эти зависимости. Эта особенность приводит к тому, что готовый агент не способен работать в последовательной среде. В такой среде каждое решение агента имеет долгосрочные последствия. Чтобы их учесть, нужна иная парадигма обучения. Именно такие задачи решает обучение с подкреплением.

Рассмотрим, как работает обучение с подкреплением. Главные участники этого процесса — агент, среда и алгоритм обучения. Шаги обучения следующие:

1. Агент выполняет действие. В начале обучения у него нет никаких инструкций. Поэтому агент выбирает действие случайно.

2. Среда реагирует на действие агента переходом в новое состояние.

3. Новое состояние среды определяет сигнал вознаграждения, который агент получает за своё действие. Сигнала вознаграждения — это некоторое числовое значение.

4. Алгоритм обучения обрабатывает сигнал вознаграждения. Он корректирует стратегию агента, исходя из значения этого сигнала. Таким образом алгоритм обучения определяет, какие решения принимает агент.

5. После корректирования стратегии, агент выполняет следующее действие. То есть он возвращается к первому шагу процесса обучения.

Цель агента следующая: изучить стратегию, которая максимизирует совокупное вознаграждение. **Совокупное вознаграждение** (cumulative reward) — это сумма всех вознаграждений за выполненные агентом действия. Алгоритм обучения служит агенту инструментом для достижения цели. Благодаря обратной связи, алгоритм соотносит положительные результаты с определёнными действиями. Так он совершенствует стратегию агента, тем самым повышая совокупное вознаграждение.

Кроме агента, среды и алгоритма обучения есть ещё четыре элемента:

* **Политика** (policy) или **стратегия** означает функцию агента. Она определяет действие агента, как ответ на конкретное наблюдаемое состояние среды. По сути, политика представляет собой план действий в данной среде.

* **Сигнал вознаграждения** (reward signal) — это скалярное значение, которое среда передаёт агенту в качестве обратной связи. Величина сигнала зависит от результата действия, которое выполнил агент. Положительное значение указывает на благоприятный результат, а отрицательное — на нежелательный.

* **Функция полезности** (value function) оценивает совокупное вознаграждение агента в разных ситуациях. Различают две функции полезности:

    * **Функция полезности состояния** (state value function) оценивает ожидаемое будущее вознаграждение агента. Агент может его получить, если начнёт действовать с состояния среды s и будет следовать определённой политике π. Функция количественно определяет желательность пребывания в некотором состоянии среды. Она обозначается как `V(s)`.

    * **Функция полезности действия** (action value function) оценивает вознаграждение агента за совершение определённого действия a в состоянии среды s и дальнейшее следование политике π. Функция количественно определяет ценность выполнения конкретного действия в некотором состоянии среды. Она обозначается как `Q(s, a)`.

* **Модель среды** (model) — это внутреннее представление агента для прогнозирования поведения среды. Модель помогает агенту эффективно планировать свои действия. Есть два типа моделей:

    * **Модель переходов** (transition model) — исходя из текущего состояния среды и действия агента прогнозирует следующее состояние и связанное с ним вознаграждение.

    * **Модель вознаграждений** (reward model) — предсказывает ожидаемое вознаграждение с учётом текущего состояния среды и действия агента.

I> Обратите внимание на различие между терминами "модель среды" и "обучаемая модель". Последнюю называют "агентом" в терминологии обучения с подкреплением.

Модель вознаграждений строится на функции вознаграждений. **Функция вознаграждений** (reward function) принимает на вход пару значений: состояние среды и выбранное агентом действие. Она возвращает число, которое соответствует немедленному вознаграждению за выполнение выбранного действия в указанном состоянии. Эта функция обозначается как `R(s, a)`.

Функции полезности и вознаграждения работают с одними и теми же входными параметрами. Они возвращают похожий результат: значение вознаграждения. Тем не менее они различаются. Функция полезности определяет долгосрочную перспективу действий агента. Она учитывает вознаграждения от будущих состояний среды, которые станут доступны после выбранного действия агента в указанном состоянии среды. Функция вознаграждения определяет немедленное вознаграждение за действие в указанном состоянии. Она не учитывает долгосрочные последствия.

I> **Немедленное вознаграждение** (immediate rewards) — это вознаграждение, которое агент получает сразу после выполнения определённого действия.

В обучении с подкреплением есть два принципиально разных подхода:

* **Основанный на модели** (model-based).

* **Без модели** (model-free).

Наличие модели — это характеристика и агента, и алгоритма обучения, который его строит.

В подходе, основанном на модели, агент поддерживает внутреннюю модель среды. Она позволяет предсказать реакцию среды на возможные действия. Агент использует модель для планирования и принятия решений. Прежде чем действовать он рассматривает будущие состояния среды и совокупные вознаграждения за их достижение.

В подходе без модели агент не поддерживает модель среды в явном виде. Он учится непосредственно на взаимодействии с окружающей средой методом проб и ошибок. Агент оценивает только наблюдаемые последствия своих действий и не моделирует будущие состояния среды.

У каждого подхода есть свои преимущества и недостатки. Вот сильные стороны агентов, основанных на модели:

1. Они требуют меньше итераций алгоритма обучения.

2. Планирование действий заранее полезно в средах с поглощающими состояниями. Из этих состояний агент не может выбраться.

3. Модель улучшает обобщающую способность агента. Он способен делать прогнозы в ситуациях, с которыми раньше никогда не сталкивался.

I> **Поглощающее состояние** (absorbing state) — это состояние среды, из которого агент не может выйти. Поглощающее состояние отличается от терминального. В **терминальном состоянии** агент больше не может выполнять какие-либо действия. Например, в шахматной партии мат приводит к терминальному состоянию. В поглощающем состоянии агент может действовать, но ни одно из его действий не меняет состояние среды.

Слабые стороны агентов, основанных на модели, следующие:

1. Обновлять внутреннюю модель и использовать её дорого с точки зрения вычислений.

2. Точность модели очень важна. Любые неточности в ней приводят к неоптимальным действиям агента.

3. Основанный на модели агент предполагает, что закономерности среды ему известны. Если реальные закономерности оказались сложнее чем модель или в поведении среды много случайностей, агент не сможет эффективно действовать.

Достоинства агентов без модели:

1. Они концептуально проще и просты в реализации.

2. Справляются со сложными средами, в поведении которых есть случайность.

Недостатки агентов без модели:

1. Требуют больше итераций алгоритма обучения.

2. Обобщающая способность таких агентов хуже. Они плохо справляются с ситуациями, с которыми раньше не сталкивались.

3. Полагаются на метод проб и ошибок в процессе обучения. Этот метод оказывается неэффективен в некоторых средах.

Что такое поиск методом проб и ошибок? Это важная концепция, поэтому остановимся на ней подробнее. Прежде всего, так называется врождённый метод мышления человека и животных. Он заключается в повторении различных действий до тех пор, пока задача не будет решена.

В общем виде **метод проб и ошибок** состоит из следующих шагов:

1. Выполнить действие или применить некоторое решение.

2. Наблюдать за результатом. Другими словами, получить обратную связь.

3. Оценить успешность действия или решения.

4. Скорректировать действие или решение на основе обратной связи.

Шаги метода зависят от решаемой задачи. В машинном обучении метод проб и ошибок применяется как к отдельным действиям агента, так и к стратегии целиком. В первом случае "наблюдаемый результат" — это немедленное вознаграждение. Во втором случае — совокупное вознаграждение.

Метод проб и ошибок имеет ряд преимуществ. Он прост в реализации, подходит для решения многих задач и часто не требует значительных вычислений. С другой стороны, недостатки метода в том, что он работает медленно и находит первое сработавшее решение, а не оптимальное.

Оба подхода обучения с подкреплением (основанный на модели и без модели) сталкиваются с [**дилеммой исследования и эксплуатации**](https://en.wikipedia.org/wiki/Exploration-exploitation_dilemma). Разберёмся с этой концепцией по порядку. Начнём с определений исследования и эксплуатации.

**Исследование** (exploration) — это выполнение неопробованных действий в незнакомой среде. Цель таких действий — получить информацию о ещё неизвестных аспектах среды. Исследование позволяет обнаружить потенциально лучшие действия и стратегии, которые дают высокие совокупные вознаграждения в долгосрочной перспективе. Исследование приводит к низкому немедленному вознаграждению из-за неоптимальных действий в краткосрочной перспективе.

**Эксплуатация** (exploitation) — это выполнение действий с уже известным результатом. Такие действия приносят высокое немедленное вознаграждение. Эксплуатация неоптимальна в долгосрочной перспективе. Если агент выбирает всегда её, он не обнаружит более эффективные действия и стратегии.

Саму дилемму можно сформулировать так:

> Имея ограниченные ресурсы, в какой пропорции агенту следует их тратить на исследование и эксплуатацию, чтобы максимизировать совокупное вознаграждение?

Другими словами, перед каждым действием агент выбирает между:

* Гарантированное высокое немедленное вознаграждение.

* Потенциальное высокое совокупное вознаграждение в долгосрочной перспективе.

Дилемму исследования и эксплуатации хорошо иллюстрирует проблема многорукого бандита. [**Проблема многорукого бандита**](https://en.wikipedia.org/wiki/Multi-armed_bandit) (multi-armed bandit problem) — это классическая задача обучения с подкреплением. Проблема получила своё название из аналогии с игроком, который стоит перед рядом игровых автоматов типа однорукий бандит.

I> Одноруким бандитом называется [слот-машина](https://ru.wikipedia.org/wiki/Слот-машина) с рычагом и барабаном. Пользователь должен бросить монету и дёрнуть за рычаг. Тогда запускается барабан, который при остановке показывает комбинацию символов. Некоторые комбинации выигрышные. При их выпадении пользователь получает сумму, превышающую размер ставки.

Суть проблемы многорукого бандита следующая. В распоряжении агента есть несколько игровых автоматов и ограниченное число монет. У каждого автомата своё соотношение затрат к выигрышу. Изначально агент их не знает. Он должен сыграть несколько раз на одном автомате, чтобы узнать его соотношение затрат к выигрышу. Задача агента — сыграть на автоматах на все имеющиеся у него монеты и получить максимально возможный выигрыш. Другими словами, он должен максимизировать своё совокупное вознаграждение.

В процессе решения задачи агент сталкивается с дилеммой исследования и эксплуатации. Перед каждым действием он вынужден принимать следующие решения:

1. На каком автомате ему сейчас играть?

2. Сколько раз играть на выбранном автомате?

3. В какой последовательности переходить между автоматами?

Общего решения для задач этого класса найти ещё не удалось. Но учёные уже разработали оптимальные стратегии для частных случаев. Кроме этого, есть ряд универсальных приблизительных стратегий. Они дают не наилучший, но приемлемый результат.

##### 2.6.2.3.1 Реализация обучения с подкреплением

Есть несколько подходов к реализации алгоритмов обучения с подкреплением. В зависимости от подхода все алгоритмы можно разделить на следующие три категории. 

1. **Алгоритмы, основанные на стратегии** (policy-based), явно строят стратегию агента. Она представляет собой некоторую структуру данных, которая доступна на протяжении всего процесса обучения агента и его применения для решения задачи. Некоторые policy-based алгоритмы обучения не используют функцию полезности. Другие применяют её для повышения точности финальной стратегии. Policy-based алгоритмы обучения строят стратегию одного из двух видов:

    * **Детерминированная стратегия** (deterministic policy) — предписывает агенту выполнять одни и те же действия в каждом состоянии среды.

    * **Вероятностная (стохастическая) стратегия** (stochastic policy) — агент выбирает случайно одно из нескольких возможных действий в каждом состоянии среды.

2. **Алгоритмы, основанные на полезности** (value-based), рассчитывают оценку различных состояний среды и пар "состояние-действие". Они не строят стратегию агента явно. Вместо этого алгоритмы находят функцию полезности. Из неё неявно выводится стратегия агента: в каждом состоянии среды он выбирает действие, ведущее к максимальному совокупному вознаграждению.

3. **Алгоритмы, основанные на модели** (model-based), явно строят модель поведения среды. Затем агент использует эту модель для прогнозирования будущих состояний среды, переходов между ними и потенциальных вознаграждений. На основе таких прогнозов агент принимает решения.

Рассмотрим пример обучения с подкреплением. Построим агента для игры в крестики-нолики. Для решения задачи применим алгоритм обучения Q-learning.

[**Q-learning**](https://en.wikipedia.org/wiki/Q-learning) — это алгоритм без модели, основанный на полезности. Он ищет оптимальную функцию полезности действия `Q(s, a)`. Её результат называется Q-значением. **Q-значение** — это совокупное вознаграждение, которое агент может получить за совершение действия a в состоянии среды s и дальнейшее следование политике π.

Алгоритм Q-learning выглядит так:

1. **Инициализация**. Для всех возможных пар "состояние-действие" назначить произвольное Q-значение. Обычно это ноль.

2. **Выбор действия**. Агент выбирает своё следующее действие, согласно некоторой стратегии исследования и эксплуатации. В простейшем случае это может быть смешивание шагов исследования и эксплуатации в некоторой пропорции.

3. **Выполнение действия**. В состоянии среды s агент выполняет действие a. В результате он получает немедленное вознаграждение r. При этом состояние среды меняется на новое s′.

4. **Рассчитать новое Q-значение**. Агент рассчитывает новое Q-значение для пары "состояние-действие" (s, a).

5. **Повторение** шагов 2-4.

Формула расчёта нового Q-значения выглядит так:
{width: "100%"}
![](images/ArtificialIntelligence/q-learning-formula.png)

Обозначения в формуле следующие:

* s — текущее состояние среды.

* a — действие, которое выполняет агент в состоянии среды s.

* s′ — новое состояние среды, к которому приводит действие a в состоянии s.

* r — немедленное вознаграждение за выполнение действия a в состоянии s.

* Q(s,a) — текущее Q-значение для пары "состояние-действие" (s,a).

* α — параметр обучения под названием **скорость обучения** (learning rate). Он определяет, в какой степени обновлять текущее Q-значение для пары "состояние-действие" (s,a).

* max ​Q(s′,a′) — максимальное Q-значение за выполнение лучшего действия a′ в новом состоянии среды s′.

* γ — параметр обучения под названием **коэффициент дисконтирования** (discount factor). Он определяет важность потенциальных будущих вознаграждений, доступных из состояния s′.

Применим алгоритм Q-learning для обучения агента игре в крестики-нолики. Для этого выполним следующие шаги:

1. **Создать модель среды**. Эта модель должна включать в себя: структуру игрового поля 3х3, порядок и правила ходов, правила победы и ничьи.

2. **Подготовить таблицу Q-значений агента**. Она хранит Q-значения для всех возможных пар "состояние-действие". Все значения этой таблицы в начале инициализируются нулями.

4. **Определить параметры обучения**. Параметры следующие: скорость обучения (learning rate), коэффициент дисконтирования (discount factor), соотношение исследования и эксплуатации.

5. **Обучение агента**. Два агента A и B играют против друг друга в крестики-нолики. Каждая такая игра называется **эпизодом** (episode). Агенты выбирают действия согласно своей текущей стратегии, то есть таблицы Q-значений. По окончании каждой игры агенты проходят по истории ходов и обновляют свои таблицы Q-значений согласно результату.

6. **Тестирование**. Агенты A и B играют друг с другом несколько партий. В этом случае они следуют готовой стратегии, полученной в результате обучения. Она должна быть оптимальной и вести либо к победе, либо к ничьей.

На самом деле мы обучаем не одного агента, а сразу двух. Такой подход удобнее в реализации. После завершения алгоритма обучения каждый агент будет иметь свою собственную стратегию игры.

Приложение 5.1.4 подробно описывает Python скрипт для обучения агента игре в крестики-нолики алгоритмом Q-learning. Скрипт также доступен на [Github странице проекта](https://github.com/ellysh/ai-in-strategy-games/blob/master/manuscript/resources/code/ArtificialIntelligence/q-learning.py).

Какие алгоритмы обучения с подкреплением наиболее популярны? Среди алгоритмов, основанных на стратегии, чаще всего применяют следующие:

* [**REINFORCE**](https://saturncloud.io/glossary/reinforce-algorithm/) или **Monte Carlo Policy Gradient** — фундаментальный policy-based алгоритм. Он напрямую оптимизирует эффективность стратегии. Для этого алгоритм корректирует её параметры в направлении увеличения совокупного вознаграждения. На нём основаны многие policy-based алгоритмы, которые исправляют его недостатки.

* **Proximal Policy Optimization** (PPO) — алгоритм оптимизации на основе градиента, который обучает нейронную сеть. Она называется **сеть стратегии** (policy network). Сеть принимает на вход состояние среды и выдаёт распределение вероятностей для возможных действий. Агент выбирает действие, имеющее наибольшую вероятность.

* **Trust Region Policy Optimization** (TRPO) — алгоритм оптимизации на основе градиента, который ограничивает изменения в стратегии. Для этого на каждой итерации он контролирует расхождения между старой и новой стратегиями. Эти расхождения должны находится в доверительных пределах.

Популярны следующие алгоритмы, основанные на полезности:

* **Q-learning** — фундаментальный value-based алгоритм. Он служит основой для многих других алгоритмов этой категории.

* **Deep Q Network** (DQN) — это модификация алгоритма Q-learning, в которой применяется глубокое обучение (deep learning). Нейронная сеть используется для аппроксимации функции полезности действия. Этот алгоритм применяется для обучения агентов, которые должны действовать в сложных средах с большим числом состояний.

* **Double Q-learning** — это модификация алгоритма Q-learning, которая решает его проблему с переоценкой действий. Суть проблемы в том, что в формуле Q-learning используется максимальное Q-значение за выполнение лучшего действия в новом состоянии среды `max ​Q(s',a')`. В результате в процессе обучения агент склонен выбирать действие с максимальной оценкой, даже если она слишком оптимистична и выше реальной. Double Q-learning алгоритм разделяет механизмы оценки и выбора действия во время обучения.

* [State-Action-Reward-State-Action](https://en.wikipedia.org/wiki/State–action–reward–state–action) (SARSA) — это вариация алгоритма Q-learning. В ней Q-значение для каждой пары "состояние-действие" обновляется на основе действия, которое будет выполнено в новом состоянии среды согласно текущей стратегии. В Q-learning это обновление происходит на основе действия, которое даст максимальное Q-значение в краткосрочной перспективе.

Алгоритмы, основанные на модели, не пользуются большой популярностью из-за своих недостатков. Обычно они применяются для относительно простых задач.

{pagebreak}
