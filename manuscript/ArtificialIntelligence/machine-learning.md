## 2.6 Машинное обучение

Мы рассмотрели методы формальной логики и алгоритмы поиска, которые применяются в программах интеллектуальных агентов. Это относительно простые способы их реализации.

Теперь мы познакомимся с подходами, разработанными в рамках машинного обучения. Они применяются в программах обучающихся агентов.

Машинное обучение — очень сложная тема. Чтобы в ней разобраться, нужна хорошая техническая подготовка. Мы познакомимся с основными принципами машинного обучения на примере простейших моделей класса shallow learning. Их достаточно легко понять без специальных знаний.

Более сложные модели мы рассмотрим в следующей главе, посвящённой шахматным программам.

### 2.6.1 Механизм обучения

Начнём с основных терминов, которые применяются в машинном обучении.

**Механизм обучения** — это набор методов, который автоматически создаёт программу интеллектуального агента. Любой механизм обучения состоит из двух компонентов:

* **Алгоритм обучения**
* [**Обучаемая модель**](https://wiki.loginom.ru/articles/taught-model.html)

Чтобы воспользоваться механизмом обучения, нужен [**обучающий набор данных**](https://wiki.loginom.ru/articles/training-set.html) (training set). Этот набор состоит из [**обучающих примеров**](https://wiki.loginom.ru/articles/training-sample.html) (training sample).

Алгоритм обучения получает на вход обучающий набор данных. У алгоритма две основные задачи:

* Найти закономерности в обучающем наборе данных.

* Построить обучаемую модель, которая учитывает все найденные закономерности.

Есть несколько типов обучаемых моделей. Для каждого типа применяется свой алгоритм обучения. Алгоритм обучения определяет характеристики обучающего набора данных. Эту зависимость демонстрирует иллюстрация 2-18.

{caption: "Иллюстрация 2-18. Компоненты машинного обучения", height: "30%"}
![Компоненты машинного обучения](images/ArtificialIntelligence/machine-learning.png)

По принципу работы алгоритмы обучения делятся на три класса:

* [**Обучение с учителем**](https://ru.wikipedia.org/wiki/Обучение_с_учителем) или контролируемое (supervised learning)

* [**Обучение без учителя**](https://ru.wikipedia.org/wiki/Обучение_без_учителя) или неконтролируемое (unsupervised learning)

* [**Обучение с подкреплением**](https://ru.wikipedia.org/wiki/Обучение_с_подкреплением) (reinforcement learning)

Каждый класс представляет собой отдельную парадигму. **Парадигма** — это система предположений, концепций и практических приёмов для решения какой-то задачи.

Один и тот же алгоритм обучения можно представить в разных формах:

* В виде математических формул.
* В виде псевдокода.
* В виде запускаемой программы.
* В виде программной библиотеки.

Форму представления выбирают исходя из задачи. Первые две формы чаще применяют в образовательных целях. Последние две — на практике.

Обучаемая модель хранит результаты процесса обучения. Она состоит из правил и наборов данных, которые необходимы для прогнозирования. По сути, модель — это программа. Она имеет данные и машинные инструкции для их обработки. Инструкции выполняют действия над данными. В результате этих действий модель даёт прогноз для новых наборов данных. Эти наборы представляют собой восприятие агента в процессе работы.

Обучаемая модель состоит из двух компонентов:

* **Данные модели**
* **Алгоритм для прогнозирования**

Цель машинного обучения — автоматически построить обучаемую модель. Алгоритм обучения — только средство для достижения этой цели. Он важен, поскольку влияет на качество обучаемой модели. Но конечная программа агента никак не использует алгоритм обучения.

### 2.6.2 Алгоритмы обучения

Познакомимся с парадигмами машинного обучения. Их демонстрирует таблица 2-21. В таблице приводятся также задачи, решаемые в рамках каждой парадигмы.

{caption: "Таблица 2-21. Парадигмы алгоритмов обучения", width: "100%"}
| Парадигма обучения | Задачи |
| --- | --- |
| Обучение с учителем | Регрессия |
|  | Классификация |
|  | Структурный вывод |
|  | |
| Обучение без учителя | Кластеризация |
|  | Обнаружение аномалий |
|  | Сокращение размерности |
|  | Поиск ассоциативных правил |
|  | |
| Обучение с подкреплением | Машинный перевод |
|  | Задача управления |

Рассмотрим несколько типичных задач в качестве примеров. С их помощью разберём каждую парадигму обучения.

#### 2.6.2.1 Обучение с учителем

Алгоритмы обучения с учителем принимают на вход **размеченные данные**. В терминологии интеллектуальных агентов **размеченный обучающий пример** означает следующее:

> Обучающий пример содержит акт восприятия агента и ожидаемое действие на него.

В этой же терминологии алгоритм обучения работает так:

> Алгоритм принимает на вход размеченные данные. Эти данные содержат примеры работы желаемой функции агента. В результате обучения алгоритм строит модель. Она реализует функцию агента, поведение которой близко к желаемой.

Теперь перейдём к терминологии, принятой в машинном обучении. Она более точна, но в то же время сложнее для понимания. Для начала дадим определение размеченному обучающему примеру:

> Обучающий пример представляет собой пару векторов X = (x~1~, x~2~,..., x~n~) и Y = (y~1~, y~2~,..., y~n~). X – это вектор входных значений, а Y – вектор выходных значений. Вектор Y должна вернуть обучаемая модель при получении на вход вектора X.

I> [**Вектором**](https://ru.wikipedia.org/wiki/Вектор_(математика)#Вектор_как_последовательность) в машинном обучении называют упорядоченный набор чисел.

Рассмотрим алгоритм обучения с учителем в общем виде. Его шаги следующие:

1. Алгоритм передаёт на вход модели вектор входных значений X очередного примера из обучающего набора.

2. В ответ на входные значения модель возвращает вектор выходных значений Y'.

3. Алгоритм обучения сравнивает вектора Y из обучающего примера и Y', полученный от модели.

4. Если вектора Y и Y' не совпадают, это называется [**ошибкой обучения**](https://wiki.loginom.ru/articles/training-error.html) (training error). Чтобы её скорректировать, алгоритм обучения автоматически изменяет параметры модели.

5. Если вектора Y и Y' совпали, алгоритм обучения переходит к следующему примеру из обучающего набора.

При обучении модели могут возникнуть проблемы. Вот некоторые из них:

1. Низкая производительность модели (model performance).

2. Переобучение (overfitting).

3. Недообучение (underfitting)

4. Плохая обобщающая способность модели (generalization ability).

Рассмотрим их по порядку.

Когда обучаемая модель готова, её работу оценивают с помощью размеченного [**тестового набора данных**](https://wiki.loginom.ru/articles/test-set.html) (test set). Эти данные не входят в обучающий набор. Поэтому модель ещё никогда с ними не встречалась. Для каждого **тестового примера** модель возвращает вектор выходных значений Y'. Если он не совпадает с вектором Y из примера, это называется [**ошибкой обобщения**](https://wiki.loginom.ru/articles/generalization-error.html) (generalization error). При высокой частоте таких ошибок говорят, что **производительность** (performance) или **точность** (accuracy) модели низка.

I> Производительность модели — это более широкий термин, чем точность. Он включает в себя различные показатели для оценки того, насколько хорошо модель справляется с конкретной задачей.

Низкая производительность модели — это сигнал о том, что процесс обучения завершился неудачно. Это может произойти по ряду причин. Самая распространённая из них называется проблемой недообучения. [**Недообучение**](https://wiki.loginom.ru/articles/underfitting.html) (underfitting) возникает, когда модель оказалась слишком простой. Она не способна отследить закономерности в обучающем наборе данных.

Причины недообучения могут быть следующие:

1. Обучающий набор данных слишком мал.

2. Конфигурация модели не соответствует задаче.

3. Выбран неподходящий алгоритма обучения.

4. Малое число итераций алгоритма обучения.

5. Шум в обучающем наборе данных.

I> **Шумом** в обучающих данных называются случайные изменения. Они не отражают основные закономерности, для изучения которых предназначена модель.

Чтобы решить проблему недообучения, есть [ряд методов](https://machinelearningmastery.com/improve-deep-learning-performance/). Подходящий метод выбирается исходя из причины проблемы. В результате исправления может поменяться конфигурация модели. Так модель подгоняется под обучающий набор данных. Чрезмерная подгонка приводит к переобучению. Проблема [**переобучения**](https://wiki.loginom.ru/articles/overtraining.html) (overfitting или overtraining) возникает, когда модель слишком хорошо изучает обучающий набор данных. В этом случае она начинает реагировать на шум и случайные отклонения в данных вместо основных закономерностей. Как следствие, модель хорошо работает на обучающем наборе данных, но часто ошибается на тестовых и реальных данных.

Переобучение — это одна из возможных причин плохой обобщающей способности модели. [**Обобщающая способность**](https://wiki.loginom.ru/articles/generalization-ability.html) (generalization ability) — это эффективность работы модели с новыми данными, с которыми она не встречалась в процессе обучения. Чтобы работать с ними, модель должна применять закономерности, извлечённые из обучающего набора данных. Низкая обобщающая способность говорит о том, что алгоритм обучения не смог извлечь эти закономерности.

Обобщающая способность даёт лишь примерную оценку того, как модель будет справляться с реальной задачей. Более точно это определяет её предсказательная способность. **Предсказательной способностью** (predictive ability) называется эффективность модели при решении задачи, для которой она была разработана. Для измерения этой эффективности есть много показателей. Какие из них применять, зависит от конкретной модели и решаемой ею задачи.

Чтобы оценить предсказательную способность модели, её проверяют на [**валидационном наборе данных**](https://wiki.loginom.ru/articles/validation-set.html) (validation set). Примеры этого набора не входят ни в обучающий, ни в тестовый набор. Такая проверка модели называется [**валидацией**](https://help.loginom.ru/userguide/processors/validation.html) (validation).

Обучение с учителем эффективно для решения следующих задач:

* **Классификация** (classification) — определение категории, к которой принадлежит некоторый объект.

* **Регрессия** (regression) — предсказание числового значения по входным данным.

* **Структурный вывод** (structured prediction) — порождение вектора выходных значений, между элементами которого существуют связи и зависимости.

Рассмотрим подробнее первые две задачи на примерах. Они дадут общее представление об алгоритмах обучения с учителем и построенных ими моделях.

##### 2.6.2.1.1 Классификация

Задача классификации — определить класс объекта по его параметры. Параметры объекта в терминологии машинного обучения называются **признаками**. Например, объект автомобиль имеет следующие признаки: габариты, мощность двигателя, расход топлива и т.д.

Иллюстрация 2-18 демонстрирует зависимость характеристик обучающего набора данных от класса алгоритма обучения и типа модели. Эта зависимость выполняется для задачи классификации. Чтобы построить подходящую модель, нужен специальный формат обучающих данных. Каждый пример такого набора состоит из двух векторов:

1. Вектор входных значений X содержит все проверяемые признаки конкретного объекта: x~1~, x~2~,..., x~n~.

2. Вектор Y содержит единственное выходное значение y. Это номер класса, к которому относится объект из примера. Номер класса также называется **меткой**.

Обученная модель работает по следующему принципу. Она получает на вход вектор признаков объекта X. На выходе модель возвращает вектор Y'. Он содержит единственное дискретное значение y' с номером класса объекта.

Для демонстрации задачи классификации есть каноничный пример. Это набор данных под названием [**ирисы Фишера**](https://ru.wikipedia.org/wiki/Ирисы_Фишера). Его составил английский статистик и биолог Рональд Фишер. С помощью набора учёный продемонстрировал свой новый метод статистического анализа в 1936 году.

Набор данных Фишера содержит информацию о цветках различных видов ирисов. Он представляет собой таблицу со следующей информацией:

1. Длина чашелистика (sepal) цветка.
2. Ширина чашелистика (sepal) цветка.
3. Длина лепестка (petal) цветка.
4. Ширина лепестка (petal) цветка.
5. Вид ириса.

Таблица содержит примеры для трёх видов ирисов. Для каждого вида есть 50 примеров с разными параметрами цветков. Всего в таблице 150 записей.

Мы построим модель для определения вида ириса по параметрам цветка. Для простоты будем учитывать только два параметра: длина и ширина чашелистика. Наша модель будет определять, относится ли растение к виду Iris setosa или нет.

Проходя по набору данных ирисы Фишера, алгоритм обучения строит модель для классификации. В простейшем случае это формула для расчёта выходного значения y по вектору входных значений X. Модели такого типа называются [**линейными классификаторами**](http://www.machinelearning.ru/wiki/index.php?title=Линейный_классификатор).

Формула линейного классификатора в общем виде выглядит так:
{width: "25%"}
![](images/ArtificialIntelligence/linear-classifier-formula.png)

Обозначения в формуле следующие:

* y — выходное значение модели.
* f — пороговая функция.
* w~i~ — вес признака под номером i.
* x~i~ — значение признака под номером i.

**Пороговая функция** f отображает все возможные суммы (w~i~ * x~i~) на два дискретных значения. Допустим, что в нашем случае это значения 0 и 1. Они соответствуют двум классам объектов:

* 0 — вид Iris setosa.
* 1 — другой вид ириса.

**Вес признаков** (feature weight) w~i~ — это коэффициенты, которые подбирает алгоритм обучения. Если веса подобраны правильно, пороговая функция верно определит класс объекта по заданным признакам.

В примере с ирисами мы учитываем только два признака: ширину и длину чашелистика цветка. Поэтому формулу линейного классификатора можно упростить:
{width: "35%"}
![](images/ArtificialIntelligence/iris-classifier-formula.png)

Обозначения в формуле следующие:

* y — выходное значение модели.
* f — пороговая функция.
* w~1~ — вес признака ширина чашелистика.
* x~1~ — ширина чашелистика.
* w~2~ — вес признака длина чашелистика.
* x~2~ — длина чашелистика.

Возникает вопрос: какую именно пороговую функцию f использует линейный классификатор? Эта функция зависит от выбора обучаемой модели.

На практике для классификации применяют следующие модели:

* [**Логистическая регрессия**](https://ru.wikipedia.org/wiki/Логистическая_регрессия) (logistic regression) или логит-модель (logit model).

* [**Метод опорных векторов**](https://ru.wikipedia.org/wiki/Метод_опорных_векторов) (support vector machine или SVM).

* [Нейронная сеть](https://www.mathematik.uni-muenchen.de/~deckert/teaching/SS18/sec-steps.html).

Это самые популярные модели. Кроме них есть и другие.

Мы применим модель логистической регрессии. В этом случае пороговая функция представляет собой **сигмоид** (sigmoid). Его формула выглядит так:
{height: "10%"}
![](images/ArtificialIntelligence/sigmoid-formula.png)

Обозначения в формуле следующие:

* x — входное значение функции.
* y — выходное значение функции.
* e — константа [основание натурального логарифма](https://ru.wikipedia.org/wiki/E_(число)).

Иллюстрация 2-19 демонстрирует график функции сигмоид.

{caption: "Иллюстрация 2-19. Функция активации сигмоид", height: "40%"}
![Функция активации сигмоид](images/ArtificialIntelligence/sigmoid-graph.png)

При x равном 0 график пересекает ось y в точке 0,5. Чем входное значение x больше, тем результат функции y ближе к 1. Чем значение x меньше, тем y ближе к 0.

Результат работы линейного классификатора представляют в виде графика на **пространстве признаков**. Размерность пространства совпадает с количеством проверяемых признаков объекта. График изображает границу принятия решений. [**Граница принятия решений**](https://en.wikipedia.org/wiki/Decision_boundary) (decision boundary) — это гиперповерхность, разделяющая пространство признаков на области. Каждая область соответствует одному классу объектов.

I> **Гиперповерхность** — это геометрический объект, который делит пространство любой размерности на две области.

Граница принятия решений соответствует графику формулы линейного классификатора. Это формула уже обученной модели. Поэтому в ней используются реальные веса признаков, которые подобрал алгоритм обучения.

В нашем примере проверяются только два признака. Поэтому граница принятия решений строится в двумерном пространстве. Её демонстрирует иллюстрация 2-20.

{caption: "Иллюстрация 2-20. Граница принятия решений линейного классификатора", height: "50%", width: "100%"}
![Классификация ирисов](images/ArtificialIntelligence/fisher-iris-classification.png)

Каждая запись из таблицы ирисы Фишера соответствует точке на графике. Горизонтальная ось определяет длину чашелистика цветка в сантиметрах. Вертикальная ось — ширину чашелистика в сантиметрах. Таким образом, цветок с длиной чашелистика 5 см и шириной 2 см соответствует точке на графике с координатой (5, 2).

В левой верхней части графика находятся фиолетовые точки. Они соответствуют цветкам ириса вида Iris setosa. Зелёные и жёлтые точки справа внизу — это ирисы видов Iris virginica и Iris versicolor. Синяя пунктирная линия отделяет цветки вида Iris setosa от остальных. Это и есть граница принятия решений. Её наклон определяют веса w~1~ и w~2~, которые подобрал алгоритм обучения.

Рассмотрим, как работает обученная модель. На вход она получает вектор X. У него два элемента:

* x~1~ ширина чашелистика цветка.
* x~2~ длина чашелистика цветка.

По этим признаком надо определить вид растения. Модель подставляет x~1~ и x~2~ в формулу линейного классификатора и вычисляет значение y. Если y равен 0, то проверяемый ирис относится к виду Iris setosa. Если y равен 1, это ирис какого-то другого вида.

График на иллюстрации 2-20 наглядно демонстрирует работу модели. Допустим, модель получила входной вектор X. Отложим на горизонтальной оси значение признака x~2~, а на вертикальной оси — x~1~. Получим точку в пространстве. Она соответствует цветку некоторого ириса, который надо классифицировать. Если точка оказалась выше границы принятия решений, растение относится к виду Iris setosa. В противном случае, это другой вид ириса.

Остаётся открытым ещё один вопрос. Как именно алгоритм обучения подбирает веса w~1~ и w~2~? Для моделей линейных классификаторов применятся алгоритм с названием [**стохастический градиентный спуск**](https://ru.wikipedia.org/wiki/Стохастический_градиентный_спуск) (stochastic gradient descent или SGD). Он реализован в нескольких библиотеках с открытым исходным кодом:

* [scikit-learn](https://scikit-learn.org/stable/modules/sgd.html)

* [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/experimental/SGD)

На практике применяют одну из этих готовых реализаций.

Приложение 5.1.1 подробно описывает Python скрипт для классификации ирисов Фишера. Скрипт также доступен на страничке [Github](https://github.com/ellysh/ai-in-strategy-games/blob/master/manuscript/resources/code/ArtificialIntelligence/linear-classifier.py).

##### 2.6.2.1.2 Регрессия

Регрессионный анализ — это статистический метод, который применяется в машинном обучении. Он моделирует взаимосвязи между **зависимой переменной** и несколькими **независимыми переменными**. Зависимая переменная называется **результатом** (outcome) или **ответом** (response). Зависимые переменные называются **предикторами** (predictors) или **признаками** (features).

Основная цель регрессионного анализа — понять природу и значимость взаимосвязи между переменными, а также сделать прогноз. Регрессионный анализ занимается только математическими зависимостями, а не причинно-следственными. Это значит, что он не предполагает логического вывода в каком-либо виде.

Допустим, что некоторая модель должна решать задачу регрессии. Чтобы её создать, надо подготовить обучающий набор данных. Каждый его пример состоит из двух векторов:

1. Вектор входных значений X содержит набор независимых переменных: x~1~, x~2~,..., x~n~.

2. Вектор Y содержит единственное значение зависимой переменной y. В отличие от задачи классификации y является вещественным числом. В этом случае говорят, что модель возвращает **непрерывные значения**.

Обученная модель работает по тому же принципу, что и линейный классификатор. На вход она получает вектор независимых переменных X. На выходе модель возвращает вектор Y'. Единственное значение в нём — это зависимая переменная y'.

Рассмотрим задачу регрессии на примере. Нам нужно оценить цену автомобиля Volkswagen Golf в зависимости от его пробега в километрах. Цена будет зависимой переменной, а пробег — независимой. Другими словами, пробег — это единственный признак, который мы учитываем.

Чтобы обучить модель, нам нужен набор данных. Каждый пример набора представляет собой пару значений: цена и пробег. Пробег x будет единственным элементом входного вектора X. Цена y — элемент выходного вектора Y'.

Предположим, что мы проверили рынок поддержанных автомобилей и нашли несколько предложений Volkswagen Golf. Цены и пробег этих машин мы заносим в таблицу. Далее переводим эти данные в формат, подходящий для алгоритма обучения. Алгоритм обрабатывает эти данные и строит обучаемую модель. Она принимает на вход пробег автомобиля и предсказывает его ожидаемую цену.

В качестве модели применим линейную регрессию. **Линейная регрессия** (linear regression) моделирует линейную зависимость между результатом и признаками. Для этого [**линейное уравнение**](https://ru.wikipedia.org/wiki/Линейное_уравнение) (или уравнение прямой) подгоняется под наблюдаемые данные. Цель модели — найти наиболее подходящую линию, которая минимизирует разницу между прогнозируемыми и фактическими значениями зависимой переменной. Эта линия называется **линией регрессии** (regression line).

Линейная регрессия бывает трёх видов:

1. Если модель учитывает только один признак, она называется **простой линейной регрессией**(simple linear regression).

2. Если модель учитывает несколько признаков, она называется **множественной линейной регрессией** (multiple linear regression).

3. Если модель предсказывает не одну зависимую переменную, а несколько, она называется **многомерной линейной регрессией** (multivariate linear regression).

Наш пример — это случай простой линейной регрессии.

Формула линейной регрессии в общем виде выглядит так:
{width: "30%"}
![](images/ArtificialIntelligence/linear-regressor-formula.png)

Обозначения в формуле следующие:

* y — выходное значение модели.
* w~i~ — вес признака под номером i.
* x~i~ — значение признака под номером i.
* ε — случайная ошибка модели.

Эта модель в отличие от линейного классификатора не использует пороговую функцию f. Выходное значение y рассчитывается по приведённой формуле с правильно подобранными весами w~i~.

В нашей задаче мы учитываем только один признак машины — её пробег. Поэтому в формуле остаётся только один признак x~1~ и её вес w~1~. Таким образом она принимает следующий вид:
{width: "25%"}
![](images/ArtificialIntelligence/cost-regressor-formula.png)

Здесь используются следующие обозначения:

* y — выходное значение модели.
* w~1~ — вес признака пробег автомобиля.
* x~1~ — пробег автомобиля в километрах.
* ε — случайная ошибка модели.

Результат работы линейной регрессии можно представить в виде графика. В отличие от линейного классификатора к пространству признаков надо добавить ещё одно измерение — значение зависимой переменной y. Для нашего примера пространство графика будет двумерным, поскольку мы учитываем только один признак. Первое измерение — это пробег автомобиля в километрах x~1~, а второе — его цена y.

Иллюстрация 2-21 демонстрирует линию регрессии. По ней обученная модель прогнозирует цены на автомобили.

{caption: "Иллюстрация 2-21. Линия регрессии для прогноза цен на автомобили", height: "50%", width: "100%"}
![Цены автомобили](images/ArtificialIntelligence/car-cost-regression.png)

На горизонтальной оси отображается пробег автомобиля в километрах. На вертикальной оси — его цена в долларах. Чёрные точки обозначают примеры из тестового набора данных. Исходные данные по ценам машин и их пробегу мы делим на обучающий и тестовые наборы. Примеры обучающего набора на графике не приводятся. Синяя линия отображает зависимость между пробегом и ценой автомобиля. Это и есть линия регрессии.

Вопрос — от чего зависят наклон и смещение линии регрессии по вертикальной оси? Вес признака пробег автомобиля w~1~ определяет наклон линии. Её смещение определяет случайная ошибка модели ε.

Рассмотрим, как работает обученная модель. Исходные данные для неё — это пробег некоторого автомобиля в километрах. Представляем эти данные в виде вектора X с единственным элементом x~1~. Далее передаём их на вход модели. Она подставляет значение x~1~ в формулу линейной регрессии и вычисляет результат y. Это и будет прогнозируемой ценой автомобиля с указанным пробегом.

Обратимся к графику на иллюстрации 2-21, чтобы представить работу модели наглядно. Спрогнозируем цену некоторого автомобиля по его пробегу. Для этого отложим величину пробега на горизонтальной оси. Пусть для примера он равен 2000 км. Этому значению соответствует только одна точка на синей линии. По вертикальной оси эта точка находится на уровне цены 20000$. Таким образом мы нашли прогнозируемую цену автомобиля.

Чтобы подобрать веса w~i~ в формуле линейной регрессии, на практике обычно применяют алгоритм обучения с названием [**метод наименьших квадратов**](http://www.machinelearning.ru/wiki/index.php?title=Метод_наименьших_квадратов) или МНК (least squares). Он реализован в нескольких библиотеках с открытым исходным кодом:

* [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)

* [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/linalg/lstsq)

* [PyTorch](https://pytorch.org/docs/stable/generated/torch.linalg.lstsq.html)

Приложение 5.1.2 подробно описывает Python скрипт для прогнозирования цен на автомобили. Скрипт также доступен на страничке [Github](https://github.com/ellysh/ai-in-strategy-games/blob/master/manuscript/resources/code/ArtificialIntelligence/linear-regression.py).

#### 2.6.2.2 Обучение без учителя

Обучение с учителем хорошо работает, когда есть большой объём размеченных данных. Но подготовить такие данные — трудоёмкий и дорогой процесс. Обычно разметка выполняется вручную человеком. Поэтому набор обучающих данных получается небольшими. На нём удаётся построить только относительно простую модель. Сложная модель требуют намного больше данных.

Обученная модель решает некоторую прикладную задачу. Чем сложнее задача, тем сложнее должна быть модель. Модель может оказаться намного проще чем функция, которую она должна аппроксимировать. В этом случае возникает проблема недообучения. Тогда обобщающая способность модели оказывается неприемлемо низкой.

I> **Аппроксимация** — это замена сложного объекта более простыми. При этом важные аспекты заменяющего и исходного объекта похожи. **Аппроксимация функции** означает замену исходной функции на более простую. При этом их поведение похоже.

Обучение без учителя решает проблему трудоёмкой подготовки обучающих данных. Алгоритмы этой парадигмы принимают на вход **неразмеченный набор данные**. Каждый обучающий пример в нём содержит только вектор входных значений X = (x~1~, x~2~,..., x~n~).

Возникает вопрос: как алгоритм обучения понимает, что надо делать с неразмеченными данными? Построенные в рамках этой парадигмы модели работают иначе, чем при обучении с учителем. Они решают совершенно другие практические задачи. Алгоритм обучения для создания этих моделей тоже отличается.

Обучение без учителя решает следующие задачи:

* **Кластеризация** (clustering) — объединение объектов в непересекающиеся группы на основе близости их признаков.

* **Обнаружение аномалий** (anomaly detection) — поиск редких элементов или наблюдений, которые значительно отклоняются от остальных данных и не соответствуют представлению о нормальном поведении.

* [**Понижение размерности**](https://wiki.loginom.ru/articles/data-reduction.html) (data reduction) — преобразование данных в более удобную форму для их анализа и интерпретации.

* [**Поиск ассоциативных правил**](https://en.wikipedia.org/wiki/Association_rule_learning) (association rule learning) — обнаружение неявных закономерностей в больших наборах данных.

* **Подготовка обучающих наборов для обучения с учителем** — это автоматизация ручной разметки данных.

Рассмотрим алгоритмы и модели обучения без учителя на примере кластеризации.

##### 2.6.2.2.1 Кластеризация

Задача кластеризации заключается в том, чтобы разбить множество объектов на группы. В каждую попадают объекты со сходными признаками. Есть важное условие: полученные группы не должны пересекаться. Другими словами, каждый объект должен относится только к одной группе или кластеру.

I> **Кластером** называется набор объектов, схожих по какому-то признаку.

Задачу кластеризацию решают модели, обученные без учителя. Это означает, что правило группирования объектов заранее неизвестно или не задаётся. Алгоритм обучения должен сам вывести это правило и заложить его в модель. Для этого он ориентируется на закономерности во входных данных.

Формат обучающего набора данных отличается для алгоритмов обучения с учителем и без. В случае обучения без учителя каждый пример набора состоит только из вектора входных значений X. Элементы вектора x~1~, x~2~,..., x~n~ — это признаки некоторого объекта, который надо отнести к той или иной группе. В обучающем примере нет вектора выходных значений Y, которые ожидаются от модели.

Рассмотрим задачу кластеризации на примере. Предположим, что онлайн магазину нужно сгруппировать пользователей по интересам. Тогда для каждого пользователя можно будет показывать таргетированную рекламу новых товаров.

Магазин ведёт такую статистику: сколько раз каждый пользователь просматривал какие категории товаров. Для простоты предположим, что в магазине есть только две категории: велозапчасти и автозапчасти.

Алгоритм обучения должен построить модель, которая относит пользователя к некоторому классу. В зависимости от класса сайт магазина выбирает, какую рекламу показывать пользователю. Варианты могут быть следующие:

* Показывать рекламу только велозапчастей.
* Показывать рекламу только автозапчастей.
* Показывать рекламу и велозапчастей, и автозапчастей.
* Не показывать рекламу велозапчастей и автозапчастей.

Идея в том, чтобы пользователи получали рекламу только интересной им категории товаров. Пользователи, которые просматривали велозапчасти, должны получать рекламу новых велозапчастей. То же самое для пользователей, которые когда-то выбирали автозапчасти, — они получат рекламу новых автозапчастей. Рекламу товаров обеих категорий получат те, кому они обе интересны. Новые пользователи сайта без собранной по ним статистики таргетированную рекламу не получат.

Применим алгоритм обучения для задачи кластеризации под названием [**метод k-средних**](https://en.wikipedia.org/wiki/K-means_clustering) (k-means). Среди прочих параметров этому алгоритму нужно указать число ожидаемых кластеров. В нашем примере это число равно четырём. Именно на столько групп мы собираемся разделить всех пользователей.

Метод k-средних находит геометрический центр каждого кластера на пространстве признаков. [**Геометрический центр**](https://ru.wikipedia.org/wiki/Барицентр) (centroid) множества точек — это среднее арифметическое их координат. То есть координата геометрического центра по каждой оси равна сумме координат всех точек по этой оси, разделённая на количество точек.

Метод k-средних не строит модель для прогнозов, как это делают алгоритмы обучения с учителем. Вместо этого он организует уже имеющиеся данные. Если в исходном множестве появляется новый объект, его надо добавить в обучающий набор данных и перезапустить алгоритм обучения сначала.

Представим результат работы метода k-средних для нашей задачи на графике. У каждого пользователя мы учитываем только два признака: посещения двух категорий товаров. Поэтому пространство признаков на графике будет двумерным. Его демонстрирует иллюстрация 2-22.

{caption: "Иллюстрация 2-22. Результат кластеризации пользователей онлайн магазина", height: "50%", width: "100%"}
![Кластеризация пользователей](images/ArtificialIntelligence/online-shop-clustering.png)

Горизонтальная ось показывает количество посещений категории товаров автозапчасти. Вертикальная ось — количество посещений категории велозапчасти. Серые круги обозначают центры кластеров, которые нашёл метод k-средних. Ближайшие к этим центрам точки попали в соответствующий кластер. Относящиеся к одному кластеру точки имеют один цвет.

Метод k-средних разбил всех пользователей онлайн магазина на четыре кластера:

* Жёлтые точки — пользователи с небольшим числом посещений обеих категорий товаров. Чтобы судить об их предпочтениях, собрано недостаточно статистики.

* Синие точки — пользователи, которые интересуются автозапчастями. Им нужно показывать рекламу новинок этой категории товаров.

* Фиолетовые точки — пользователи, которые интересуются велозапчастями. Им нужно показывать рекламу новинок этой категории товаров.

* Зелёные точки — пользователи, которые интересуются обеими категориями товаров. Им нужно показывать рекламу всех новинок онлайн магазина.

Благодаря кластеризации, онлайн магазин может учесть предпочтения каждого постоянного пользователя. Для него сайт будет показывать только актуальную рекламу.

Если у магазина появится новый пользователь, по нему надо собрать статистику: какие категории товаров он чаще посещает. Затем эти данные надо добавить в обучающий набор и повторить алгоритм k-средних. Тогда предпочтения пользователя попадут в результат кластеризации. На основании этого результата алгоритм сайта магазина выберет подходящую рекламу.

Приложение 5.1.3 подробно описывает Python скрипт для кластеризации методом k-средних. В этом примере обучающий набор данных сгенерирован случайным образом. Скрипт также доступен на страничке [Github](https://github.com/ellysh/ai-in-strategy-games/blob/master/manuscript/resources/code/ArtificialIntelligence/k-mean-clustering.py).

#### 2.6.2.3 Обучение с подкреплением

Мы рассмотрели алгоритмы обучения с учителем и без. Они получают на вход обучающий набор данных в том или ином формате. В парадигме обучения с подкреплением таких данных нет. В этом её принципиальное отличие.

Что означает термин "подкрепление"? Его ввёл российский физиолог Иван Петрович Павлов в начале XX века. Подкреплением он назвал стимул, который усиливает модель поведения животного. Психологи расширили термин "подкрепление". Они обратили внимание на то, что модель поведения можно не только усиливать, но и ослаблять. Важная особенность подкрепления в том, что после устранения стимула, животное сохраняет выученное им поведение. В машинном обучении "подкрепление" означает использование сигналов обратной связи. Они направляют процесс обучения.

В обучении с подкреплением применяют терминологию теории интеллектуальных агентов. Причина в том, что эта парадигма обучения работает с агентами и процессами принятия решений в интерактивных и динамических средах. Теория интеллектуальных агентов предлагает подходящую основу для моделирования таких систем и процессов.

Сначала разберёмся с принципиальным отличием обучения с подкреплением от других парадигм. Для этого рассмотрим их задачи с точки зрения теории интеллектуальных агентов.

Алгоритмы обучения с учителем и без учителя создают модели, которые работают в эпизодических средах. Для обучения применяются несвязанные между собой примеры входных данных. Готовый агент обрабатывает такие же несвязанные примеры новых данных. В этом случае его прошлые решения никак не влияют на последующие. Из-за этого агенту не надо отслеживать долгосрочные последствия своих действий. Алгоритм обучения просто игнорирует эти зависимости. Как следствие, построенный таким алгоритмом агент не может эффективно работать в последовательной среде. В такой среде успешные в краткосрочной перспективе действия могут привести к негативным результатам в будущем. Чтобы учесть подобные долгосрочные последствия, нужна иная парадигма обучения. Именно такие задачи решает обучение с подкреплением.

Теперь рассмотрим, как работает обучение с подкреплением. Главные участники этого процесса — агент, среда и алгоритм обучения. Шаги обучения следующие:

1. Агент выполняет действие. В начале обучения у него нет никаких инструкций. Поэтому агент выбирает случайные действия.

2. Среда реагирует на действие агента переходом в новое состояние.

3. Новое состояние среды определяет сигнал вознаграждения, который агент получает за своё действие. **Сигнала вознаграждения** (reward signal) — это некоторое числовое значение.

4. Алгоритм обучения получает сигнал вознаграждения. Он корректирует стратегию агента исходя из значения этого сигнала. Таким образом алгоритм обучения определяет, какие решения принимает агент.

5. После корректирования стратегии, агент выполняет следующее действие. То есть он возвращается к первому шагу обучения.

Цель агента заключается в том, чтобы изучить стратегию, которая максимизирует совокупное вознаграждение. **Совокупное вознаграждение** (cumulative reward) — это сумма всех вознаграждений. Алгоритм обучения служит агенту инструментом для достижения цели. Благодаря обратной связи, алгоритм соотносит положительные результаты с определёнными действиями. Так он совершенствует стратегию агента, тем самым повышая совокупное вознаграждение.

Кроме агента и среды в обучении с подкреплением есть четыре второстепенных элемента:

* **Политика** (policy) или **стратегия** означает функцию агента в терминологии обучения с подкреплением. Она определяет действие агента, как ответ на конкретное наблюдаемое состояние среды. По сути, политика представляет собой план действий в данной среде.

* **Сигнал вознаграждения** (reward signal) — это скалярное значение, которое среда передаёт агенту в качестве обратной связи. Величина сигнала зависит от результата действия, которое выполнил агент. Положительное значение указывает на благоприятный результат, а отрицательное — на нежелательный.

* **Функция полезности** (value function) оценивает совокупное вознаграждение агента в разных ситуациях. Различают две функции полезности:

    * **Функция полезности состояния** (state value function) оценивает ожидаемое будущее вознаграждение агента. Агент может его получить, если начнёт действовать с состояния среды s и будет следовать определённой политике π. Функция количественно определяет желательность пребывания в некотором состоянии среды. Она обозначается как `V(s)`.

    * **Функция полезности действия** (action value function) оценивает вознаграждение агента за совершение определённого действия a в состоянии среды s и дальнейшее следование политике π. Функция количественно определяет ценность совершения действия в некотором состоянии среды. Она обозначается как `Q(s, a)`.

* **Модель среды** (model) — это внутреннее представление агента для прогнозирования поведения среды. Модель помогает агенту эффективно планировать свои действия. Есть два типа моделей:

    * **Модель переходов** (transition model) — исходя из текущего состояния среды и действия агента прогнозирует следующее состояние и связанное с ним вознаграждение.

    * **Модель вознаграждений** (reward model) — предсказывает ожидаемое вознаграждение с учётом текущего состояния среды и действия агента.

I> Обратите внимание на различие между терминами "модель среды" и "обучаемая модель". Последнюю называют "агентом" в терминологии обучения с подкреплением.

Модель вознаграждений строится на функции вознаграждений. **Функция вознаграждений** (reward function) — принимает на вход пару: состояние среды и выбранное агентом действие. Она возвращает численное значение, которое соответствует немедленному вознаграждению за выполнение выбранного действия в указанном состоянии. Эта функция как `R(s, a)`.

Функции полезности и вознаграждения работают с одними и теми же входными параметрами. Они возвращают похожий результат: числовое значение вознаграждения. Тем не менее они различаются. Функция полезности определяет долгосрочную перспективу действий агента. Она учитывает вознаграждения от будущих состояний среды, которые станут доступны после выбранного действия агента в указанном состоянии среды. Функция вознаграждение определяет немедленный результат действия в указанном состоянии. Она не учитывает его долгосрочные последствия.

В обучении с подкреплением есть два принципиально разных подхода:

* **Основанный на модели** (model-based).

* **Без модели** (model-free).

Наличие модели — это характеристика и агента, и алгоритма обучения, который его строит.

В подходе основанном на модели агент поддерживает внутреннюю модель среды. Она позволяет предсказать реакцию среды на возможные действия. Агент использует модель для планирования и принятия решений. Прежде чем действовать он рассматривает будущие состояния среды и совокупные вознаграждения за их достижение.

В подходе без модели агент не поддерживает модель среды в явном виде. Он учится непосредственно на взаимодействии с окружающей средой методом проб и ошибок. Агент оценивает только наблюдаемые последствия своих действий и не моделирует будущие состояния среды.

У каждого подход есть свои преимущества и недостатки. Вот сильные стороны агентов, основанных на модели:

1. Они требуют меньше итераций алгоритма обучения.

2. Планирование действий заранее полезно в средах, в которых есть состояния из которых агент не может вернуться.

3. Модель улучшает обобщающую способность агента. Он способен делать прогнозы в ситуациях, с которыми раньше никогда не сталкивался.

Слабые стороны агентов, основанных на модели, следующие:

1. Обновление внутренней модели и её использование дорого обходятся с точки зрения вычислений.

2. Точность модели очень важна. Любые неточности в ней приводят к неоптимальным действиям агента.

3. Основанный на модели агент предполагает, что закономерности среды ему известны. Если реальные закономерности оказались сложнее чем модель или в поведении модели много случайностей, агент не сможет в ней эффективно действовать.

Достоинства агентов без модели:

1. Они концептуально проще и просты в реализации.

2. Устойчивы к неточностям и неопределённостям в поведении среды. Они учатся непосредственно на взаимодействии с ней.

3. Справляются со сложными средами, в поведении которых есть случайность.

Недостатки агентов без модели:

1. Требуют больше итераций алгоритма обучения.

2. Обобщающая способность таких агентов хуже. Они плохо справляются с ситуациями, с которыми раньше не сталкивались.

3. Полагаются на метод проб и ошибок в процессе обучения. Этот метод оказывается неэффективен в некоторых средах.

Что означает поиск методом проб и ошибок? Это важная концепция, поэтому остановимся на ней подробнее. Прежде всего так называется врождённый метод мышления человека и животных. Он заключается в повторении различных действий до тех пор, пока задача не будет решена.

В общем виде метод проб и ошибок состоит из следующих шагов:

1. Выполнить действие или применить некоторое решение.

2. Наблюдать за результатом. Другими словами, получить обратную связь.

3. Оценить успешность действия или решения.

4. Скорректировать действие или решение на основе обратной связи.

Шаги метода зависят от решаемой задачи. В машинном обучении "действие" соответствует определённой стратегии агента. "Результат" — это совокупное вознаграждение за её применение.

Метод проб и ошибок имеет ряд преимуществ. Он прост в реализации, подходит для решения многих задач и часто не требует значительных вычислений. С другой стороны недостатки метода в том, что он работает медленно и находит первое сработавшее решение, а не оптимальное. 

Оба подхода обучения с подкреплением (основанный на модели и без модели) сталкиваются с [**дилемма исследования и эксплуатации**](https://en.wikipedia.org/wiki/Exploration-exploitation_dilemma). Разберёмся с этой концепцией по порядку. Начнём с определений исследования и эксплуатации.

**Исследование** (exploration) — это выполнение новых действий в незнакомой среде. Такие действия выполняются, чтобы получить информацию о ещё неизвестных аспектах среды. Исследование позволяет обнаружить потенциально лучшие действия и стратегии, которые приводят к высоким совокупным вознаграждениям в долгосрочной перспективе. Исследование приводит к неоптимальным действиям в краткосрочной перспективе. Это снижает немедленное вознаграждение агента.

**Эксплуатация** (exploitation) — это выполнение действий с уже известным результатом. Такие действия приносят высокое немедленное вознаграждение. Эксплуатация неоптимальна в долгосрочной перспективе. Выбирая всегда её, агент не обнаружит более эффективные действия и стратегии.

I> **Немедленное вознаграждение** (immediate rewards) — это это вознаграждение, которое агент получает сразу после выполнения определённого действия.

Суть дилеммы такая: агент должен выделять свои ограниченные ресурсы на исследование или эксплуатацию? Другими словами, у агента такой выбор:

* Гарантированное высокое немедленное вознаграждение.

* Потенциальное высокое совокупное вознаграждение в долгосрочной перспективе.

[**Проблема многорукого бандита**](https://en.wikipedia.org/wiki/Multi-armed_bandit) хорошо демонстрирует дилемму исследования и эксплуатации. Одноруким бандитом называется [игровой автомат](https://ru.wikipedia.org/wiki/Слот-машина). Пользователь должен бросить монету и дёрнуть за рычаг. После этого вращается барабан, который показывает комбинацию символов. Некоторые комбинации являются выигрышными.

В проблеме многорукого бандита у игрока есть несколько монет и ряд игровых автоматов. У каждого автомата своё соотношение затрат к выигрышу. Изначально игрок не знает этих соотношений, но может их выяснить после нескольких игр на автомате.

Задача игрока — максимизировать свой выигрыш, имея на руках ограниченное количество монет. Для этого он должен решить для себя следующее:

1. На каких автоматах ему играть?

2. Сколько раз играть на каждом автомате и в какой последовательности?

3. Стоит ли продолжать играть на текущем автомате или перейти к следующему?

Найти точное решение проблемы многорукого бандита пока не удалось. Есть только ряд приблизительных решений. Они дают не наилучший, но приемлемый результат.

##### 2.6.2.3.1 Подходы к обучению с подкреплением

Чтобы реализовать обучение с подкреплением на практике, применяют следующие подходы:

1. **Подход, основанный на стратегии** (policy-based) явно строит стратегию агента. Она хранится в памяти на протяжении всего процесса обучения агента и его применения для решения задачи. Некоторые policy-based алгоритмы обучения не используют функцию полезности. Другие применяют её для повышения точности финальной стратегии. Policy-based алгоритмы обучения могут построить следующие стратегии:

   * **Детерминированная стратегия** — предписывает агенту выполнять одни и те же действия в каждом состоянии среды.

   * **Вероятностная (стохастическая) стратегия** — агент выбирает случайно одно из нескольких возможных действий в каждом состоянии среды.

2. **Подход, основанный на полезности** (value-based) не строит стратегию агента явно. Вместо этого он находит функцию полезности. Из неё неявно выводится стратегия агента: в каждом состоянии среды он выбирает действие с максимальной полезность.

3. [**Подход, основанный на модели**](https://ai.stackexchange.com/questions/4456/whats-the-difference-between-model-free-and-model-based-reinforcement-learning) (model-based) — агент в процессе обучения и последующей работы использует **модель среды** для прогнозирования результатов своих действий. Моделью среды называется механизм, который воспроизводит ответную реакцию среды на действия агента. Другими словами модель сообщает, какие состояния станут доступны после совершения конкретного действия и какие вознаграждения (reward signal) агент в них получит.

Рассмотрим подходы обучения с подкреплением на примере. Допустим, что мы создаём агента для игры в крестики-нолики. Этот агент будет соревноваться с несовершенным игроком. Такой игрок допускает ошибки и иногда выбирает не лучший из возможных ходов.

Для решения задачи применим обучение с подкреплением. Наш агент будет многократно играть с оппонентом. То каким образом агент сформирует свою финальную стратегию, будет зависеть от выбранного нами подхода.

Для примера выберем подход, основанный на стратегии (policy-based). В качестве метода применим эволюционный алгоритм. Перед запуском алгоритма надо определить, какую именно стратегию мы ищем. Для этого достаточно указать вероятность победы агента над оппонентом, если он будет играть по найденной стратегии. Предположим, что нас устроит стратегия с вероятностью победы 90%.

Эволюционный алгоритм генерирует набор случайных стратегий. В нашем случае стратегия — это набор правил, которые определяют следующий ход агента для любого возможного состояния поля игры крестики-нолики. Самые первые стратегии, сгенерированные алгоритмом, называются **первым поколением**.

Перед запуском алгоритма полезно указать максимальное число поколений, за которое он должен найти интересующую нас стратегию. Если найти её невозможно в принципе, алгоритм будет продолжаться без конца. Будет лучше прервать его и скорректировать, либо выбрать другой метод обучения с подкреплением.

Предположим, что мы указали все необходимые входные данные и запустили алгоритм обучения. В ходе его выполнения агент многократно играет с оппонентом, используя первую стратегию из списка сгенерированных. В результате получается три числа: количество побед, поражений и ничьих. Основываясь на этом результате рассчитывается вероятность победы агента при использовании этой стратегии. Закончив с ней, агент переходит ко второй стратегии и снова многократно играет с оппонентом. Этот процесс повторяется для всех стратегий первого поколения.

На следующем этапе эволюционный алгоритм отбирает несколько стратегий с максимальной вероятностью победы. Каждая из них модифицируется случайным образом. В результате получается набор стратегий второго поколения. В него входят самые успешные стратегии первого поколения и их модификации.

Агент снова многократно играет с оппонентом, используя стратегии второго поколения. Так отбираются наиболее успешные из них. Этот процесс повторяется снова и снова. Алгоритм обучения завершается, когда находит стратегию с вероятностью выигрыша 90%, либо достигает максимально допустимого числа поколений.

Эволюционные алгоритмы плохо справляются со сложными задачами. Кроме того они требуют значительных вычислительных ресурсов и времени. Сегодня более популярны две группы методов, которые можно отнести к (policy-based):

1. [Policy gradient](https://jonathan-hui.medium.com/rl-policy-gradients-explained-9b13b688b146) — алгоритмы обучения этой группы последовательно приближают текущую стратегию к оптимальной. Для этого они увеличивают вероятность выбора вариантов стратегии, которые дают высокое итоговое вознаграждение. Одновременно с этим вероятность выбора вариантов стратегий с низким вознаграждением понижается. Примеры policy gradient методов:

   * [REINFORCE](https://julien-vitay.net/deeprl/PolicyGradient.html#sec:reinforce)

   * [Trust Region Policy Optimization](https://julien-vitay.net/deeprl/NaturalGradient.html#sec:trust-region-policy-optimization-trpo) (TRPO)

   * [Proximal Policy Optimization](https://julien-vitay.net/deeprl/NaturalGradient.html#sec:proximal-policy-optimization-ppo) (PPO)

* [Actor–critic](https://julien-vitay.net/deeprl/ActorCritic.html#sec:advantage-actor-critic-methods) — алгоритмы обучения этой группы приближают к оптимальной текущую стратегию и функцию полезности одновременно. Алгоритмы состоят из двух компонентов: actor (действующий объект) и critic (critic). Actor выполняет несколько действий, согласно выбранной стратегии. Critic вычисляет полезность состояний среды, которые стали доступны после действий actor. В зависимости от результатов critic, actor улучшает свою стратегию. После выполнения действий actor и расчёта фактических вознаграждений в новых состояниях среды, critic улучшает свою функцию полезности. Примеры actor–critic методов:

   * [Advantage actor-critic](https://julien-vitay.net/deeprl/ActorCritic.html#sec:advantage-actor-critic-a2c) (A2C)

   * [Asynchronous advantage actor-critic](https://julien-vitay.net/deeprl/ActorCritic.html#sec:asynchronous-advantage-actor-critic-a3c) (A3C)

   * [Soft actor-critic](https://spinningup.openai.com/en/latest/algorithms/sac.html) (SAC)

Подход, основанный на полезности (value-based), работает иначе. В этом подходе алгоритм обучения строит таблицу всех возможных состояний игрового поля или позиций. Для каждой позиции алгоритм подбирает некоторое число. Оно оценивает вероятность выигрыша агента, если он продолжит игру из соответствующего состояния поля.

Числовые оценки позиций представляют собой **полезность** (value). Большая полезность означает, более выгодное для агента состояние игрового поля. Построенная алгоритмом обучения таблица позиций и их оценок соответствует функции полезности (value function). Конкретная позиция — это пример входного параметра функции, на который она возвращает соответствующую оценку.

Предположим, что наш агент играет всегда крестиками. Тогда любая позиция с тремя крестиками в ряд или по диагонали имеет полезность равную 1. В таком состоянии игрового поля агент уже одержал победу. Аналогично любая позиция с тремя ноликами в ряд или по диагонали имеет полезность равную 0. Здесь агент уже проиграл.

Перед запуском алгоритма обучения мы выставляем полезность выигрышных позиций в 1, а проигрышных — в 0. Для всех остальных состояний поля устанавливаем полезность 0.5. Это число означает неизвестную нам позицию, в которой агент может добиться победы в 50% случаев. Задача алгоритма обучения — скорректировать оценки этих неизвестных состояний.

Алгоритм обучения проводит многократные игры между агентом и его оппонентом. Агент выбирает свои ходы исходя из таблицы позиций и их оценок. В большинстве случаев он выбирает **жадный** (greedy) ход, который приводит к позиции с максимальной полезностью. Изредка агент отклоняется от этого правила и выбирает ход случайно. В этом случае ход называется **исследовательским** (exploratory). Таким образом агент получает возможность проверить позиции, которые обычно он избегает. Иногда они приводят к неизвестным ранее состояниям поля с высокой полезностью.

В ходе игр между агентом и оппонентом алгоритм обучения корректирует полезность позиций в которых оказывается агент. Это происходит следующим образом:

1. Полезность текущего состояния сохраняется.

2. Агент выполняет жадный ход и оказывается в новом состоянии.

3. Полезность прошлого состояния увеличивается или уменьшается на небольшое значение так, чтобы приблизиться к полезности нового состояния.

Многократно повторяя эти шаги, алгоритм обучения находит пути из начального состояния поля ко всем возможным выигрышным позициям. После окончания обучения агенту достаточно в любой позиции выбирать жадный ход. Такие ходы в итоге приведут его к выигрышу.

Рассмотренный нами метод называется [обучением с временной разницей](http://www.scholarpedia.org/article/Temporal_difference_learning) (temporal difference или TD learning). Семейство подобных методов называется [Q-learning](https://en.wikipedia.org/wiki/Q-learning).

Кроме Q-learning сегодня популярны следующие основанные на полезности (value-based) методы:

* [State-Action-Reward-State-Action](https://en.wikipedia.org/wiki/State–action–reward–state–action) (SARSA) — в отличие от Q-learning оценивает полезность не жадных ходов, а тех которые соответствуют наилучшей найденной в данный момент стратегией.

* Deep Q Network (DQN) — использует нейронную сеть для нахождения функции полезности. Подходит для обучения агента действовать в дискретной среде.

* Deep Deterministic Policy Gradient (DDPG) — использует две нейронные сети. Одна из них выбирает действия, а вторая даёт им оценку.

Эта [статья](https://habr.com/ru/post/561746/) приводит их краткое описание.

{pagebreak}
