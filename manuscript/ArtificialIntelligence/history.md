## История

Искусственный интеллект как направление исследований появился на стыке нескольких дисциплин. На ИИ оказали влияние идеи, открытия и методы из следующих наук:

* Неврология
* Математика
* Информатика
* Теория управления
* Кибернетика

Рассмотрим наиболее важные этапы развития ИИ.

### Появление новой науки 1943 – 1956

#### Нейронная сеть

Первые исследования в области ИИ связаны с неврологией. Американские учёные [Уолтер Питтс](https://ru.wikipedia.org/wiki/Питтс,_Уолтер) [Уоррен Мак-Каллок](https://ru.wikipedia.org/wiki/Мак-Каллок,_Уоррен) предложили модель [**искусственного нейрона**]((https://ru.wikipedia.org/wiki/Искусственный_нейрон)). Эта модель основывалась на исследованиях в области нейрофизиологии. Она могла выполнять простые логические выводы. Свои достижения учёные описали в статье "Логическое исчисление идей, относящихся к нервной активности" (A Logical Calculus of Ideas Immanent in Nervous Activity). Статья была опубликована в 1943 году.

[Нейрон](https://ru.wikipedia.org/wiki/Нейрон) — это клетка, которая является минимальным строительным блоком [нервной системы](https://ru.wikipedia.org/wiki/Нервная_система). Нейроны соединяется друг с другом в [нервную сеть](https://ru.wikipedia.org/wiki/Нервная_сеть). По этой сети передаются электрические и химические сигналы. С помощью этих сигналов передаётся информация между узлами нервной системы.

Искусственный нейрон Питтса и Мак-Каллока представлял собой упрощённую математическую модель биологического нейрона. Искусственный нейрон выполняет единственную задачу: получает на вход набор сигналов и генерирует один выходной сигнал.

В 1949 году канадский нейропсихолог [Дональд Хебб](https://ru.wikipedia.org/wiki/Хебб,_Дональд) опубликовал книгу "Организация поведения" (Organization of Behavior). В ней учёный предложил теорию обучения нейронов человеческого мозга. Эта теория получила название [правило обучения Хебба]((http://www.machinelearning.ru/wiki/index.php?title=Правило_Хэбба)). Это правило можно сформулировать так:

*Связи нейронов, которые активируются совместно, усиливаются. Связи нейронов, которые срабатывают независимо, ослабевают*

Правило Хебба легло в основу моделей первых [**нейронных сетей**](https://ru.wikipedia.org/wiki/Нейронная_сеть). Оно остаётся актуальным и сегодня.

Нейронная сеть — это соединение искусственных нейронов. Первый компьютер на основе такой сети был сконструирован аспирантами математиками [Марвином Мински](https://ru.wikipedia.org/wiki/Минский,_Марвин_Ли) и Дином Эдмондсом в 1951 году. Компьютер получил название [Snarc](https://en.wikipedia.org/wiki/Stochastic_neural_analog_reinforcement_calculator). Snarc моделировал сеть их 40 нейронов. Мински и Эдмондс пытались научить эту сеть проходить лабиринт.

#### Статья Тьюринга

Первое систематичное обсуждение искусственного интеллекта предложил [Алан Тьюринг](https://ru.wikipedia.org/wiki/Тьюринг,_Алан) в статье ["Вычислительные машины и разум"](https://ru.wikipedia.org/wiki/Вычислительные_машины_и_разум) (Computing Machinery and Intelligence). Она была опубликована в 1950 году.

В своей статье Тьюринг определяет термины "машина" и "мыслить". Он предлагает [тест](https://ru.wikipedia.org/wiki/Тест_Тьюринга) для проверки разумности машины. В этом тесте человек выполняет роль судьи. Он должен обменяться сообщениями с другим человеком и машиной. В результате этой переписки судья определяет, кто из собеседников машина. Участники теста не видят друг друга. То есть обмен сообщениями происходит вслепую. Машина проходит тест, если судья не может однозначно решить, кто из собеседников является человеком.

Тьюринг в своей работе также описывает принципы машинного обучения и генетические алгоритмы. Эти идеи намного предвосхитили соответствующие технологии.

Многие из прогнозов Тьюринга о развитии вычислительной техники и искусственного интеллекта оказались верны.

#### Logic Theorist

В 1956 году американские учёные [Аллен Ньюэлл](https://ru.wikipedia.org/wiki/Ньюэлл,_Аллен) и [Герберт Саймон](https://ru.wikipedia.org/wiki/Саймон,_Герберт_Александер) разработали программу под названием [Logic Theorist](https://en.wikipedia.org/wiki/Logic_Theorist). Эта программа выполняла [автоматические рассуждения](https://en.wikipedia.org/wiki/Automated_reasoning). Она смогла доказать 38 теорем из книги ["Начала математики"](https://ru.wikipedia.org/wiki/Principia_Mathematica) Бертрана Рассела. Книга Рассела считается фундаментальным трудом по [логике](https://ru.wikipedia.org/wiki/Логика).

В середине 1950-х годов считалось, что компьютеры могут оперировать только числами. Вопреки этому мнению программа Ньюэлла и Саймона работала с символами. Это позволило перейти от простых вычислений к логическим высказываниям. Разработчики Logic Theorist утверждали, что создали программу, способную "мыслить в нечисловых терминах". Их подход стал доминирующим в ИИ на следующие 30 лет.

Ньюэлл и Саймон применили в Logic Theorist следующие концепций:

* **Рассуждение как поиск**. Logic Theorist представляет доказательство гипотезы как поиск по дереву. Корень дерева соответствует начальной гипотезе. Исходящие из корня ветви — это логические операции над начальной гипотезой. Каждая ветвь приводит к определённому выводу. Один из этих выводов является целью рассуждения.

* [**Эвристики**]((https://ru.wikipedia.org/wiki/Эвристика)) — это дополнительные правила, которые исключают из рассмотрения некоторые ветви дерева поиска. Согласно правилам маловероятно, что эти ветви приведут к решению. Эвристики позволяют значительно сократить время поиска решений.

* **Обработка списков**. Чтобы написать программу Logic Theorist, был разработан специальный язык программирования [IPL](https://en.wikipedia.org/wiki/Information_Processing_Language). Этот язык предоставлял возможности для обработки списков символов. Позднее на основе IPL Джон Маккарти разработает язык Lisp. Этот язык станет самым популярным в области ИИ на два десятилетия.

Эти концепций стали ключевыми для последующих исследований в области ИИ. 

#### Дартмутский семинар

Все описанные исследования относились к области ИИ. Однако, такой науки не существовало до лета 1956 года. Такие исследования "думающих машин" называли кибернетикой, теорией автоматов и сложной обработкой информации. Название зависело от подхода, выбранного учёными в каждом конкретном случае.

В 1955 году американский учёный в области информатики Джон Маккарти решил организовать [семинар](https://ru.wikipedia.org/wiki/Дартмутский_семинар), посвященный "думающим машинам". Задачей семинара было обсуждение достижений в этой области и разработка новых идей.

На тот момент Маккарти работал в Дартмутском колледже. Поэтому семинар стал известен как Дартмутский семинар. В заявке на проведение мероприятия Маккарти написал: "исследование искусственного интеллекта". Термин "искусственный интеллект" был выбран за свою нейтральность. Такой подход позволил отойти от принятых методов кибернетики и теории автоматов.

В семинаре принимали участие ведущие учёные, занимающиеся теорией управления, теорией автоматов, нейронными сетями, теорией игр и когнитивной психологией.

На семинаре не было предложено принципиально новых идей. Но он позволил познакомиться учёным, которые работали в области "думающих машин". Кроме того на семинаре было решено назвать новую науку искусственный интеллект.

### Символьный подход 1956 – 1974

В исследованиях по искусственному интеллекту с середины 1950-х до конца 1980-х доминировал [**символьный подход**](https://ru.wikipedia.org/wiki/Символический_искусственный_интеллект). Этот подход предлагает представлять задачи в виде понятных человеку символов. Таким способом решались задачи логического вывода и поиска.

В основе символьного подхода лежит [гипотеза Ньюэлла — Саймона](https://ru.wikipedia.org/wiki/Гипотеза_Ньюэлла_—_Саймона) о **физической символьной системе**. Согласно этой гипотезе, для выполнения интеллектуальных операций необходима и достаточна специальная система символов. Другими словами любой интеллектуальный агент должен манипулировать структурами данных, состоящими из символов.

#### Рассуждение как поиск

В этот период появился метод под названием "рассуждение как поиск" (reasoning as search). Позднее он был переименован в метод [**анализа средств и результатов**](https://en.wikipedia.org/wiki/Means–ends_analysis) (means–ends analysis). Его суть заключается в применении алгоритма поиска для решения логических задач. Этот подход напоминает поиск выхода из лабиринта. Когда программа заходит в тупик, она возвращается назад на несколько шагов и пробует другие варианты рассуждений.

Самым ярким примером применения метода "рассуждение как поиск" была программа [General Problem Solver](https://ru.wikipedia.org/wiki/Универсальный_решатель_задач) (универсальный решатель задач). Её разработали Аллен Ньюэлл и Герберт Саймон в 1959 году. Это была улучшенная версия Logic Theorist. Она могла доказывать теоремы евклидовой геометрии и логики предикатов, а также решать шахматные задачи.

#### Обработка естественного языка

В 1960-е годы начались первые исследования в области обработки естественного языка. Дэниэль Бобров разработал программу [STUDENT](https://en.wikipedia.org/wiki/STUDENT_(computer_program)) в 1964 году. Она решала алгебраические задачи, сформулированные на английском языке.

В 1966 году Джозеф Вейценбаум разработал программу [ELIZA](https://ru.wikipedia.org/wiki/Элиза_(программа)). Она была [виртуальным собеседником](https://ru.wikipedia.org/wiki/Виртуальный_собеседник), который парадирует разговор с психотерапевтом. Сегодня подобные программы называются чат-ботами.

#### Микромиры

В конце 1960-х годов Марвин Минский и Сеймур Пейперт работали в лаборатории ИИ Массачусетского технологического института (МТИ). Они предложили сфокусировать исследования ИИ на упрощённых задачах. Такие задачи получили название **микромиры**. Минский и Пейперт утверждали, что полученные в микромирах результаты будут работать и для реальных задач.

Один из примеров микромира — это [мир блоков](https://en.wikipedia.org/wiki/Blocks_world). В нём на плоской поверхности лежат блоки различных цветов и форм. Исследования в рамках мира блоков привело к нескольким инновационным разработкам:

* [Машинное зрение](https://ru.wikipedia.org/wiki/Машинное_зрение), которым занимались [Дэвид Хаффман](https://ru.wikipedia.org/wiki/Хаффман,_Дэвид), Дэвид Уолтс и [Патрик Уинстон](https://ru.wikipedia.org/wiki/Уинстон,_Патрик).

* Робот-манипулятор для перекладывания блоков. Его разработали Минский и Пейперт.

* Программа [SHRDLU](https://ru.wikipedia.org/wiki/SHRDLU). Её разработал [Терри Виноград](https://ru.wikipedia.org/wiki/Виноград,_Терри). Программа выполняла задачи над блоками. Эти задачи формулировались на английском языке.

#### Перцептрон

В 1957 году американский психолог и нейрофизиолог [Фрэнк Розенблатт](https://ru.wikipedia.org/wiki/Розенблатт,_Фрэнк) предложил модель восприятия информации мозгом. Эта модель получила название [**перцептрон**](https://ru.wikipedia.org/wiki/Перцептрон).

Первая версия перцептрона была реализованна в виде программы для компьютера [IBM 704](https://ru.wikipedia.org/wiki/IBM_704). Перцептрон представляет собой искусственную нейронную сеть с особенной архитектурой. Иллюстрации 2-1 приводит схему этой архитектуру.

{caption: "Иллюстрация 2-1. Архитектура перцептрона", height: "50%", width: "100%"}
[Архитектура перцептрона](images/ArtificialIntelligence/perceptron.png)

Сеть состоит трёх слоёв нейронов:

1. S-элементы (сенсорные)
2. A-элементы (ассоциативные)
3. R-элементы (реагирующие)

Чтобы лучше понять структуру перцептрона, проведём аналогию с работой мозга. Перцептрон создаёт набор ассоциаций (A-элементы) между входными стимулами (S-элементы) и реакцией на них (R-элементы). Пример такой реакции — физиологический ответ двигательных нейронов на зрительную информацию.

Перед работой перцептрон необходимо обучить. После этого перцептрон способен определять к какому из классов относится предъявленный ему объект. Розенблатт использовал идеи Хебба и разработал два метода обучения перцептрона.

В 1960 году Розенблатт сконструировал первый [нейрокомпьютер](https://ru.wikipedia.org/wiki/Нейрокомпьютер) [Mark I Perceptron](https://ru.wikipedia.org/wiki/Марк-1). Этот компьютер представлял собой аппаратную реализацию перцептрона. Он работал на аналоговых электронных компонентах. Этими компонентами были фотодетекторы, потенцометры и электромоторы. Основной задачей компьютера было распознавание изображений.

В 1962 году Розенблатт опубликовал книгу "Принципы нейродинамики" (Principles of Neurodynamics). В ней он доказал [теорему сходимости перцептрона](https://ru.wikipedia.org/wiki/Теорема_сходимости_перцептрона). Согласно ей обучение перцептрона всегда приведёт к решению задачи за конечное число шагов.

### Первая зима ИИ 1974 – 1980

#### Финансирование DARPA

Первые успехи в области ИИ привлекли финансирование со стороны министерства обороны США. С 1963 года лаборатории ИИ в МТИ и Стэнфордском университете получали несколько миллионов долларов ежегодно. Эти деньги выделялись не на конкретные проекты, а для конкретных учёных и лабораторий. Благодаря этому, первые исследования ИИ проходили по разным направлениям.

За финансирование отвечало [управление перспективных исследовательских проектов министерства обороны США](https://ru.wikipedia.org/wiki/Управление_перспективных_исследовательских_проектов_Министерства_обороны_США) DARPA. Директор управления Джозеф Ликлайдер верил, что финансирование "людей, а не проектов" приведёт к практическим результатам. Однако, этого не произошло. Фундаментальные исследования не имели прикладных результатов для военной области. В результате сенат США принял поправку Мэнсфилда в 1969 году. Эта поправка обязывала DARPA финансировать только целевые исследования.

Управление DARPA провело собственное исследование. Оно показало, что большинство разработок в области ИИ не принесут пользы в ближайшем будущем. В результате деньги DARPA были на правлены на проекты новых военных систем.

#### Отчет Лайтхилла

В 1973 году британский парламент поручил профессору математики [Джеймсу Лайтхиллу](https://ru.wikipedia.org/wiki/Лайтхилл,_Джеймс) оценить перспективы исследований в области ИИ. Свои результаты Лайтхилл опубликовал в статье "Искусственный интеллект: общий обзор" (Artificial Intelligence: A General Survey). Позднее она стала известна как [отчёт Лайтхилла](https://ru.wikipedia.org/wiki/Отчёт_Лайтхилла).

Лайтхилл высказал пессимистичный прогноз в отношении перспектив [машинного перевода текста](https://ru.wikipedia.org/wiki/Машинный_перевод) и робототехники. Эти направления считались наиболее перспективными. При этом Лайтхилл дал положительные оценки моделированию нейрофизиологических и психических процессов.

Отчёт Лайтхилла стал причиной, по которой правительство Великобритании прекратило финансирование большинства университетских исследований в области ИИ. Это решение привлекло к себе внимание всех европейских стран. В итоге финансирование разработок в новой области сократились по всей Европе.

#### Проблемы систем ИИ

Исследователи ИИ после первых успехов давали очень оптимистичные прогнозы. Некоторые из них сбылись. Однако, для этого новой науке понадобилось гораздо больше времени, чем ожидалось.

Первые системы ИИ успешно справлялись с простыми примерами. Но попытки применить их для реальных задач заканчивались неудачей. Причин этому было несколько. Рассмотрим их по порядку.

Первая проблема связана с ограниченной производительностью компьютеров того времени. Например, памяти компьютера не хватало для словаря программ перевода. Первые версии таких программ хранили только пару десятков слов. Это всё что помещалось в память.

Аналогичная ситуация складывалась с быстродействием. Задачи распознавания образов требовали от компьютера порядка 10^9^ [операций в секунду](https://ru.wikipedia.org/wiki/IPS_(быстродействие)) (1000 мегаинструкций в секунду или MIPS). Но самый быстрый суперкомпьютер 1976 года [Cray-1](https://ru.wikipedia.org/wiki/Cray-1) выполнял всего 160 MIPS.

Вторая проблема ИИ упоминалась в отчёте Лайтхилл. Первые системы ИИ использовали алгоритмы поиска. Эти алгоритмы перебирают все возможные шаги для достижения цели. Такой подход хорошо работал на небольших тестовых примерах и моделях микромиров. Подобные задачи решаются за короткую последовательность шагов. Найти её было очень просто.

При переходе на сложные задачи, число шагов для решения увеличивалось на порядок. Системы ИИ были способные найти решение, но время его поиска оказывалось слишком большим. Эта проблема получила название [**комбинаторный взрыв**](https://ru.wikipedia.org/wiki/Комбинаторный_взрыв). Комбинаторный взрыв означает резкий рост [**временной сложности**](https://ru.wikipedia.org/wiki/Временная_сложность_алгоритма) алгоритма при увеличении входных данных. Временная сложность — это время, которое нужно компьютеру для выполнения алгоритма.

В 1970-е годы была разработана [теория вычислительной сложности](https://ru.wikipedia.org/wiki/Теория_сложности_вычислений). Она показала, что некоторые классы задач не имеют эффективных решений. Это значит, что нет алгоритмов их решения за приемлемое время. Такие задачи называются [**тродноразрешимыми**](https://www.slideshare.net/mkurnosov/12-34608847).

Третья проблема ИИ связана с очевидными фактами об окружающем мире. Для задач компьютерного зрения и машинного перевода нужен огромный объём информации об очевидных вещах. Например, система распознавания образов должна знать очертания предметов разного типа. Если этих знаний у неё нет, система будет путать предметы. То же самое касается обработки естественного языка. Без понимания предмета обсуждения, система будет совершать смысловые ошибки при переводе.

Первые системы ИИ обошли проблему представления знаний о мире благодаря простоте тестовых примеров. Когда исследователи начали решать реальные задачи, эта проблема стала очевидной.

### Развитие ИИ 1980 – 1987

#### Экспертные системы

В 1960-е годы большинство исследователей придерживалось символьного подхода. "Рассуждение как поиск" было доминирующим методом решения задач. Этот метод предлагал общий механизм для решения задач любого типа. Однако, он плохо [**масштабировался**](https://ru.wikipedia.org/wiki/Масштабируемость). Масштабируемость означает повышение производительности системы при добавлении ресурсов. Другими словами поиск не мог решить сложные задачи за приемлемое время из-за комбинаторного взрыва.

В 1965 году группа учёных из Стэнфордского университета начала проект системы ИИ, основанной на знаниях. В группу входили [Эдвард Фейгенбаум](https://ru.wikipedia.org/wiki/Фейгенбаум,_Эдвард_Альберт), Брюс Бьюкенен, [Джошуа Ледерберг](https://ru.wikipedia.org/wiki/Ледерберг,_Джошуа) и [Карл Джерасси](https://ru.wikipedia.org/wiki/Джерасси,_Карл). Система получила название [Dendral](https://ru.wikipedia.org/wiki/Dendral).

Группа Фейгенбаума решала практическую задачу из области органической химии. Химикам часто приходится определять неизвестные органические соединения, анализируя их [масс-спектр](https://ru.wikipedia.org/wiki/Масс-спектрометрия). Масс-спектрометрия измеряет отношение массы фрагментов молекулы к их заряду.

Система Dendral принимала на вход масс-спектр вещества. В ходе анализа, она должна была определить его химический состав. Первая версия системы использовала метод поиска. Она генерировала предполагаемые картины масс-спектра для проверяемого вещества. Затем система сравнивала эти картины с реальными наблюдениями. Этот подход работал для небольших молекул. Однако для сложных органических веществ он оказался неприменим.

Проблема заключалась в том, что системе приходилось проверять много вариантов картин масс-спектра. Разработчики Dendral обсудили это затруднение с химиками. Вместе они пришли к идее, как сократить число потенциальных решений. Для этого в систему добавили знания о типичных картинах пиков в масс-спектре вещества. Эти пики указывают на наличие определённых фрагментов молекул. Такое решение позволило системе успешно различать большие органические молекулы.

Знания о картинах пиков в масс-спектре вещества в Dendral стали первыми эвристиками, основанными на знаниях из предметной области. До этого эвристики строились на правилах формальной логики, как в программе Logic Theorist.

Системы ИИ, которые используют в своей работе знания из предметной области, стали называться **экспертными системами**. Опыт Dendral показал, что этот подход позволяет решать реальные сложные задачи.

В 1972 году Фейгенбаум, Бьюкенен и генетик [Стэнли Коэн](https://ru.wikipedia.org/wiki/Коэн,_Стэнли_Норман) разработали экспертную систему [MYCIN](https://ru.wikipedia.org/wiki/MYCIN). Она диагностировала инфекционные заболевания и давала рекомендации для их лечения.

В 1978 году Джон Макдермотт из университета из университета Карнеги — Меллона разработал первую коммерческую экспертную систему [XCON](https://en.wikipedia.org/wiki/Xcon). Заказчиком системы был производитель компьютеров — компания DEC. Система XCON помогала сотрудникам DEC составлять конфигурации компьютеров, согласно требованиям заказчиков. Экспертная система экономила компании до 40 миллионов долларов в год.

Громкий успех системы XCON привел к тому, что корпорации по всему миру начали разработку экспертных систем для внутреннего использования. В эти разработки включились производители оборудования. Начали появляться специальные компьютеры под названием [Lisp-машины](https://ru.wikipedia.org/wiki/Лисп-машина). Они предназначались для запуска программ на языке [Lisp](https://ru.wikipedia.org/wiki/Лисп). Этот язык стал основным для разработки экспертных систем.

#### Компьютеры пятого поколения

В 1981 году правительство Японии начало спонсировать проект [компьютера пятого поколения](https://ru.wikipedia.org/wiki/Компьютеры_пятого_поколения). В рамках проекта за 10 лет планировалось спроектировать производительный суперкомпьютер и запустить его в серийное производство.

Компьютер должен был решать ряд задач с применением ИИ. Среди этих задач были следующие:

* Распознавание речи и автоматический набор текста.
* Машинный перевод.
* Анализ печатного текста и его категоризация.
* Распознавание образов.
* Саморазвитие системы.

Чтобы реализовать возможности ИИ, разработчики компьютера предложили несколько инновационных для того времени решений. Прежде всего, все вычисления должны были выполняться [параллельно](https://ru.wikipedia.org/wiki/Параллельные_вычисления). Для такого режима работы была разработана многопроцессорная архитектура. Компьютеры того времени имели только один процессор. Он выполнял только одну задачу в каждый момент времени. Многопроцессорная архитектура позволяла решать несколько задач одновременно. Также появлялась возможность делить задачу на подзадачи и выполнять их все разом на нескольких процессорах.

Вторым важным решением стало применение концепции [**логического программирования**](https://ru.wikipedia.org/wiki/Логическое_программирование). Она предлагает представлять программу в виде предложений. Эти предложения сообщают факты и правила о некоторой предметной области. С помощью формальной логики из этих фактов выводится нужная информация. Данные для логического вывода хранились не в файловой системе, а в базе данных.

Таблица 2-1 объясняет отличительные черты разных поколений компьютеров.

{caption: "Таблица 2-1. Поколения компьютеров", width: "100%"}
| Поколение | Рабочий элемент |
| --- | --- |
| Первое | [Электровакуумная лампа](https://ru.wikipedia.org/wiki/Электронная_лампа) |
|  | |
| Второе | [Транзистор](https://ru.wikipedia.org/wiki/Транзистор) |
|  | |
| Третье | [Интегральная схема](https://ru.wikipedia.org/wiki/Интегральная_схема) |
|  | |
| Четвёртое | [Микропроцессор](https://ru.wikipedia.org/wiki/Микропроцессор) |

Другие страны ответили на амбициозные планы Японии. В 1984 году правительство Великобритании начало спонсировать программу [Alvey](https://en.wikipedia.org/wiki/Alvey). Целью программы было не создание суперкомпьютера, а исследования в области информационных технологий. Среди направлений исследований были следующие:

* Сверхбольшие интегральные схемы (VLSI)
* Системы ИИ, основанные на знаниях.
* Разработка программ.
* Интерфейс взаимодействия человека и машины.

Группа американских компаний объединилась и создала [корпорацию MCC](https://en.wikipedia.org/wiki/Microelectronics_and_Computer_Technology_Corporation), также известную как Консорциум Микроэлектрони и Компьютеров. Эта корпорация должна была спонсировать фундаментальные исследования в области ИИ и информационных технологий. В то же время управление министерства обороны США DARPA утроило финансирование в области ИИ.

#### Возвращение коннекционизма

На заре искусственного интеллекта существовало два принципиально разных направления: символьный подход и [**коннекционизм**](https://ru.wikipedia.org/wiki/Коннекционизм). Иногда выделяют ещё третий подход [**логицизм**](https://brickofknowledge.com/articles/logic-and-artificial-intelligence#header0), который близок к символьному подходу.

Таблица 2-2 приводит общую информацию о подходах в ИИ.

{caption: "Таблица 2-2. Подходы в ИИ", width: "100%"}
| Название | Идея | Основоположники |
| --- | --- | --- |
| Символьный подход | Задача решается через действия над понятными человеку символическими обозначениями. | Аллен Ньюэлл |
|  | | Герберт Саймон |
|  | | |
| Логицизм | Задача решается методами логического рассуждения. | Джон Маккарти |
|  | | |
| Коннекционизм | Задача решается сетью из связанных между собой простых элементов (нейронной сетью). | Уолтер Питтс |
|  | | Уоррен Мак-Каллок |

Коннекционизм — это направление, которое пытается объяснить психические явления с помощью искусственных нейронных сетей. Первые исследования нейронных сетей в 1950-е годы заложили основу коннекционизма. Однако это направление не пользовалось популярностью среди учёных. Большинство из них придерживалось символьного подхода. Кроме того наиболее авторитетные исследователи ИИ критиковали коннекционизм. Примером такой критики была книга "Перцептроны" Марвина Мински и Сеймура Пейперта, опубликованная в 1969 году.

В 1970-е и 1980-е годы исследования в рамках коннекционизма проводили учёные не связанные с ИИ. Один из них — американский физик Джон Хопфилд. Он использовал методы из статистической механики, чтобы изучить свойства хранения данных в нейронной сети. В результате Хопфилд разработал [новую архитектуру нейронной сети](https://ru.wikipedia.org/wiki/Нейронная_сеть_Хопфилда), которая была названа в его честь. Эта сеть стала моделью для изучения человеческой памяти.

В начале 1980-х годов психологи [Джеффри Хинтон](https://ru.wikipedia.org/wiki/Хинтон,_Джеффри) и [Дэвид Румельхарт](https://ru.wikipedia.org/wiki/Румельхарт,_Дэвид) из университета Карнеги — Меллона исследовали процесс обучения и модель памяти на основе нейронных сетей. Они применили существующие наработки и предложили **метод обратного распространения ошибки**. Этот метод позволял эффективно обучать [нейронные сети с прямой связью](https://ru.wikipedia.org/wiki/Нейронная_сеть_с_прямой_связью).

В 1986 году Румельхарт и [Джеймс Макклелланд](https://ru.wikipedia.org/wiki/Макклелланд,_Джеймс) опебликовали цикл статей под названием "Параллельная распределенная обработка" (Parallel Distributed Processing). В этом цикле авторы демонстрировали области применения метода обратного распространения ошибки в компьютерных науках и психологии. Благодаря статьям Румельхарта и Макклелланд, исследователи ИИ снова заинтересовались коннекционизмом.

### Вторая зима ИИ 1987 — 1993

В 1984 году состоялась ежегодная встреча Американской ассоциации искусственного интеллекта (AAAI). На одном обсужденийй Марвин Мински и Роджер Шэнк обратились к предпринимателям. Они предупредили о завышенных ожиданиях от исследований в области ИИ.

На протяжении 1980-х годов инвестиции коммерческих компаний в ИИ постоянно росли. К 1985 году корпорации по всему миру тратили на разработку экспертных систем свыше миллиарда долларов. Появилось много новых компаний, который предлагали программные и аппаратные решения для нового направления в ИИ. Однако, в 1987 году этот рынок рухнул по сценарию [экономического пузыря](https://ru.wikipedia.org/wiki/Экономический_пузырь). Большинство компаний не смогли выполнить свои обещания и предоставить рабочие решения в срок. Это разочаровало инвесторов и поставило под сомнение, что разработки в области ИИ прибыльны с коммерческой точки зрения.

#### Банкротство производителй Lisp-машин

В конце 1979-х сотрудники лаборатории ИИ Массачусетского технологического института начали работать над специальным компьютером для запуска экспертных систем. Языком для разработки этих систем был Lisp. Поэтому новые компьютеры были нацелены на исполнение только Lisp-программ и получили название Lisp-машины.

На базе института была создана компания [Lisp Machines](https://en.wikipedia.org/wiki/Lisp_Machines) в 1979 году. Её первым клиентом стала компания [Control Data Corporation](https://ru.wikipedia.org/wiki/Control_Data_Corporation) — крупный производитель компьютерной переферии и суперкомпьютеров.

В 1980-ом году часть сотрудников покинули Lisp Machines чтобы организовать новую компанию [Symbolics](https://en.wikipedia.org/wiki/Symbolics). Позднее к производству Lisp-машин подключились крупные производители электроники: [Texas Instruments](https://ru.wikipedia.org/wiki/Texas_Instruments) и [Xerox](https://ru.wikipedia.org/wiki/Xerox). Рынок специализированных компьютеров выглядел перспективно и обещал долгосрочный рост. Инвестиции в разработки Lisp-машин достигли полмилииарда долларов.

Однако, в 1987 году рынок Lisp-машин неожиданно рухнул. Причиной этого стало появление [рабочих станций](https://en.wikipedia.org/wiki/Workstation) от компании Sun Microsystems. Рабочая станция отличается от персонального компьютера высокой ценой и производительностью. Её аппаратное обеспечение оптимизированно для задач визуализации, 3D моделирования и математических рассчётов.

[Рабочие станции Sun Microsystems](https://ru.wikipedia.org/wiki/Sun-1) изначально не поддерживали язык Lisp. Это останавливало пользователей экспертных систем от перехода на более мощное оборудование. Но компания [Lucid](https://en.wikipedia.org/wiki/Lucid_Inc.) разработала интерпретатор языка Lisp для операционной системы Unix. Под управлением этой системы работали компьютеры Sun.

Кроме интерпретатора от компании Lucid в 1980-е годы в университете Беркли был разработан интерпретатор [Franz Lisp](https://en.wikipedia.org/wiki/Franz_Lisp). В 1982 года его портировали на компьютеры Sun.

В середине 1980-х годов относительно недорогие ПК от компаний Apple и IBM достигли производительности Lisp-машин. В скором времени на ПК были портированы популярные интерпретаторы языка Lisp. С этого момента пользователям не было никакого смысла переплачивать за специальное оборудование. Компании стали массово отказываться от использования Lisp-машин.

#### Проблемы экспертных систем

К началу 1990-х годов компании накопили достаточно опыта в использовании экспертных систем. Этот опыт показал, что обслуживание таких систем обходится дорого. Основная проблема заключалась в обновлении больших баз знаний. Сама система не имела функции обучения. Поэтому изменения делались вручную специалистами. Во время этих изменений требовалось проверять всю базу знаний на согласованность. Это требовало много времени и значительных усилий.

Научные круги к этому времени потеряли интерес к экспертным системам. Разработки в области архитектуры баз знаний показали, что представление общих знаний о реальном мире — трудновыполнимая задача. Один из примеров таких разработок — проект [Cyc](https://ru.wikipedia.org/wiki/Cyc) под руководством [Дугласа Лената](https://ru.wikipedia.org/wiki/Ленат,_Дуглас). Этот проект неоднократно подвергался критики со стороны научного сообщества. Многие считают его бесполезным.

Экспертные системы показали свою полезность в некоторых узкоспециализированных областях. Однако, разработчики этих систем не смогли выполнить своих обещаний по созданию ИИ. Это разочаровало инвесторов. В результате многие научные исследования и компании потеряли финансирование.

#### Провал разработки компьютеров пятого поколения

К 1991 году разработка компьютера пятого поколения в Японии не достигла поставленных целей. В ходе проекта были созданы несколько рабочих станций. Они использовали специальное аппаратное обеспечение с многопроцессорной архитектурой.

К моменту своего появления рабочие станции оказались устаревшими. Причиной этого стало появление мощных ПК. Проект длился 10 лет. В момент его начала исследователи предполагали, что однопроцессорные компьютеры достигли предела производительности. Поэтому, по их мнению, увеличить мощность могли только параллельные вычисления. Однако, эти прогнозы оказались ошибочными. В результате к 1991 году производительность недорогих ПК, собранных на стандартных компонентах, превзошла производительность рабочих станций пятого поколения.

Кроме роста производительности процессоров появилось несколько новых технологий. В 1984 ПК от компании Appl начали использовать графический интерфейс. В начале 1990-х появился Internet. Компьютер пятого поколения не поддерживал эти передовые технологии. В результате он стал неинтересен пользователям.

Другая проблема компьютера пятого поколения связана с программным обеспечением. Разработчики выбрали язык [Prolog](https://ru.wikipedia.org/wiki/Пролог_(язык_программирования)) для написания программ. Этот язык следует концепции логического программирования. Эта концепция была ключевой в новом компьютере. Однако, Prolog не использует преимущества параллельных вычислений. Все попытки добавить поддержку параллельности в язык не дали ожидаемого результата.

Возможности ИИ, которые планировалось реализовать в компьютере пятого поколения, были переоценены. Производительности оборудования и уровня исследования ИИ того времени не хватило для задач распознавания образов и машинного перевода текстов. Идея саморазвития системы оказалась непродуктивной. После нескольких самостоятельных изменений система теряла надёжность и становилась неработоспособной.

Провал проекта вызвал реакцию в США и Англии. Эти страны прекратили разработки суперкомпьютеров и сократили инвестиции в исследования ИИ.

### Развитие ИИ в 1993–2011

#### Научный подход в ИИ

С 1970-х годов ииследования в области ИИ проводились [двумя принципиально разными путями](https://en.wikipedia.org/wiki/Neats_and_scruffies).

Первый подход назывался **neat**, что по-английски означает "аккуратный". Лучше всего смысл этого подхода передаёт слово "теоретический". Исследователь-теоретик применяет только [**формальные методы**](https://ru.wikipedia.org/wiki/Формальные_методы). Эти методы основаны на математике и логике. Такой подход даёт более надёжный и научно обоснованный результат. Ожидается, что такой путь приведёт к идее или методу, который можно обобщить и на его основе создать ИИ.

Второй подход называется **scruffy** или "неряшливый". Его лучше назвать "практическим". Исследователи-практики пробуют разные алгоритмы и методы, чтобы добиться от системы нужного поведения. Они утверждают, что для создания ИИ необходимо решить ряд несвязанных между собой задач. При этом ни одна программа не сможет стать интеллектуальной самостоятельно.

Споры между теоретиками и практиками продолжались до 1990-х годов. Они прекратились, когда новейшие подходы математики и статистики стали применяться в ИИ. Благодаря им, появилось новое направление в разработке программ — [**математическая оптимизация**](https://ru.wikipedia.org/wiki/Оптимизация_(математика)). Кроме того эти подходы подстегнули развитие нейронных сетей. Такие направления как [**машинное обучение**](https://ru.wikipedia.org/wiki/Машинное_обучение) и компьютерное зрение достигли больших успехов.

I> Машинное обучение — это класс методов в ИИ. Эти методы не решают поставленную задачу, а обучают систему на примерах решения аналогичных задач. После процесса обучения система решает поставленную задачу сама.

Переход к формальным методам позволили исследователям ИИ использовать достижения других наук. Среди них теория вероятностей и теория принятия решений. Использование математического аппарата ускорилоло обмен идеями между ИИ и смежными с ним науками.

#### Интеллектуальные агенты

В 1990-х годах концепция интеллектуальных агентов получила признание среди исследователей ИИ. Эта концепция пришла в ИИ из экономики и теории принятия решений. Её прототипом стало понятие рацинального агента из экономики. Главная особенность интеллектуального агента в том, что он пытается максимизировать свою эффективность при решении поставленной задачи.

Понятие интеллектуального агента разрешило несколько философских проблем в области ИИ. Первая из них заключалась в спорах о том, что считается интеллектом. Тест Тьюринга подчёркивал сравнение искусственных систем с человеческим интеллектом. Понятие рационального агента устранило это сравнение. Теперь интеллектуальным стало считаться любое поведение, которое ведеёт к эффективному достижению цели.

Кроме того теперь потеряли смысл обсуждения о том, может ли машина обладать сознанием и настоящим пониманием. Эти свойства стали неважны для интеллектуального агента. Исследователи ИИ смогли сконцентрироваться на решении конкретных практических задач. Формальные методы позволили им находить решения, правильность которых можно было доказать.

Новая концепция помогла исследователям ИИ и с практической точки зрения. Она позволила непосредственно сравнивать эффективность различных систем, основанных на разных подходах. Принцип работы системы отошёл на второй план. Раньше каждый исследователь работал только в рамках одного подхода: символьный или коннекционизм. Теперь система оценивалась более объективно по своим результатам. Начались попытки совмещения разных подходов для повышения эффективности.

#### Восстановление репутации ИИ

Многие инвесторы разочаровались в области ИИ после краха рынка Lisp-машин, проблем с экспертными системами и провала проекта компьютера пятого поколения. Это вызвало сокращение финансирования фундаментальных исследований и разработок.

На протяжении 1990-х годов многие исследователи избегали термина ИИ, когда описывали свою работу. Вместо этого они говорили об информатике, когнитивных системах и вычислительном интеллекте. Так исследователи имели больше шансов найти финансирование. В результате многие достижения в области ИИ стали считаться частью информатики. Например, к этим достижениям относятся алгоритмы поиска и способы представления данных.

В конце 1990-х и начале 2000-х годах исследователи ИИ смогли продемонстрировать несколько впечатляющих успехов. Эти успехи вызвали широкий резонанс в обществе. Они частично восстановили испорченную репутацию науки об ИИ.

В 1997 году суперкомпьютер [DeepBlue](https://ru.wikipedia.org/wiki/Deep_Blue) от IBM смог обыграть чемпиона мира по шахматам Гарри Каспарова.

В 2005 году робот, разработанный в Стэнфордском университете, победил в гонках [DARPA Grand Challenge](https://ru.wikipedia.org/wiki/DARPA_Grand_Challenge). Автономно управляемая машина смогла проехать весь маршрут в 211 километров по пустыне Мохаве. На это у робота ушло почти семь часов.

В 2011 году ИИ ситема [Watson](https://ru.wikipedia.org/wiki/IBM_Watson) от компании IBM смогла обыграть двух лучших игроков в телевизионной игре-викторине Jeopardy!. В этой игре участники отвечают на вопросы из области общих знаний.

Эти успехи удалось достичь благодаря увеличению производительности компьютеров и качеству разработки программ в 1990-х годах. Каждая из нашумевших ИИ систем использовала хорошо известные подходы, разработанные ещё в 1960-х годах.

### Современные направления в ИИ

#### Big Data

Big data или [**большие данные**](https://ru.wikipedia.org/wiki/Большие_данные) — это одно из современных направлений в ИИ. Оно изучает методы обработки наборов данных, которые слишком велики для применения традиционных подходов.

Направление big data возникло по двум причинам:

* [Компьютеризация](https://ru.wikipedia.org/wiki/Компьютеризация) общества. Все больше компаний и правительственных учреждений используют компьютеры. В результате накапливаются огромные объёмы информации.

* Распространение портативных устройств, которые способны накапливать информацию. Примеры таких устройств: мобильные телефоны, телевизоры, камеры наблюдения, микрофоны.

Исследователи big data используют следующие технологии:

* Машинное обучение.
* [Обработка естсественных языков](https://ru.wikipedia.org/wiki/Обработка_естественного_языка).
* [Облачные вычисления](https://ru.wikipedia.org/wiki/Облачные_вычисления).
* [Базы данных](https://ru.wikipedia.org/wiki/База_данных).
* Визуализация данных.

Сегодня big data активно применяется в следующих областях:

* Государственные учреждения. Они обрабатывают личную информацию граждан и ведут разного рода статистику.

* Здравоохранение. Методы big data применяются для анализа результатов клинических исследований. Также они помогают вести статистику заболеваний.

* Экономика. Для составления прогнозов и планирования крупных проектов приходится обрабатывать значительные объёмы данных.

* Страховые компании должны обрабатываеть данные о своих клиентах.

* Информационные технологии. Многие информационные системы собирают стсатистику об их использовании. Это помогает разработчикам улучшать свои продукты.

#### Deep Learning

Перцептрон Розенблатта и нейронные сети 1980-х годов моделировали работу мозга. Для этого исследователи ИИ применяли знания из области нейрофизиологии. Низкая производительность компьютеров ограничивала размеры этих сетей. Число нейронов в них было на несколько порядков меньше, чем в мозге мыши. Это привело к скромным достижениям коннекционизма в 1980-е годы по сравнению с символьным подходом.

В 1986-ом году в машинном обучении появилось новое направление под названием deep learning. Оно также известно как [**глубокое обучение**](https://ru.wikipedia.org/wiki/Глубокое_обучение). Отличительная особенность этого подхода в архитектуре нейронных сетей. Эти сети называются **глубокими** или **многослойными**. В отличие от перцептрона Розенблатта они имеют более трёх слоёв. Благодаря такой архитектуре, алгоритм обучения может извлекать признаки разного уровня асбтракции из входных данных. Например, алгоритм машинного зрения может различать на низком уровне отдельные линии, а на более высоком цифры и буквы.

Активные исследования по изучению многослойных сетей начались в середине 1960-х годов. В 1967 году советский математик [Алексей Ивахненко](https://ru.wikipedia.org/wiki/Ивахненко,_Алексей_Григорьевич) разработал первый алгоритм обучения многослойных перцептронов.

В 1980-м году японский учёный Кунихико Фукусима предложил архитектуру многослойной сети под названием [**неокогнитрон**](https://ru.wikipedia.org/wiki/Неокогнитрон). Она предназначалась для распознавания образов.

Исследования многослойных нейронных сетей продолжались на протяжении 1980-х годов. Однако, только в 2000-х годах глубокие сети стали широко использоваться. Причин этому несколько:

* Недостаточно проработанная теория обучения глубоких сетей.

* Возросла общая производительность компьютеров.

* В различных предметных областях были накоплены большие наборы данных для обучения сетей.

* Появилось специальное оборудование ([графические процессоры](https://ru.wikipedia.org/wiki/Графический_процессор) или GPU), которое на порядок ускорило обучение сетей.

Все эти проблемы удалось преодолеть к концу 2000-х годов. Поэтому в начале 2010-х глубокие нейронные сети стали занимать призовые места на конкурсах по распознованию образов.

Сегодня глубокие нейронные сети показывают хоршие результаты в следующих областях:

* Распознавание речи.
* Распознавание образов.
* Машинный перевод.
* Системы рекомендаций.
* Медицинская диагностика.
