## 3.4 Теория ИИ и шахматы

Мы познакомились с идеями Клода Шеннона. Он применил теорию игр к шахматам. Так учёный разработал идеи и алгоритмы для реализации шахматных программ.

Рассмотрим теперь шахматы с точки зрения теории интеллектуальных агентов. Это расскажет нам о возможных архитектурах и способах реализации шахматных программ.

### 3.4.1 Проблемная среда

Игра в шахматы — это проблемная среда. Её свойства однозначно определяют классы агентов, которые способны в ней работать. Поэтому на первом шаге определим свойства проблемной среды, согласно таблице 2-4. Так мы определим её тип и узнаем классы подходящих агентов по таблице 2-6.

Агент знает расположение всех фигур на доске в любой момент времени. Кроме этого, он имеет полную информацию обо всех произошедших игровых событиях. Эти знания легко доступны для каждого участника партии, потому что в шахматах нет скрытых состояний. Следовательно, шахматы — это **полностью наблюдаемая** среда.

В шахматах нет случайных событий. Каждая новая позиция фигур на доске зависит только от трёх факторов:

1. Предыдущая позиция на доске.
2. Действие агента.
3. Действие оппонента.

Это означает, что каждое следующее состояние среды определяется её текущим состоянием и действиями агентов. Такая среда является **детерминированной**.

В шахматах каждое действие агента влияет на будущие состояния среды. Он не может принимать решения, исходя только из текущей позиции. Вместо этого агент должен планировать свои действия и их результат наперёд. Поэтому шахматы являются **последовательной** средой.

В шахматах игроки делают ходы друг за другом. Позиция на доске меняется только после очередного хода. Это означает, что пока агент принимает решение, среда остаётся неизменной. Такое поведение характерно для статических сред. С другой стороны, на турнирных партиях каждому участнику отводится определённое время на все ходы. Когда оно заканчивается, игроку засчитывается поражение. Поэтому показатели производительности агента меняются с течением времени. Это означает, что шахматы являются **полудинамической** средой.

В шахматах фигуры могут находиться только на определённых полях доски. Поэтому агент способен однозначно различать все возможные игровые состояния. Их количество огромно, но конечно. Также различимы и конечны игровое время и ходы участников. Следовательно, шахматы — это **сложная дискретная** среда.

В шахматной партии только два участника. Победа одного из них означает поражение другого. Таким образом показатели производительности игроков обратно пропорциональны. Из этого следует, что шахматы — это **мультиагентная конкурентная** среда.

В шахматах не так много правил. Они определяют допустимые ходы фигур и условия окончания партии. Эти правила легко представить в удобной для агента форме такой как таблица или программа. Знание правил обязательно для агента. Без них он не сможет делать ходы. Поэтому шахматы — это **известная агенту** среда.

Подведём итог. С точки зрения теории интеллектуальных агентов шахматы — это проблемная среда со следующими признаками:

* Полностью наблюдаемая
* Детерминированная
* Последовательная
* Полудинамическая
* Дискретная
* Мультиагентная и конкурентная
* Известна агенту
* Сложная

### 3.4.2 Интеллектуальный агент

Мы определили тип проблемной среды, которую представляют собой шахматы. Теперь выясним, агенты каких классов способны в ней работать.

Таблица 2-6 приводит шесть классов агентов. Их можно разделить на следующие три группы:

1. Рефлексные агенты.
2. Агенты, использующие поиск.
3. Обучающиеся агенты.

Агенты одной группы используют похожий алгоритм работы. Например, все рефлексные агенты основаны на таблице правил или формальной логике. Это означает, что их архитектура и принцип работы очень близки.

Для удобства мы рассмотрим не каждый из шести классов агентов, а каждую из трёх групп.

#### 3.4.2.1 Рефлексные агенты

Может ли играть в шахматы какой-то из рефлексных агентов? Чтобы ответить на этот вопрос, начнём с **простого рефлексного агента**. Он полностью игнорирует последовательность актов восприятия. При выборе действия агент учитывает только текущее состояние среды. Он не рассматривает последствия своих действий и будущие возможные состояния среды.

Несмотря на все свои ограничения, простой рефлексный агент может работать в среде, похожей на шахматы. Например, его возможностей достаточно, чтобы реализовать оптимальную стратегию игры в крестики-нолики.

Крестики-нолики считаются [решённой игрой](https://en.wikipedia.org/wiki/Solved_game). Для каждого игрока найдена оптимальная стратегия, которая приводит к наилучшему результату. Если оба игрока придерживаются своей оптимальной стратегии, партия всегда заканчивается вничью.

Для игры в крестики-нолики простому рефлексному агенту нужна таблица правил. Она должна содержать следующую информацию:

* Все возможные состояния поля игры, допустимые правилами.

* Ход агента для каждого возможного состояния поля, который реализует оптимальную стратегию.

Используя такую таблицу, агент будет выбирать лучший ход на любое действие противника.

Выясним размер таблицы правил с реализацией оптимальной стратегии. Всего на поле игры 9 клеток. Каждая клетка имеет одно из трёх состояний: пустая, нолик, крестик. Поэтому всего есть 3^9^ = 19683 возможных состояний игрового поля. Среди них есть варианты, когда все клетки заполнены крестиками и все клетки — нолики. Такие ситуации не могут возникнуть в реальной игре. Поэтому из 19683 возможных состояний поля только 5478 состояний удовлетворяют правилам игры. Из этих расчётов мы делаем вывод, что таблица правил простого рефлексного агента должна иметь 5478 строк.

Каждая строка таблицы правил — это состояние игрового поля из 9 клеток плюс соответствующее ему действие агента. Действие агента можно закодировать в 9 битах. Каждый бит соответствует ходу в одну из девяти клеток. Состояние поля можно сохранить в `9 * 3 = 27` битах. Это три состояния для каждой из девяти клеток: пустая, крестик или нолик. Итого мы получаем размер таблицы `5478 * (27 + 9) = 197208` бит или 24651 байт.

Мы выяснили, что для игры в крестики-нолики простому рефлексному агенту нужна таблица правил размером около 24 Кбайт. Теперь оценим размер такой таблицы для шахмат. Согласно расчётам Клода Шеннона, существует около 10^120^ возможных позиций на шахматной доске. Для удобства сравнения грубо округлим число возможных состояний в крестиках-ноликах до 10^4^. Тогда получится, что в шахматах состояний игрового поля на 116 порядков больше.

Для простоты мы не будем учитывать, что запись в таблице правил для шахмат занимает больше памяти, чем для крестиков-ноликов. Причина в том, что на шахматной доске больше полей, а у игроков больше вариантов действий. Даже несмотря на это, простому рефлексному агенту нужна таблица правил размером порядка 10^120^ байт, чтобы играть в шахматы. Работа с таким объёмом информации выглядит недостижимой для современных компьютерных технологий.

У простого рефлексного агента для игры в шахматы есть ещё одна проблема. Шахматы до сих пор не решены. Это означает, что оптимальные стратегии для белых и чёрных неизвестны. Поэтому если бы даже компьютерные технологии позволили работать с таблицей правил размером 10^120^ байт, её содержимое остаётся загадкой.

Есть два более продвинутых класса рефлексных агентов:

* Рефлексный агент с внутренним состоянием.

* Рефлексный агент, основанный на модели.

Внутреннее состояние решает проблему с запоминанием прошлых ходов. Но оно никак не поможет агенту в планировании действий наперёд. Поэтому он должен опираться на таблицу правил, как и простой рефлексный агент. Как мы выяснили, на практике такая реализация невозможна.

Модель помогает рефлексному агенту прогнозировать изменения среды. Но она работает только для простых проблемных сред. Шахматы — это сложная среда. При попытке применить к ней модель возникает комбинаторный взрыв. Количество состояний среды и переходов между ними оказывается слишком велико для просчёта с помощью модели. Поэтому такой рефлексный агент также не способен играть в шахматы.

Мы пришли к выводу, что шахматы слишком сложны для алгоритмов рефлексных агентов. Теоретически создать такого агента возможно. Но на практике это потребует вычислительных ресурсов, которых просто не существует.

#### 3.4.2.2 Агенты, использующие поиск

Рассмотрим интеллектуальных агентов, которые строятся на алгоритмах поиска. Эти алгоритмы делятся на два класса:

* Классический поиск (неинформированный и информированный).

* Состязательный поиск.

Классический поиск хорошо работает в одноагентных средах, но не справляется с конкурентными мультиагентными. Причина в том, что алгоритм учитывает действия агента и полностью игнорирует противодействие оппонента. Шахматы как раз являются конкурентной мультиагентной средой. **Агенты, основанные на цели,** используют алгоритмы неинформированного поиска. Поэтому ни один из агентов этого класса не способен играть в шахматы.

**Агенты, основанные на полезности,** используют алгоритмы информированного и состязательного поиска. Первый тип относится к классическому поиску, поэтому основанные на нём агенты не могут играть в шахматы. Второй тип — состязательный поиск. Такие алгоритмы строятся на методах теории игр.

Здесь мы приходим к выводам Клода Шеннона. Его стратегия типа А — это применение поиска минимакс к игре в шахматы. Учёный не привёл строгих доказательств, что предложенный им алгоритм работает. Но мы можем оценить его перспективность с точки зрения теории интеллектуальных агентов.

Может ли агент, построенный на состязательном поиске, играть в шахматы? Из таблицы 2-6 мы знаем, что агент, основанный на полезности, успешно работает в средах со следующими характеристиками:

* Полностью наблюдаемая
* Детерминированная
* Последовательная
* Статическая
* Дискретная
* Мультиагентная и конкурентная
* Неизвестная
* Сложная

У этого списка есть два отличия от характеристик шахмат. Во-первых, шахматы являются средой, известной агенту. Алгоритмы состязательного поиска справляются с неизвестными средами. Это означает, что они также успешно работают и в более простых известных средах.

Во-вторых, шахматы являются полудинамической средой. Конкретнее, у агента ограниченно время на выбор своего действия. Это означает, что алгоритм состязательного поиска должен давать удовлетворительный результат даже тогда, когда он не может отработать до конца. Такое ограничение может стать критическим.

Когда мы говорим о работе состязательного поиска в полудинамической среде, надо учитывать сложность этой среды. Число Шеннона показывает сложность дерева игры в шахматы. Она настолько велика, что у алгоритма поиска почти никогда не будет возможности доработать до конца и просчитать партию до последнего хода. Клод Шеннон оценил, что алгоритм минимакс сможет просчитывать позицию на три хода вперёд. Сделаем грубую поправку на мощность современных компьютеров и умножим эту оценку на два. Получим просчёт на шесть ходов вперёд. Для качественной игры этого по-прежнему недостаточно.

Проблему сложной полудинамической среды могут решить модификации алгоритма минимакс, например альфа-бета отсечение. Эти модификации удаляют из рассмотрения целые поддеревья, опираясь на некоторые эвристики. Такое отсечение сокращает дерево игры более чем в четыре раза. Это приближает глубину просчёта ходов алгоритмом поиска к возможностям профессионального шахматиста. В этом случае уже можно рассчитывать на качественную игру со стороны агента.

Мы пришли к выводу, что теоретически агент, построенный на состязательном поиске, может играть в шахматы. Для его реализации на практике надо решить ряд технических задач:

1. Разработать достаточно точную функцию оценки позиций.

2. Разработать высокопроизводительную аппаратную платформу для генерации ходов и оценки шахматных позиций.

3. Найти компромисс для эвристик поиска: они должны отсекать значительное количество ветвей дерева игры, но при этом оставлять ветви, ведущие к лучшим ходам.

Несмотря на эти технические трудности, нет никаких принципиальных ограничений для сильной игры агента в шахматы. В это означает, что стратегия типа А Клода Шеннона должна работать на практике.

#### 3.4.2.3 Обучающиеся агенты

Согласно таблице 2-6, обучающиеся агенты могут работать в средах намного более сложных, чем шахматы. Вопрос заключается в том, какие именно алгоритмы машинного обучения смогут создать подходящего для игры агента.

Мы познакомились с тремя парадигмами алгоритмов обучения:

* Обучение с учителем.
* Обучение без учителя.
* Обучение с подкреплением.

Первые две парадигмы хорошо показывают себя в эпизодических средах. Обучение без учителя решает задачи, которые не встречаются в обычной шахматной партии. Примеры таких задач: кластеризация, обнаружение аномалий, поиск ассоциативных правил. Это может быть полезно при анализе и выводе статистики из большого количества уже сыгранных партий. Но в самой игре подобные решения не нужны. Поэтому алгоритмы обучения без учителя вряд ли смогут создать агента, играющего в шахматы.

Обучение с учителем более перспективно. В теории, такой алгоритм может построить достаточно точную функцию оценки позиций. Для этого надо подготовить большой обучающий набор позиций фигур. Это должны быть размеченные данные, то есть каждой позиции эксперт шахматист должен дать свою оценку. В результате обучения на таком наборе, алгоритм построит функцию, которая оценивает позиции почти также, как профессиональный шахматист. Проблема в том, что функция оценки — это только часть агента, играющего в шахматы. Для выбора ходов ему понадобится ещё некоторый алгоритм поиска. Это означает, что только обучения с учителем недостаточно для создания полноценного агента.

Алгоритмы обучения с подкреплением способны создать агента, работающего в последовательной мультиагентной конкурентной среде. Нет никаких принципиальных ограничений, чтобы такой агент смог играть в шахматы. Единственная проблема заключается в высокой сложности дерева игры. Это создаёт ряд технических трудностей, которые придётся решить разработчикам агента:

1. Разработать достаточно сложную архитектуру модели. Она должна учитывать стратегические и тактические аспекты шахматной игры.

2. Разработать высокопроизводительную программно-аппаратную платформу для длительного выполнения алгоритма обучения.

3. Разработать решение для тестирования и сравнения производительности разных версий агента.

Разработка обучающегося агента для игры в шахматы будет итеративным процессом. Прежде всего потому, что создание подходящей архитектуры модели представляет собой поиск компромисса. Слишком сложная модель требует долгого обучения, которое может и не сходиться. Слишком простая модель не сможет научиться сильной игре.

I> [**Сходимостью**](https://wiki.loginom.ru/articles/convergence-of-algorithm.html) (convergence) алгоритма обучения называется свойство достигать оптимума целевой функции или подходить достаточно близко к нему за конечное число шагов. При этом параметры модели (веса и смещения) устанавливаются в значения, которые не меняются при дальнейшем обучении.

Мы пришли к выводу, что обучающийся агент может играть в шахматы. Но создание такого агента очень трудоёмко. Разработчикам понадобятся огромные вычислительные ресурсы и время, чтобы построить подходящую модель.

### 3.4.3 Агент для игры в шахматы

Шахматы представляют собой проблемную среду, в которой могут работать следующие виды агентов:

1. Агент, использующий состязательный поиск.

2. Агент, использующий состязательный поиск, с функцией оценки, построенной алгоритмом обучения с учителем.

3. Агент, построенный алгоритмом обучения с подкреплением.

К такому выводу мы пришли, опираясь на теорию интеллектуальных агентов. Теперь рассмотрим наиболее успешные шахматные программы. Мы выясним, какие именно подходы выбрали их разработчики.

{pagebreak}
