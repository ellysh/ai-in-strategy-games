## 3.3 Теория ИИ и шахматы

Мы познакомились с идеями Клода Шеннона. Он применил теорию игр к шахматам. Так учёный разработал идеи и алгоритмы для реализации шахматных программ.

Рассмотрим теперь шахматы с точки зрения теории интеллектуальных агентов. Это расскажет нам о возможных архитектурах и способах реализации шахматных программ.

### 3.3.1 Проблемная среда

Игра в шахматы — это проблемная среда. Её свойства однозначно определяют классы агентов, которые способны в ней работать. Поэтому на первом шаге определим свойства проблемной среды, согласно таблице 2-4. Так мы определим её тип и узнаем классы подходящих агентов по таблице 2-6.

Агент знает расположение всех фигур на доске в любой момент времени. Кроме этого он имеет полную информацию обо всех произошедших игровых событиях. Эти знания легко доступны для каждого участника партии, потому что в шахматах нет скрытых состояний. Следовательно, шахматы — это **полностью наблюдаемая** среда.

В шахматах нет случайных событий. Каждая новая позиция фигур на доске зависит только от трёх факторов:

1. Предыдущая позиция на доске.
2. Действие агента.
3. Действие оппонента.

Это означает, что каждое следующее состояние среды определяется её текущим состоянием и действиями агентов. Такая среда является **детерминированной**.

В шахматах каждое действие агента влияет на будущие состояния среды. Он не может принимать решения, исходя только из текущей позиции. Вместо этого агент должен планировать свои действия и их результат наперёд. Поэтому шахматы являются **последовательной** средой.

В шахматах игроки делают ходы друг за другом. Позиция на доске меняется только после очередного хода. Это означает, что пока агент принимает решение, среда остаётся неизменной. Такое поведение характерно для статических сред. С другой стороны, на турнирных партиях каждому участнику отводится определённое время на все ходы. Когда оно заканчивается, игроку засчитывается поражение. Поэтому показатели производительности агента меняются с течением времени. Это означает, что шахматы являются **полудинамической** средой.

В шахматах фигуры могут находиться только на определённых полях доски. Поэтому агент способен однозначно различать все возможные игровые состояния. Их количество огромно, но конечно. Также различимы и конечны игровое время и ходы участников. Следовательно, шахматы — это **сложная дискретная** среда.

В шахматной партии только два участника. Победа одного из них означает поражение другого. Таким образом показатели производительности игроков обратно пропорциональны. Из этого следует, что шахматы — это **мультиагентная конкурентная** среда.

В шахматах не так много правил. Они определяют допустимые ходы фигур и условия окончания партии. Эти правила легко представить в удобной для агента форме такой как таблица или программа. Знание правил обязательно для агента. Без них он не сможет делать ходы. Поэтому шахматы — это **известная агенту** среда.

Подведём итог. С точки зрения теории интеллектуальных агентов шахматы — это проблемная среда со следующими признаками:

* Полностью наблюдаемая
* Детерминированная
* Последовательная
* Полудинамическая
* Дискретная
* Мультиагентная и конкурентная
* Известна агенту
* Сложная

### 3.3.2 Интеллектуальный агент

Мы определили тип проблемной среды, которую представляют собой шахматы. Теперь выясним, агенты каких классов способны в ней работать.

Таблица 2-6 приводит шесть классов агентов. Их можно разделить на следующие три группы:

1. Рефлексные агенты.
2. Агенты, использующие поиск.
3. Обучающиеся агенты.

Агенты одной группы используют похожий алгоритм работы. Например, все рефлексные агенты основаны на таблице правил или формальной логике. Это означает, что их архитектура и принцип работы очень близки.

Для удобства мы рассмотрим не каждый из шести классов агентов, а каждую из трёх групп.

#### 3.3.2.1 Рефлексные агенты

Может ли играть в шахматы какой-то из рефлексных агентов? Чтобы ответить на этот вопрос, начнём с **простого рефлексного агента**. Он полностью игнорирует последовательность актов восприятия. При выборе действия агент учитывает только текущее состояние среды. Он не рассматривает последствия своих действий и будущие возможные состояния среды.

Несмотря на все свои ограничения, простой рефлексный агент может работать в среде, похожей на шахматы. Например, его возможностей достаточно, чтобы реализовать оптимальную стратегию игры в крестики-нолики.

Крестики-нолики считаются [решённой игрой](https://en.wikipedia.org/wiki/Solved_game). Для каждого игрока найдена оптимальная стратегия, которая приводит к наилучшему результату. Если оба игрока придерживаются своей оптимальной стратегии, партия всегда заканчивается вничью.

Для игры в крестики-нолики простому рефлексному агенту нужна таблица правил. Она должна содержать следующую информацию:

* Все возможные состояния поля игры, допустимые правилами.

* Ход агента для каждого возможного состояния поля, который реализует оптимальную стратегию.

Используя такую таблицу, агент будет выбирать лучший ход на любое действие противника.

Выясним размер таблицы правил с реализацией оптимальной стратегии. Всего на поле игры 9 клеток. Каждая клетка имеет одно из трёх состояний: пустая, нолик, крестик. Поэтому всего есть 3^9^ = 19683 возможных состояний игрового поля. Среди них есть варианты, когда все клетки заполнены крестиками и все клетки — нолики. Такие ситуации не могут возникнуть в реальной игре. Поэтому из 19683 возможных состояний поля только 5478 состояний удовлетворяют правилам игры. Из этих расчётов мы делаем вывод, что таблица правил простого рефлексного агента должна иметь 5478 строк.

Каждая строка таблицы правил — это состояние игрового поля из 9 клеток плюс соответствующее ему действие агента. Действие агента можно закодировать в 9 битах. Каждый бит соответствует ходу в одну из девяти клеток. Состояние поля можно сохранить в `9 * 3 = 27` битах. Это три состояния для каждой из девяти клеток: пустая, крестик или нолик. Итого мы получаем размер таблицы `5478 * (27 + 9) = 197208` бит или 24651 байт.

Мы выяснили, что для игры в крестики-нолики простому рефлексному агенту нужна таблица правил размером около 24 Кбайт. Теперь оценим размер такой таблицы для шахмат. Согласно расчётам Клода Шеннона, существует около 10^120^ возможных позиций на шахматной доске. Для удобства сравнения грубо округлим число возможных состояний в крестиках-ноликах до 10^4^. Тогда получится, что в шахматах состояний игрового поля на 116 порядков больше.

Для простоты мы не будем учитывать, что запись в таблице правил для шахмат занимает больше памяти, чем для крестиков-ноликов. Причина в том, что на шахматной доске больше полей, а у игроков больше вариантов действий. Даже несмотря на это, простому рефлексному агенту нужна таблица правил размером порядка 10^120^ байт, чтобы играть в шахматы. Работа с таким объёмом информации выглядит недостижимой для современных компьютерных технологий.

У простого рефлексного агента для игры в шахматы есть ещё одна проблема. Шахматы до сих пор не решены. Это означает, что оптимальные стратегии для белых и чёрных неизвестны. Поэтому если бы даже компьютерные технологии позволили работать с таблицей правил размером 10^120^ байт, её содержимое остаётся загадкой.

Есть два более продвинутых класса рефлексных агентов:

* Рефлексный агент с внутренним состоянием.

* Рефлексный агент, основанный на модели.

Внутреннее состояние решает проблему с запоминанием прошлых ходов. Но оно никак не поможет агенту в планировании действий наперёд. Таким образом он должен опираться на таблицу правил, как и простой рефлексный агент. Как мы выяснили, на практике такая реализация невозможна.

Модель помогает рефлексному агенту прогнозировать изменения среды. Но она работает только для простых проблемных сред. Шахматы — это сложная среда. При попытке применить к ней модель возникает комбинаторный взрыв. Количество состояний среды и переходов между ними оказывается слишком велико для просчёта с помощью модели. Поэтому такой рефлексный агент также не способен играть в шахматы.

Мы пришли к выводу, что шахматы слишком сложны для алгоритмов рефлексных агентов. Теоретически создать такого агента возможно. Но на практике это потребует вычислительных ресурсов, которых просто не существует.

#### 3.3.2.2 Агенты, использующие поиск

Рассмотрим интеллектуальных агентов, которые используют алгоритмы поиска. Есть два типа таких агентов:

1. Основанные на цели.
2. Основанные на полезности.

Первый тип использует знание о цели. Цель шахматной партии заключается в том, чтобы поставить оппоненту мат. В теории агент мог бы перебрать все возможные ходы обеих сторон и найти позицию с выигрышем партии.

На практике основанный на цели агент сталкивается с рядом проблем. Первая из них — это сложность среды. Агент начинает свой поиск из корневого узла дерева. Этот узел соответствует начальному положению фигур на шахматной доске. Далее дерево игры стремительно растёт. Клод Шеннон оценил его сложность как 10^120^ узлов от начала до конца типичной партии. У современных компьютеров не достаточно памяти, чтобы сохранять посещённые узлы дерева такого размера. Кроме того нет вычислительной мощности, чтобы обойти столько узлов за адекватное время.

Вторая проблема агента в том, что он не оценивает качество проверяемых ходов. Если узел дерева соответствует очевидно плохой позиции, все его дочерние узлы можно не рассматривать. Так агент мог бы отсекать целые поддеревья без проверки их узлов. К сожалению, основанный на цели агент знает признаки конечного состояния, которое нужно найти (то есть мат оппоненту). У него нет информации для проверки промежуточных состояний.

Проблему с оценкой промежуточных состояний решает агент, основанный на полезности. Функция полезности позволяет оптимизировать поиск несколькими путями.

Во-первых, все дерево игры из 10^120^ узлов можно разбить на поддеревья. Их глубина может составлять 10-15 ходов вперёд. Функция полезности даст оценку каждому проверяемому состоянию среды. Так агент найдёт состояние с наибольшей оценкой. Последовательность ходов, которая к нему приведёт, будет лучшей в краткосрочной перспективе.

Второе преимущество функции полезности в том, что она позволяет отсекать поддеревья. Предположим, что агент проверяет очередной узел дерева. Функция полезности даёт низкую оценку соответствующей ему позиции. В этом случае агент может исключить из рассмотрения все дочерние узлы. Таким образом алгоритм поиска проверяет только перспективные ходы. Это позволяет сэкономить время и ресурсы компьютера.

Самое сложность для реализации основанного на полезности агента — выбрать надёжную функции полезности. Шахматная теория предлагает ряд универсальных правил для оценки позиций. Функция полезности должна учитывать все эти правила.

Возможны неоднозначные позиции на доске, в которых разные правила шахматной теории требуют разных действий. В этом случае агент должен решить, какое правило важнее и предпочесть его.

Качество игры основанного на полезности агента зависит от двух факторов:

* Количество ходов, которые агент просчитывает наперёд.

* Качество функции полезности для оценки позиций.

Постепенно улучшая каждый из этих двух факторов, от агента можно добиться высокого уровня игры в шахматы. Однако, с точки зрения когнитивных наук такой агент играет тем же способом, что и начинающий шахматист. Он использует стратегию сравнительной оценки, сопоставляя несколько позиций по ряду критериев. Чтобы применить эту стратегию, человеку требуется очень много времени. Компьютер решает эту проблему благодаря высокой вычислительной мощности.

#### 3.3.2.3 Обучающиеся агенты

Обучающиеся агенты успешно справляются с такими средами как шахматная игра.

Мы рассмотрели три парадигмы алгоритмов обучения:

* Обучение с учителем.
* Обучение без учителя.
* Обучение с подкреплением.

Первые две парадигмы работают только в эпизодических средах. Поэтому они не подходят для создания агента, играющего в шахматы. Только обучение с подкреплением может решить эту задачу.

Шахматы — это сложная среда со множеством состояний. Поэтому алгоритму обучения потребуется значительное время и вычислительные ресурсы, чтобы создать агента.

Разработчик должен задать правила шахмат для алгоритма обучения. Прежде всего это допустимые ходы фигур, а также условия победы и ничьей. Без этих фундаментальных знаний об игре, создать агента не получится.

Рассмотрим типичный алгоритм обучения с подкреплением в общем виде. Он будет проводить шахматные партии между разными модификациями агента. Первое поколение модификаций будет выполнять случайные ходы, допустимые правилами. Несмотря на случайные ходы сторон, партия закончится либо ничьей, либо чьей-то победой.

На следующем шаге алгоритм обучения отсеет менее успешные модификации, которые проигрывают чаще чем выигрывают. Более успешные станут прототипами для модификаций агента второго поколения. Подготовив агентов второго поколения, алгоритм обучения снова проводит партии между ними.

Продолжая совершенствовать модификации агента, алгоритм обучения достигнет требуемого процента побед на каком-то поколении. В этом случае задача алгоритма будет решенной.

С точки зрения когнитивных наук обучающийся агент играет в шахматы так же как эксперт. Алгоритм обучения путём многократных повторений выводит типичные позиции на доске и правильные реакции на них. Эти позиции могут упаковываться иерархически в блоки в терминологии Саймона. Таким образом во время партии агент располагает огромной библиотекой блоков.

Обучающийся агент не тратит время на просчёт ходов вперёд. Вместо этого он распознает типичные позиции и сразу реагирует на них.

Чем больше времени и ресурсов есть у алгоритма обучения, тем больше блоков создаваемый агент научится распознавать. Чем большим количеством шаблонов располагает агент, тем реже он будет ошибаться в своей игре. Таким образом качество агента во многом зависит от объёма ресурсов, которые были потрачены на его создание.

{pagebreak}
