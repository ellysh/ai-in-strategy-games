## 3.3 Теория ИИ и шахматы

Мы познакомились с идеями Клода Шеннона. Он применил теорию игр к шахматам. Так учёный разработал идеи и алгоритмы для реализации шахматных программ.

Рассмотрим теперь шахматы с точки зрения теории интеллектуальных агентов. Это расскажет нам о возможных архитектурах и способах реализации шахматных программ.

### 3.3.1 Проблемная среда

Игра в шахматы — это проблемная среда. Её свойства однозначно определяют классы агентов, которые способны в ней работать. Поэтому на первом шаге определим свойства проблемной среды, согласно таблице 2-4. Так мы определим её тип и узнаем классы подходящих агентов по таблице 2-6.

Агент знает расположение всех фигур на доске в любой момент времени. Кроме этого он имеет полную информацию обо всех произошедших игровых событиях. Эти знания легко доступны для каждого участника партии, потому что в шахматах нет скрытых состояний. Следовательно, шахматы — это **полностью наблюдаемая** среда.

В шахматах нет случайных событий. Каждая новая позиция фигур на доске зависит только от трёх факторов:

1. Предыдущая позиция на доске.
2. Действие агента.
3. Действие оппонента.

Это означает, что каждое следующее состояние среды определяется её текущим состоянием и действиями агентов. Такая среда является **детерминированной**.

В шахматах каждое действие агента влияет на будущие состояния среды. Он не может принимать решения, исходя только из текущей позиции. Вместо этого агент должен планировать свои действия и их результат наперёд. Поэтому шахматы являются **последовательной** средой.

В шахматах игроки делают ходы друг за другом. Позиция на доске меняется только после очередного хода. Это означает, что пока агент принимает решение, среда остаётся неизменной. Такое поведение характерно для статических сред. С другой стороны, на турнирных партиях каждому участнику отводится определённое время на все ходы. Когда оно заканчивается, игроку засчитывается поражение. Поэтому показатели производительности агента меняются с течением времени. Это означает, что шахматы являются **полудинамической** средой.

В шахматах фигуры могут находиться только на определённых полях доски. Поэтому агент способен однозначно различать все возможные игровые состояния. Их количество огромно, но конечно. Также различимы и конечны игровое время и ходы участников. Следовательно, шахматы — это **сложная дискретная** среда.

В шахматной партии только два участника. Победа одного из них означает поражение другого. Таким образом показатели производительности игроков обратно пропорциональны. Из этого следует, что шахматы — это **мультиагентная конкурентная** среда.

В шахматах не так много правил. Они определяют допустимые ходы фигур и условия окончания партии. Эти правила легко представить в удобной для агента форме такой как таблица или программа. Знание правил обязательно для агента. Без них он не сможет делать ходы. Поэтому шахматы — это **известная агенту** среда.

Подведём итог. С точки зрения теории интеллектуальных агентов шахматы — это проблемная среда со следующими признаками:

* Полностью наблюдаемая
* Детерминированная
* Последовательная
* Полудинамическая
* Дискретная
* Мультиагентная и конкурентная
* Известна агенту
* Сложная

### 3.3.2 Интеллектуальный агент

Мы определили тип проблемной среды, которую представляют собой шахматы. Теперь выясним, агенты каких классов способны в ней работать.

Таблица 2-6 приводит шесть классов агентов. Их можно разделить на следующие три группы:

1. Рефлексные агенты.
2. Агенты, использующие поиск.
3. Обучающиеся агенты.

Агенты одной группы используют похожий алгоритм работы. Например, все рефлексные агенты основаны на таблице правил или формальной логике. Это означает, что их архитектура и принцип работы очень близки.

Для удобства мы рассмотрим не каждый из шести классов агентов, а каждую из трёх групп.

#### 3.3.2.1 Рефлексные агенты

Может ли играть в шахматы какой-то из рефлексных агентов? Чтобы ответить на этот вопрос, начнём с **простого рефлексного агента**. Он полностью игнорирует последовательность актов восприятия. При выборе действия агент учитывает только текущее состояние среды. Он не рассматривает последствия своих действий и будущие возможные состояния среды.

Несмотря на все свои ограничения, простой рефлексный агент может работать в среде, похожей на шахматы. Например, его возможностей достаточно, чтобы реализовать оптимальную стратегию игры в крестики-нолики.

Крестики-нолики считаются [решённой игрой](https://en.wikipedia.org/wiki/Solved_game). Для каждого игрока найдена оптимальная стратегия, которая приводит к наилучшему результату. Если оба игрока придерживаются своей оптимальной стратегии, партия всегда заканчивается вничью.

Для игры в крестики-нолики простому рефлексному агенту нужна таблица правил. Она должна содержать следующую информацию:

* Все возможные состояния поля игры, допустимые правилами.

* Ход агента для каждого возможного состояния поля, который реализует оптимальную стратегию.

Используя такую таблицу, агент будет выбирать лучший ход на любое действие противника.

Выясним размер таблицы правил с реализацией оптимальной стратегии. Всего на поле игры 9 клеток. Каждая клетка имеет одно из трёх состояний: пустая, нолик, крестик. Поэтому всего есть 3^9^ = 19683 возможных состояний игрового поля. Среди них есть варианты, когда все клетки заполнены крестиками и все клетки — нолики. Такие ситуации не могут возникнуть в реальной игре. Поэтому из 19683 возможных состояний поля только 5478 состояний удовлетворяют правилам игры. Из этих расчётов мы делаем вывод, что таблица правил простого рефлексного агента должна иметь 5478 строк.

Каждая строка таблицы правил — это состояние игрового поля из 9 клеток плюс соответствующее ему действие агента. Действие агента можно закодировать в 9 битах. Каждый бит соответствует ходу в одну из девяти клеток. Состояние поля можно сохранить в `9 * 3 = 27` битах. Это три состояния для каждой из девяти клеток: пустая, крестик или нолик. Итого мы получаем размер таблицы `5478 * (27 + 9) = 197208` бит или 24651 байт.

Мы выяснили, что для игры в крестики-нолики простому рефлексному агенту нужна таблица правил размером около 24 Кбайт. Теперь оценим размер такой таблицы для шахмат. Согласно расчётам Клода Шеннона, существует около 10^120^ возможных позиций на шахматной доске. Для удобства сравнения грубо округлим число возможных состояний в крестиках-ноликах до 10^4^. Тогда получится, что в шахматах состояний игрового поля на 116 порядков больше.

Для простоты мы не будем учитывать, что запись в таблице правил для шахмат занимает больше памяти, чем для крестиков-ноликов. Причина в том, что на шахматной доске больше полей, а у игроков больше вариантов действий. Даже несмотря на это, простому рефлексному агенту нужна таблица правил размером порядка 10^120^ байт, чтобы играть в шахматы. Работа с таким объёмом информации выглядит недостижимой для современных компьютерных технологий.

У простого рефлексного агента для игры в шахматы есть ещё одна проблема. Шахматы до сих пор не решены. Это означает, что оптимальные стратегии для белых и чёрных неизвестны. Поэтому если бы даже компьютерные технологии позволили работать с таблицей правил размером 10^120^ байт, её содержимое остаётся загадкой.

Есть два более продвинутых класса рефлексных агентов:

* Рефлексный агент с внутренним состоянием.

* Рефлексный агент, основанный на модели.

Внутреннее состояние решает проблему с запоминанием прошлых ходов. Но оно никак не поможет агенту в планировании действий наперёд. Таким образом он должен опираться на таблицу правил, как и простой рефлексный агент. Как мы выяснили, на практике такая реализация невозможна.

Модель помогает рефлексному агенту прогнозировать изменения среды. Но она работает только для простых проблемных сред. Шахматы — это сложная среда. При попытке применить к ней модель возникает комбинаторный взрыв. Количество состояний среды и переходов между ними оказывается слишком велико для просчёта с помощью модели. Поэтому такой рефлексный агент также не способен играть в шахматы.

Мы пришли к выводу, что шахматы слишком сложны для алгоритмов рефлексных агентов. Теоретически создать такого агента возможно. Но на практике это потребует вычислительных ресурсов, которых просто не существует.

#### 3.3.2.2 Агенты, использующие поиск

Рассмотрим интеллектуальных агентов, которые строятся на алгоритмах поиска. Эти алгоритмы делятся на два класса:

* Классический поиск (неинформированный и информированный).

* Состязательный поиск.

Классический поиск хорошо работает в одноагентных средах, но не справляется с конкурентными мультиагентными. Причина в том, что алгоритм учитывает действия агента и полностью игнорирует противодействие оппонента. Шахматы как раз являются конкурентной мультиагентной средой. **Агенты, основанные на цели,** используют алгоритмы неинформированного поиска. Поэтому ни один из агентов этого класса не способен играть в шахматы.

**Агенты, основанные на полезности,** используют алгоритмы информированного и состязательного поиска. Первый тип относится к классическому поиску, поэтому основанные на нём агенты не могут играть в шахматы. Второй тип — состязательный поиск. Такие алгоритмы строятся на методах теории игр.

Здесь мы приходим к выводам Клода Шеннона. Его стратегия типа А — это применение поиска минимакс к игре в шахматы. Учёный не привёл строгих доказательств, что предложенный им алгоритм работает. Но мы можем оценить его перспективность с точки зрения теории интеллектуальных агентов.

Может ли агент, построенный на состязательном поиске, играть в шахматы? Из таблицы 2-6 мы знаем, что агент, основанный на полезности, успешно работает в средах со следующими характеристиками:

* Полностью наблюдаемая
* Детерминированная
* Последовательная
* Статическая
* Дискретная
* Мультиагентная и конкурентная
* Неизвестная
* Сложная

У этого списка есть два отличия от характеристик шахмат. Во-первых, шахматы являются средой, известной агенту. Алгоритмы состязательного поиска справляются с неизвестными средами. Это означает, что они также успешно работают и в более простых известных средах.

Во-вторых, шахматы являются полудинамической средой. Конкретнее, у агента ограниченно время на выбор своего действия. Это означает, что алгоритм состязательного поиска должен давать удовлетворительный результат даже тогда, когда он не может отработать до конца. Такое ограничение может стать критическим.

Когда мы говорим о работе состязательного поиска в полудинамической среде, надо учитывать сложность этой среды. Число Шеннона показывает сложность дерева игры в шахматы. Она настолько велика, что у алгоритма поиска почти никогда не будет возможности доработать до конца и просчитать партию до последнего хода. Клод Шеннон оценил, что алгоритм минимакс сможет просчитывать позицию на три хода вперёд. Сделаем грубую поправку на мощность современных компьютеров и умножим эту оценку на два. Получим просчёт на шесть ходов вперёд. Для качественной игры этого по-прежнему недостаточно.

Проблему сложной полудинамической среды могут решить модификации алгоритма минимакс, например альфа-бета отсечение. Эти модификации удаляют из рассмотрения целые поддеревья, опираясь на некоторые эвристики. Такое отсечение сокращает дерево игры более чем в четыре раза. Это приближает глубину просчёта ходов алгоритмом поиска к возможностям профессионального шахматиста. В этом случае уже можно рассчитывать на качественную игру со стороны агента.

Мы пришли к выводу, что теоретически агент, построенный на состязательном поиске, может играть в шахматы. Для его реализации на практике надо решить ряд технических задач:

1. Разработать достаточно точную функцию оценки позиций.

2. Разработать высокопроизводительную аппаратную платформу для генерации ходов и оценки шахматных позиций.

3. Найти компромисс для эвристик поиска: они должны отсекать значительное количество ветвей дерева игры, но при этом оставлять ветви, ведущие к лучшим ходам.

Несмотря на эти технические трудности, нет никаких принципиальных ограничений для сильной игры агента в шахматы. В это означает, что стратегия типа А Клода Шеннона должна работать на практике.

#### 3.3.2.3 Обучающиеся агенты

Обучающиеся агенты успешно справляются с такими средами как шахматная игра.

Мы рассмотрели три парадигмы алгоритмов обучения:

* Обучение с учителем.
* Обучение без учителя.
* Обучение с подкреплением.

Первые две парадигмы работают только в эпизодических средах. Поэтому они не подходят для создания агента, играющего в шахматы. Только обучение с подкреплением может решить эту задачу.

Шахматы — это сложная среда со множеством состояний. Поэтому алгоритму обучения потребуется значительное время и вычислительные ресурсы, чтобы создать агента.

Разработчик должен задать правила шахмат для алгоритма обучения. Прежде всего это допустимые ходы фигур, а также условия победы и ничьей. Без этих фундаментальных знаний об игре, создать агента не получится.

Рассмотрим типичный алгоритм обучения с подкреплением в общем виде. Он будет проводить шахматные партии между разными модификациями агента. Первое поколение модификаций будет выполнять случайные ходы, допустимые правилами. Несмотря на случайные ходы сторон, партия закончится либо ничьей, либо чьей-то победой.

На следующем шаге алгоритм обучения отсеет менее успешные модификации, которые проигрывают чаще чем выигрывают. Более успешные станут прототипами для модификаций агента второго поколения. Подготовив агентов второго поколения, алгоритм обучения снова проводит партии между ними.

Продолжая совершенствовать модификации агента, алгоритм обучения достигнет требуемого процента побед на каком-то поколении. В этом случае задача алгоритма будет решенной.

С точки зрения когнитивных наук обучающийся агент играет в шахматы так же как эксперт. Алгоритм обучения путём многократных повторений выводит типичные позиции на доске и правильные реакции на них. Эти позиции могут упаковываться иерархически в блоки в терминологии Саймона. Таким образом во время партии агент располагает огромной библиотекой блоков.

Обучающийся агент не тратит время на просчёт ходов вперёд. Вместо этого он распознает типичные позиции и сразу реагирует на них.

Чем больше времени и ресурсов есть у алгоритма обучения, тем больше блоков создаваемый агент научится распознавать. Чем большим количеством шаблонов располагает агент, тем реже он будет ошибаться в своей игре. Таким образом качество агента во многом зависит от объёма ресурсов, которые были потрачены на его создание.

{pagebreak}
