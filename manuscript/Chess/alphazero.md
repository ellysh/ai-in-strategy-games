## 3.6 AlphaZero

5 декабря 2017 года компания [DeepMind](https://en.wikipedia.org/wiki/Google_DeepMind) опубликовала статью о своей новой системе. В статье исследователи рассказали о принципах работы и результатах [AlphaZero](https://en.wikipedia.org/wiki/AlphaZero).

AlphaZero 24 часа обучалась шахматам, играя сама с собой. После этого она сыграла матч из 100 партий со Stockfish версии 8. Контроль времени был укороченный: каждая сторона имела в среднем 1 минуту на ход. Результаты матча были следующими:

* AlphaZero выиграла 28 партии.
* Ничьей закончили 72 партии.
* Stockfish 8 выиграл 0 партий.

AlphaZero работала по совершенно иному принципу, чем типичный шахматный движок. В ней не было ни минимаксного поиска, ни вручную запрограммированной функции оценки позиции. Именно новый подход исследователей DeepMind вызвал сенсацию в шахматном мире.

AlphaZero стала прототипом для нового типа современных шахматных движков. Самый известный и успешный из них — [Leela Chess Zero](https://ru.wikipedia.org/wiki/Leela_Chess_Zero) (Lc0). Он распространяется с открытым исходным кодом, так же как и Stockfish.

### 3.6.1 История AlphaZero

Перед тем как рассмотреть внутреннее устройство системы AlphaZero, познакомимся с его историей. Она поможет нам лучше понять почему он работает именно так, а не иначе.

#### 3.6.1.1 AlphaGo

Исследователи DeepMind создали свою революционную систему AlphaZero не с нуля. Её архитектура основана на идеях двух более ранних систем AlphaGo и AlphaGo Zero. Разработчики следовали путём проб и ошибок. Так они пришли к техническим решениям, которые воплотились в AlphaZero.

Команда DeepMind уже занималась какое-то время проектами для настольных игр и классических аркад для приставок [Atari](https://ru.wikipedia.org/wiki/Atari). Эти игры были достаточно простыми. На них исследователи изучали возможности обучения с подкреплением.

Проект [AlphaGo](https://en.wikipedia.org/wiki/AlphaGo) начался в 2014 году. Он преследовал несколько целей. Во-первых, AlphaGo была не просто системой для решения конкретной практической задачи. Она должна была стать платформой для тестирования передовых методов ИИ. В ней объединялись несколько сетей deep learning с различными архитектурами и алгоритм поиска по дереву методом Монте-Карло.

Во-вторых, исследователи хотели проверить свои наработки в сложной задаче. Обучение с подкреплением хорошо показало себя в простых аркадных играх. Но оставался открытым вопрос: способна ли модель справиться с очень сложной игрой? Именно поэтому команда выбрала [го](https://ru.wikipedia.org/wiki/Го). Считается, что это самая сложная настольная игра как для человека, так и для системы ИИ.

К началу 2010-х годов программы для го сильно уступали профессиональным игрокам. Они играли на среднем любительском уровне. Лучшие результаты показали две программы. В 2012 году программа [Zen](https://en.wikipedia.org/wiki/Zen_(software)) дважды выиграла у профессионального игрока Масаки Такемия (9 дан). Сначала это была победа с [форой](https://ru.wikipedia.org/wiki/Го#Фора) в пять камней, а затем — с форой в четыре камня. В 2013 году программа [Crazy Stone](https://en.wikipedia.org/wiki/Crazy_Stone_(software)) выиграла у Ёсио Исида (9 дан) с форой в четыре камня. Несмотря на эти победы, силы сторон в каждой партии сильно различались. Фора на высоком уровне игры даёт огромное преимущество.

Для решения задачи игры в го исследователи DeepMind экспериментировали с разными идеями. Они спроектировали пять нейронных сетей. У каждой из них была своя архитектура. Некоторые модели настраивал алгоритм обучения с учителем. В этом случае обучающим набором данных были позиции из турнирных партий и их оценка. Другие модели обучались с подкреплением. Каждая из них играла сама с собой и запоминала удачные ходы.

Разработчики объединили все пять нейронных сетей в единую систему AlphaGo. Она работала как ансамбль моделей. При этом поиск по дереву игры выполнялся методом Монте-Карло.

I> [**Ансамбль моделей**](https://wiki.loginom.ru/articles/ensemble-models.html) (ensemble of models) — это объединение нескольких отдельных моделей для повышения общей производительности и точности прогнозов. Ансамбль может исправлять ошибки отдельных моделей и компенсировать проблему переобучения (overfitting).

AlphaGo превзошла все разработанные ранее программы для игры в го. Лучшая из них CrazyStone имела уровень Эло порядка 2000. Когда AlphaGo начала участвовать в официальных матчах, оказалось что она играет на уровне 3000 Эло. Разница в 1000 Эло — это совершенно другой уровень. Для примера, 1000 Эло разделяют любителя и гроссмейстера в шахматах.

Вот список громких побед AlphaGo, которые вызвали сенсацию и ажиотаж вокруг неё:

* В октябре 2015 года AlphaGo обыграла чемпиона Европы [Фань Хуэя](https://ru.wikipedia.org/wiki/Фань_Хуэй) со счётом 5-0.

* В марте 2016 года AlphaGo победила одного из лучших игроков в мире [Ли Седоля](https://ru.wikipedia.org/wiki/Ли_Седоль) из Южной Кореи со счётом 4-1.

* В декабре 2016 года AlphaGo одержала 60 побед в онлайн играх с профессионалами и лучшими игроками го.

* В мае 2017 AlphaGo обыграла лучшего игрока в го [Кэ Цзе](https://ru.wikipedia.org/wiki/Кэ_Цзе) из Китая со счётом 3-0.

После победы над Кэ Цзе [Китайская Ассоциация Вэйци](https://ru.wikipedia.org/wiki/Китайская_ассоциация_вэйци) официально присвоила AlphaGo самый высокий профессиональный дан — 9.

#### 3.6.1.2 AlphaGo Zero

>>> REVIEW

Проект AlphaGo продемонстрировал, что комбинация deep learning моделей и поиска по дереву методом Монте-Карло может решать сложные зачади. Этот подход переняли многие исследовательские и коммерческие проекты.

Несмотря на выдающиеся результаты, у системы AlphaGo были серьёзные недостатки:

1. Чрезмерно сложная архитектура. Все нейронные сети AlphaGo обучались отдельно. Для каждой применялся свой алгоритм обучения. Некоторые из алгоритмов требовали на вход большие обучающие наборы данных. Из-за этого изменять и улучшать систему было слишком трудоёмко.

2. Некоторые алгоритмы обучения требовали ручной [**настройки гиперпараметров**](https://ru.wikipedia.org/wiki/Оптимизация_гиперпараметров) (hyperparameter optimization). **Гиперпараметр** — некоторое значение, которое управляет процессом обучения. Настройка была трудоёмкой и усложняла изменение системы.

3. Система требовала огромных вычислительных ресурсов для обучения и работы. Это было следствием её высокой сложности.

Команда DeepMind прекратила развивать систему AlphaGo. Работая над ней, исследователи получили хороший опыт. Они поняли, какие технические решения работают, а с какими возникают проблемы.

Разработчики DeepMind учли свои ошибки и начали новый проект. Он получил название [AlphaGo Zero](https://en.wikipedia.org/wiki/AlphaGo_Zero). Компания DeepMind впервые объявила об этой системе 19 октября 2017 года в статье журнала Nature. Цели проекта были следующими:

1. Создать полностью самообучающуюся систему. Алгоритм обучения должен был работать без каких-либо данных по партиям в го между людьми.

2. Упростить процесс обучения и изменения системы. Для этого исследователи заменили пять нейронных сетей AlphaGo на одну единственную.

3. Повысить эффективность использования вычислительных ресурсов.

В отличие от предшественника, система AlphaGo Zero училась играть в го исключительно на своих собственных играх. Слово "Zero" (ноль) в её названии означает, что при разработке не использовались знания экспертов в предметной области. То есть система не изучала позиции камней по партиям профессиональных игроков.

Через три дня самостоятельного обучения AlphaGo Zero достигла уровня игры одной из первых версий AlphaGo. Через 21 день — AlphaGo Zero поднялась до уровня версии AlphaGo, которая обыграла 60 профессионалов онлайн. Через 40 дней — AlphaGo Zero превзошла все предыдущие версии AlphaGo.

Успех AlphaGo Zero в обучении игры оставил открытым один вопрос. Какой алгоритм обучения даёт лучший результат: с учителем или без? Силу игры AlphaGo Zero можно было объяснить более простой архитектурой и эффективностью использования вычислительных ресурсов.

Чтобы найти ответ, исследователи DeepMind разработали тестовую модель. Её архитектура была очень похожа на AlphaGo Zero. Главное отличие заключалось в том, что тестовая модель обучалась по алгоритму с учителем на примерах позиций из профессиональных игр. Для чистоты эксперимента процесс обучения запустили с нуля для обоих моделей. Результаты сравнения двух моделей оказались неожиданными.

Во-первых, в самом начале обучения AlphaGo Zero стабильно проигрывала тестовой модели. Но после 20 часов обучения игра AlphaGo Zero стала сильнее. Через 40 часов уровень игры тестовой модели достиг примерно 3500 пунктов Эло и больше не рос. В отличие от неё, AlphaGo Zero продолжила совершенствовать свою игру и превысила уровень 4000 пунктов Эло.

Во-вторых, обе модели оценивали текущую позицию на доске и по ней предсказывали исход партии. Для этого эксперимента исследователи брали позиции из базы данных профессиональных игр. Сначала тестовая модель давала более точные предсказания. Но через 15 часов обучения прогнозы AlphaGo Zero стали лучшие.

В-третьих, обе модели полностью закончили своё обучение. Их сравнили в точности предсказания хода, который в заданной позиции сделает профессиональный игрок. Снова использовалась база данных профессиональных партий. Тестовая модель давала более точные предсказания, чем AlphaGo Zero.

Из последнего эксперимента исследователи DeepMind сделали вывод, что сила игры AlphaGo Zero превзошла уровень профессионалов. Во время своего обучения модель обнаружила закономерности в игре го, которые люди ещё не понимают. Когда системе показывали позиции из профессиональных партий, она выбирала самый сильный ход с точки зрения своего сверхчеловеческого уровня игры. Реальные ходы профессионалов в этой позиции оказывались слабее. Поэтому система их не предлагала.

Проект AlphaGo Zero показал, что машинное обучение может решать задачи, для которых у людей ещё нет экспертных знаний. Подобная система способна обучаться с нуля самостоятельно и находить сложные зависимости в проблемной среде. После обучения её навыки могут значительно превосходить способности специалистов в этой области.

#### 3.6.1.3 Особенности и архитектура AlphaZero

Разработчики AlphaGo Zero решили проверить, насколько универсален разработанный ими подход. Компания DeepMind запустила несколько проектов в [области биохимии](https://ru.wikipedia.org/wiki/AlphaFold), чтобы оценить эффективность аналогичных систем для научных задач. Кроме этого исследователи решили применить систему, аналогичную AlphaGo Zero, для других настольных игр. Для этой цели они начали новый проект под названием AlphaZero.

Команда DeepMind подготовила три экземпляра системы AlphaZero. Это были три модели с одинаковыми параметрами: архитектура нейронной сети, алгоритм обучения и гиперпараметры. Каждый экземпляр системы учился только одной из трёх игр: го, шахматы и сёги.

7 декабря 2017 года команда DeepMind опубликовала [статью](https://arxiv.org/abs/1712.01815) в журнале Science, посвящённую новой модели и её результатам. После 24 часов обучения с подкреплением каждый экземпляр AlphaZero превзошёл уровень профессионалов в той игре, которой он обучался. Соответствующие модели смогли обыграть сильнейшие на 2017 год программы: Stockfish в шахматах, [Elmo](https://en.wikipedia.org/wiki/Elmo_(shogi_engine)) в сёги и AlphaGo Zero в го.

Алгоритмы обучения AlphaZero и AlphaGo Zero отличаются следующим:

1. AlphaGo Zero оценивает вероятность результата партии, предполагая только два возможных исхода в го: победа или поражение. AlphaZero оценивает вероятность результата, учитывая ещё и ничьи в шахматах и сёги.

2. Алгоритм обучения AlphaGo Zero учитывал, что у каждой позиции в го есть 8 симметричных позиций. Они создаются поворотами доски вправо, влево и вокруг плоскостей симметрии. Поэтому в процессе обучения с подкреплением алгоритм случайным образом заменял реальную позицию на симметричную. Это повышало качество обучения. В шахматах и сёги симметрии позиций нет. Поэтому механизм замены не использовался для AlphaZero.

3. При обучении AlphaGo Zero применялись разные экземпляры модели. Самый сильный из них играл сам с собой. В результате обучения с подкреплением он настраивал свои параметры. Получившуюся новую модель сравнивали с прошлым самым сильным экземпляром. Если новая модель одерживала в среднем 55% побед, она становилась самым сильным экземпляром и играла сама с собой на следующей итерации. При обучении AlphaZero всегда применялся только один экземпляр модели. В результате обучения с подкреплением он последовательно настраивал свои параметры.

4. В процессе обучении AlphaGo Zero оптимизировались гиперпараметры обучения. В случае AlphaZero гиперпараметры не менялись на протяжении всего процесса обучения.

В качестве механизма поиска AlphaZero использует [**поиск по дереву методом Монте-Карло**](https://habr.com/ru/articles/282522/) (Monte Carlo tree search или MCTS). Статическую оценку позиции выполняет одна deep learning нейронная сеть.

### 3.6.2 Поиск по дереву методом Монте-Карло

[**Методом Монте-Карло**](https://en.wikipedia.org/wiki/Monte_Carlo_method) называется широкий класс вычислительных алгоритмов, которые полагаются на повторную случайную выборку и статистический анализ для получения численных результатов.

В 1930-х годах физики начали экспериментировать с ранними вариантами метода Монте-Карло. Он давал хорошие результаты в физике элементарных частиц. Современную версию метода Монте-Карло разработал польский математик [Станислав Улам](https://ru.wikipedia.org/wiki/Улам,_Станислав) в конце 1940-х годов. Он учавствовал в  Манхэттенском проекте по разработке ядерного оружия. Новый метод позволил создать более точную модель [**диффузии нейтронов**](https://ru.wikipedia.org/wiki/Диффузия_нейтронов), чем применявшиеся ранее комбинаторные методы.

Хорошее интуитивное представление о методе Монте-Карло даёт рассказа Станислава Улама о том, как он пришёл к этой идее. В 1946 году учёный выздоравливал после болезни и раскладывал пасьянс, чтобы скоротать время. Он задался вопросом: какова вероятность того, что пасьянс из 52 карт сложится успешно? Для решения этой задачи Станислав попытался применить комбинаторные методы, но это оказалось слишком трудоёмко. Тогда учёный решил, что практичнее будет разложить пасьянс, например, сто раз и подсчитать долю успешных игр. Именно в этом сусть метода Монте-Карло: многократно повторять эксперимент и оценивать статистику полученных результатов.

Метод Монте-Карло нашел применение в поиске по дереву. В 1987 году Брюс Абрамсон из Колумбийского университета Нью-Йорка защищал докторскую диссертацию. В ней он предложил заменить статическую функцию оценки в минимаксном поиске на специальную модель. Она давала ожидаемый результат игры для заданного состояния игрового поля. Чтобы получить результат, модель начинала с некоторого хода, допустимого в заданной позиции, и проигрывала партию до конца. Во время такой игры наперёд каждая сторона делала случайные ходы, разрешённые правилами. Брюс применил свой подход для шахмат и крестики-нолики. Он утверждал, что его **модель ожидаемого результата** (expected-outcome model) даёт точные и легко вычислимые результаты, которые не зависят от предметной области. Предложенный Брюс алгоритм стал прототипом поиска по дереву методом Монте-Карло.

Скорее всего, вы зададитесь вопросом: как статистический поиск по дереву может в принципе превзойти строго детерминированный минимаксный поиск с альфа-бета отсечением? Вы просто недооцениваете статистические данные.

Когда профессиональные шахматисты готовят свои дебюты, они в равной степени полагаются на собственный анализ и статистику. В интернете доступны сайты с перечнем часто отыгрываемых дебютов на турнирах. Например, сайт [365chess](https://www.365chess.com/opening.php) приводит следующую информацию:

1. Первый ход белых e4 был сыгран в официальных партиях 1833019 раз. Он привел к следующим результатам:

* 38% партий закончились победой белых
* 30,3% партий — ничья
* 31,6% партий — победа черных.

2. Первый ход белых f3 был сыгран в официальных партиях 90 раз. Он привел к следующим результатам:

* 31,1% партий — победа белых
* 25,6% партий — ничья
* 43,3% партий — победа черных.

Теперь представьте, что вы играете турнирную партию и должны сделать первый ход за белых. Какой из ходов e4 или f3 вы предпочтёте? Профессиональные игроки предпочитают вариант e4. Кроме того, он даёт белым больше шансов на победу: 38% против 31,1%. Логичнее из двух предложенных ходов выбрать именно его.

Вернёмся у поиску методом Монте-Карло. Он перебирает возможные ходы в текущей позиции на доске. Проблема в том, что не существует готовой базы данных с результатами партии для каждого рассматриваемого хода. Однако, вычислительная мощность современных компьютеров позволяет быстро составить нечто похожее на такую базу данных.

Представьте себе ситуацию. Вы играете в шахматы, и ваш оппонент допускает ошибку. На своём текущем ходу вы можете взять ладью. Упростим пример и допустим, что вы выбираете между двумя возможными вариантами: брать или не брать ладью. Чтобы сделать выбор, вам нужна статистика. Тогда вы берёте другую доску и расставляете на ней позицию после вашего взятия ладьи. Далее из этой позиции вы играете партию до конца 1000 раз. Причём в каждой из этих партий вы просто делаете случайные ходы, допустимые шахматными правилами. В результате у вас есть процент побед, ничей и поражений для варианта взять ладью. Точно так же вы собираете статистику для варианта без взятия ладьи. Теперь достаточно сравнить, какой из двух ходов приведёт к потенциально большему числу побед. Скорее всего, ход со взятием ладьи даст больший процент побед и окажется лучшим. Вы возвращаетесь к доске, за которой вас ждёт оппонент, и играете лучший из двух ходов. Именно в этом заключается идея поиска по дереву методом Монте-Карло.

#### 3.6.2.1 Алгоритм MCTS

Рассмотрим подробнее алгоритм поиска методом Монте-Карло. Он заключается в циклическом повторении следующих четырёх шагов:

1. **Selection** (выбор)

2. **Expansion** (расширение)

3. **Simulation** (симуляция)

4. **Backpropagation** (обратное распространение)

Рассмотрим иллюстрацию 3-35. Она демонстрирует первый шаг selection (выбор) алгоритма поиска. Перед вами дерево игры в шахматы, который построил алгоритм поиска после нескольких итераций. Зеленые узлы соответствуют возможным ходам программы, которая применяет поиск методом Монте-Карло. Красные узлы — это ходы её оппонента.

Только что оппонент сделал ход A, который привёл к текущей позиции на доске. Теперь программа рассматривает свои возможные ответные ходы: B, C и D.

На первом шаге алгоритм выбирает, какой из узлов дерева ему исследовать. У этого узла должен быть минимум один потенциальный дочерний узел, для которого ещё не выполнялся шаг simulation (симуляция).

Для выбора узла нужна стратегия, которая определяет наиболее перспективные ходы. Допустим, что у программы есть такая стратегия и она выбрала узел B. На иллюстрации 3-35 этот выбор отмечен стрелкой.

{caption: "Иллюстрация 3-35. Первый шаг selection в поиске по дереву методом Монте-Карло", height: "30%"}
![Шаг selection в MCTS](images/Chess/monte-carlo-search-selection.png)

Второй шаг алгоритма expansion (расширение) демонстрирует иллюстрация 3-36. На этом шаге для выбранного узла B создаётся дочерний узел F. Он отмечен стрелкой. F соответствует потенциальному ходу оппонента, который разрешён правилами игры. Алгоритм может выбрать этот ход случайным образом или с помощью какой-то стратегии.

{caption: "Иллюстрация 3-36. Второй шаг expansion в поиске по дереву методом Монте-Карло", height: "30%"}
![Шаг expansion в MCTS](images/Chess/monte-carlo-search-expansion.png)

Третий шаг алгоритма simulation (симуляция) демонстрирует иллюстрация 3-37. На этом шаге программа запускает вычисления, чтобы получить статистическую информацию о новом узле. Самый простой способ — сыграть определённое количество партий, начиная с позиции в этом узле. Ходы в них выбираются случайным образом, пока партия не завершится победой, ничьей или поражением. Одна такая сыгранная партия называется **playout** (воспроизведение).

На иллюстрации 3-37 мы видим, что программа выполнила один playout. Он обозначен пунктирной стрелкой. В результате симуляции программа выиграла. 

{caption: "Иллюстрация 3-37. Третий шаг simulation в поиске по дереву методом Монте-Карло", height: "50%"}
![Шаг simulation в MCTS](images/Chess/monte-carlo-search-simulation.png)

Четвёртый шаг алгоритма backpropagation (обратное распространение) демонстрирует иллюстрация 3-38. На этом шаге программа передаёт статистическую информацию, полученную в результате simulation, для узла F в корень дерева игры A. Направление передачи информации обозначено стрелками.

{caption: "Иллюстрация 3-38. Четвёртый шаг backpropagation в поиске по дереву методом Монте-Карло", height: "30%"}
![Шаг backpropagation в MCTS](images/Chess/monte-carlo-search-backpropagation.png)

Каждый узел дерева игры хранит следующую информацию:

1. Число побед игрока, делающего ход в этом узле. Это сумма его побед во всех playout, выполненных для всех дочерних узлов.

2. Число выполненных playout для всех дочерних узлов.

В результате шага expansion алгоритм добавил узел F. В нём ход делает оппонент. Программа выполнила один playout для этого узла и он привёл к поражению оппонента.

Вернёмся к иллюстрации 3-37. Она показывает состояние узла B до симуляции партии в узле F. В узле B ход делает программа. Его состояние 1/3 означает, что для его дочерних узлов было выполнено три playout и в них один раз выиграла программа. На самом деле все три playout выполнялись для узла E с состоянием 2/3. Обратите внимание, что две победы оппонента в трёх playout в узле E, означают одну победу программы в трёх playout в узле B.

Иллюстрация 3-38 показывает обновлённое состояние 0/1 узла F. Учитывая это, программа обновляет состояние узла B до 2/4. Это означает, что в двух дочерних узлах E и F в сумме было выполнено четыре playout. В двух из них победил оппонент. Следовательно, два других playout привели к победе программы.

После обновления состояния узла B, программа переходит к узлу A. Это текущая позиция после хода оппонента. На иллюстрации 3-37 состояние A равно 9/12. Программа обновляет это значение до 9/13. Потому что для одного из дочерних узлов A (узла F) выполнился один playout, который привел к поражению оппонента.

Предположим, что у программы закончилось время на поиск по дереву. Теперь она должна выбрать свой ход. Из трёх вариантов B, C и D наилучшим будет ход B. Согласно статистике, которую программа успела собрать, этот ход в 50% случаев приводит к победе и в 50% к поражению. При этом ход C приводит к 100% поражению, а ход D — к поражению с вероятностью 60%.

Для простоты нашего примера мы проигнорировали случаи, когда шаг simulation приводит к ничьей. Если это учесть, то возможные результаты каждого playout будут такими:

* 1 — победа
* 0.5 — ничья
* 0 — поражение.

В этом случае состояния узлов на шаге backpropogation рассчитываются точно так же как для целочисленных результатов.

Иллюстрация 3-39 демонстрирует шаг backpropogation, когда playout в узле F закончился ничьей (результат 0.5/1).

{caption: "Иллюстрация 3-39. Результат backpropagation в для дробного результата simulation", height: "30%"}
![Backpropagation для дробного результата simulation](images/Chess/monte-carlo-search-backpropagation-fraction.png)

Рассмотрим порядок backpropogation на иллюстрации 3-39 по шагам:

1. После симуляции одного playout для узла F, необходимо обновить состояние его родительского узла B.

2. Для обновления узла B сначала надо посчитать сумму playout у всех его дочерних узлов (E и F). Она равна: `3 + 1 = 4`. Запишем 4, как число playout, выполненных для узла B.

3. Затем посчитаем сколько playout закончились победами игрока, делающего ход в узлах E и F. Сумма побед равна: `2 + 0.5 = 2.5`.

4. Посчитаем сколько побед у игрока, делающего ход в узле B. Для этого вычтем из общего числа playout в узле B число побед игрока, делающего ход в дочерних узлах E и F. Получим `4 - 2.5 = 1.5`. Запишем это число в состояние узла B.

Следуя этому же алгоритму, мы обновляем состояние корневого узла A. Сначала находим сумму playout в его дочерних узлах B, C и D: `4 + 3 + 6 = 13`. Затем число побед игрока, делающего ход в этих узлах: `1.5 + 0 + 2 = 3.5`. Наконец, вычитаем из суммы playout число побед в узлах B, C и D: `13 - 3.5 = 9.5`. Мы получили новое состояние узла A 9.5/13.

#### 3.6.2.2 Формула UCT

Когда мы знакомились с алгоритмом поиска методом Монте-Карло и его первым шагом selection, мы говорили про стратегию выбора узла. Она должна отличать перспективные ходы для их дальнейшей проверки. Давайте рассмотрим, как именно это работает.

Существует две стратегии для выбора узла в дереве игры:

1. **Exploration** (исследование) — посещать узлы с наименьшим числом симуляций для дочерних узлов.

2. **Exploitation** (эксплуатирование) — глубже проверять узел с максимальным числом побед.

Очевидно, что следование только одной из этих стратегий неоптимально. Только exploration не даст возможности глубоко просчитать перспективные ходы. Таким образом у лучшего найденного хода может оказаться **опровержение**, которое алгоритм поиска не увидел.

I> Опровержение в шахматах — это ход или серия ходов, доказывающих несостоятельность действий противника.

Если следовать только стратегии exploitation, то лучшие ходы остануться вне поля зрения алгоримта поиска. Он будет глубоко просчитывать несколько первых попавшихся ходов и выберет наилучший из них.

Мы уже обсуждали эти стратегии в разделе "2.6.2.3 Обучение с подкреплением". Там речь шла о проблеме многорукого бандита. В случае поиска по дереву методом Монте-Карло задача та же самая — найти оптимальный компромисс между exploration и explotation.

В 2006 году венгерские учёные в области информатики [Левенте Кочиш](https://www.chessprogramming.org/Levente_Kocsis) и [Чаба Сепешвари](https://www.chessprogramming.org/Csaba_Szepesvári) предложили формулу под названием [**Upper Confidence bounds applied to Trees**](https://www.chessprogramming.org/UCT) или UCT (верхние границы уверенности, применяемые к деревьям).

Предположим, что алгоритм поиска находится в некотором узле дерева i. Формула UCT вычисляет оценку для его каждого дочернего узла j:
{width: "50%"}
![](images/Chess/uct-formula.png)

В ней используются следующие обозначения:

* X~j~ — коэффициент выигрыша для дочернего узла j. Рассчитывается как w~j~/n~j~. То есть отношение числа побед w~j~ к количеству посещений узла n~j~.

* n~i~ — количество посещений родительского узла i.

* n~j~ — количество посещений дочернего узла j.

* C — константа, задающее соотношение между шириной и глубиной поиска. Чем она больше, тем глуюже будет поиск.

Первая часть формулы (X~j~) соответствует стратегии exploitation. Это значение больше для дочерних узлов с высоким коэффициентом выигрыша. Вторая часть формулы соответствует стратегии exploration. Это значение выше для дочерних узлов с маленьким числом посещений и соответственно выполненных playout.

На шаге selection алгоритм поиска методом Монте-Карло должен выбирать дочерний узел j с максимальной оценкой UCT~j~.

Формула UCT предлагает альтернативу для стратегии финального выбора хода. Ранее мы рассматривали шаги поиска методом Монте-Карло. После выполнения backpropogation мы получили состояние дерева как на иллюстрации 3-38. Из этого состояния мы сделали вывод, что наилучшим ходом будет B. В этом узле у программы самый высокий коэфициент побед.

С другой стороны, если бы мы применяли формулу UCT для всех шагов selection, то лучшим ходом оказался бы D. Почему? Поиск методом Монте-Карло с формулой UCT чаще посещает самые перспективные дочерние узлы. Это означает, что узел с наибольшим числом посещений и есть наилучший.

Для выбора дочернего узла на шаге selection у формулы UCT есть несколько альтернатив. Одна из них — нейронная сеть. Она должна отличать более перспективные ходы в текущей позиции. В этом случае нейронная сеть сможет направлять шаги selection и expansion. Таким образом алгоритм поиска потратит меньше времени на анализ бесперспективных ходов.

#### 3.6.2.3 Преимущества и недостатки MCTS

Поиск по дереву методом Монте-Карло разрабатывался как универсальный алгоритм. Брюс Абрамсон особенно подчёркивал его независимость от предметной области. Учёный применял его в различных играх с двумя участниками. А в 1989 году австрийские учёные Вольфганг Эртель, Иоганн Шуман и Кристиан Саттнер применили MCTS в своей системе для автоматического доказательства теорем.

В 1993 году немецкий учёный Бернд Брюгманн из института физики общества Макса Планка впервые применил поиск методом Монте-Карло в программе для игры в го. В своей [статье](http://www.ideanest.com/vegos/MonteCarloGo.pdf) учёный утверждает, что программа играла на начальном любительском уровне 25 кю на уменьшенной доске 9x9 линий. При этом в неё не закладывалось никаких знаний об игре го кроме правил.

Программы для игры в го продолжали развиваться на протяжении 1990-х и 2000-х годов. Самые сильные из них применяли тот или иной вариант поиска методом Монте-Карло. В 2012 году программа Zen впервые выиграла матч у любителя 2-ого дана на стандартной доске 19x19 линий. В этой программе не использовались модели машинного обучения, а только MCTS с некоторыми эвристиками.

Поиск методом Монте-Карло оказался особенно успешным в играх и задачах с очень высоким **фактором ветвления**. Фактор ветвления — это среднее количество вариантов действий у игроков на каждом ходу. В шахматах он равен 35, а в го — 250. Именно из-за высокого фактора ветвления минимаксный поиск с альфа-бета отсечением плохо справляется с деревом игры го.

Ещё одно преимущество поиска методом Монте-Карло — это слабая зависимость от функции оценки. Эта функция играет ключевую роль в поиске альфа-бета. Если оценка низкого качества, то поиск вместо лучших ходов выбирает посредственные. В MCTS важнее количество симуляций и статистика их результатов. С другой стороны для игр с низким фактором ветвления альфа-бета поиск работает быстрее и даёт более точный результат чем MCTS.

Поиск методом Монте-Карло хорошо работает вообще без функции оценки. Для него достаточно подготовить модель игровой механики: допустимые ходы в каждой позиции и условия окончания партии. Таким образом MCTS применим в играх и задачах, для которых ещё не разработана подходящая теория.

У поиска методом Монте-Карло есть несколько недостатков. Один из них — алгоритм всегда предполагает, что работает в стохастической среде. В конкурентной среде, когда оппонент активно противодействует, алгоритму MCTS сложнее находить оптимальные действия.

Ещё один недостаток MCTS — это ошибка при оценки ходов у которых есть опровержение, состоящее из последовательности лучших ходов. MCTS может упускать из виду такие последовательности из-за своей стратегии выбора узлов на шагах selection и expansion.

Поиск методом Монте-Карло чувствителен к следующим параметрам:

1. Константа C в формуле UCT, которая определяет соотношение стратегий exploration и exploitation.

2. Число симуляций для проверяемых дочерних узлов.

Подобрать оптимальные значения этих параметров для конкретной задачи бывает сложно и трудоёмко.

Поиск методом Монте-Карло требует намного больше ресурсов чем альфа-бета поиск. Ему нужны большие вычислительные мощности для выполнения многократных симуляций и больше памяти для хранения их результатов. Только при условии неограниченного времени работы результат поиска методом Монте-Карло с формулой UCT сходится к результату минимаксного поиска.

### 3.6.3 Свёрточная нейронная сеть

В нейронной сети AlphaZero применяются **свёрточные слои** (convolutional layer). Поэтому перед тем как изучить сеть AlphaZero, познакомимся со свёрточными нейронными сетями (CNN).

До начала 1980-х годов задача распознавания изображений решалась моделями с архитектурой [многослойного перцептрона](https://en.wikipedia.org/wiki/Multilayer_perceptron). Это универсальная архитектура, которая также может решать задачи классификации, регрессии и апроксимации функций.

Если применять многослойный перцептрон для распознавания изображений, возникает несколько проблем. Главная из них связана с большим количество связей между нейронами. В сети перцептрона все слои линейные и, следовательно, полносвязные. Каждый нейрон следующего слоя связан с каждым нейроном предыдущего. Это приводит к проблеме известной как [**проклятие размерности**](https://en.wikipedia.org/wiki/Curse_of_dimensionality#Machine_learning).

Суть проклятия размерности в том, что небольшое увеличение разрешения входного изображения приводит к взрывному росту связей между нейронами в сети. Рассмотрим пример. Предположим, что модель многослойного перцептрона обрабатывает [RGB изображение](https://ru.wikipedia.org/wiki/RGB) с тремя цветовыми каналами (красный, зелёный, синий). Его размер 32x32 пикселя. Тогда во входном слое модели должно быть по три нейрона на каждый пиксель. То есть список входов выглядит примерно так:

* пиксель с координатой (1, 1) канал красного цвета
* пиксель с координатой (1, 1) канал зелёного цвета
* пиксель с координатой (1, 1) канал синего цвета
* пиксель с координатой (2, 1) канал красного цвета
* пиксель с координатой (2, 1) канал зелёного цвета
* пиксель с координатой (2, 1) канал синего цвета
* пиксель с координатой (3, 1) канал красного цвета
* пиксель с координатой (3, 1) канал зелёного цвета
* пиксель с координатой (3, 1) канал синего цвета
и т.д.

В случае 8-битной кодировки цветов каждый из этих нейронов принимает на вход число от 0 до 255. 0 соответствует минимальной интенсивности цвета у данного пикселя, а 255 — максимальной. Всего входных нейронов у модели будет `32 * 32 * 3 = 3072`.

Перейдём к первому скрытому слою. Каждый нейрон этого слоя соединяется с каждым нейроном входного слоя. Таких соединений будет 3072. Допустим для примера, что в первом скрытом слое будет 512 нейронов. Тогда мы получим `3072 * 512 = 1572864` соединений между слоями. У каждого соединения есть вес, который надо корректировать в процессе обучения.

Теперь представим, что многослойный перцептрон обрабатывает RGB изображение размером 200x200 пикселей. В этом случае входной слой вырастет до `200 * 200 * 3 = 120000` нейронов. Число соединений между входным и первым скрытым слоем вырастет до `120000 * 512 = 61440000`. Именно столько весов надо подобрать при обучении модели.

Мы увеличили размер изображения в 6,25 раза, в результате число соединений между слоями выросло примерно в 39 раз. Из-за такого взрывного роста соединений многослойный перцептрон не может обрабатывать изображения с высоким разрешением. Это вычислительно неразрешимая задача.

У многослойного перцептрона есть и другие проблемы. Полносвязные слои не учитывают пространственную структуру данных. Что это значит? Некоторые пиксели изображения расположены рядом друг с другом. Другие находятся на разных концах картинки. Перцептрон обрабатывает их одинаково. Такой подход малоэффективен. Причина в том, что изображения состоят из наборов близко расположенных элементов (например, линий), которые складываются в определённые шаблоны (например, контуры деревьев). Эти отношения между соседними элементами очень важны, но перцептрон не способен их учесть.

Свёрточная нейронная сеть (CNN) — это специальная архитектура для распознавания изображений. Она решает проблемы многослойного перцептрона.

В основе архитектуры CNN лежит идея применения фильтров (filter) для извлечения признаков (features) изображённого объекта. Это очень похоже на то, как мозг человека распознаёт объекты. Например, чтобы различать лица, мы ориентируемся на следующие признаки: контур всего лица, линии носа, ушей и глаз, оттенок кожи, цвет волос и глаз. Фильтры выделяют из исходного изображения ключевые признаки и отбрасывают несущественные детали. Таким образом уменьшается объём информации для обработки на следующем этапе.

Рассмотрим, как именно работают фильтры на примере. В качестве исходного изображения возьмём известный фрагмент [фотографии Лены Сёдерберг](https://ru.wikipedia.org/wiki/Лена_(изображение)) (см. иллюстрацию 3-40).

{caption: "Иллюстрация 3-40. Фрагмент фотографии Лены Сёдерберг", height: "30%"}
![Фрагмент фотографии Лены Сёдерберг](images/Chess/lena.png)

Применим к этой фотографии фильтр, который [выделяет границы объектов](https://ru.wikipedia.org/wiki/Выделение_границ). После его применения мы ожидаем, что линии лица и шляпы станут хорошо различимы. Сам фильтр представляет собой следующую матрицу размера 3x3:
{width: "25%"}
![](images/Chess/sobel-vertical-kernel.png)

Это вертикальный фильтр [**оператора Собеля**](https://ru.wikipedia.org/wiki/Оператор_Собеля). Сам оператор заключается в применении двух фильтров (горизонтального и вертикального) размера 3x3 к исходному изображению. Их результат комбинируется в выходное изображение, на котором чётко видны границы объектов. Такое изображение, полученное в результате наложения фильтров, называется **картой признаков** (feature map).

Иллюстрация 3-41 демонстрирует карту признаков после применения вертикального фильтра оператора Собеля.

{caption: "Иллюстрация 3-41. Карта признаков после применения фильтра", height: "30%"}
![Карта признаков после применения фильтра](images/Chess/lena-edge-detection-filter.png)

Наложение фильтра сделало линии границ объёктов более отчётливыми. Они хорошо выделяются на чёрном фоне. Это упрощает следующие шаги по распознаванию лица.

Python скрипт для применения фильтра описан в приложении 5.2.1 и доступен на [Github](https://github.com/ellysh/ai-in-strategy-games/blob/master/manuscript/resources/code/Chess/sobel-vertical-filter.py).

Хороший пример использования оператора Собеля описан в следующей [статье](https://www.adeveloperdiary.com/data-science/computer-vision/how-to-implement-sobel-edge-detection-using-python-from-scratch/). В ней автор применяет и горизонтальный и вертикальный фильтры. Затем он комбинирует их результаты. Исходный код этого примера доступен на [Github](https://github.com/adeveloperdiary/blog/tree/master/Computer_Vision/Sobel_Edge_Detection).

Типичная CNN сеть последовательно выполняет следующие операции над входными данными:

1. **Свёртка**

2. **Пакетная нормализация** (batch normalization)

3. Применение **функции активации** (activation function)

4. **Подвыборка** (pooling)

5. **Сглаживание** (flattening)

Некоторый из этих операций могут повторяться несколько раз.

Иллюстрация 3-42 демонстрирует архитектуру типичной CNN сети для распознавания изображений. Пример задачи для такой сети — определить вид животного на картинке: собака или кошка.

{caption: "Иллюстрация 3-42. Архитектура типичной CNN сети", height: "60%"}
![Архитектура CNN сети](images/Chess/typical-cnn-architecture.png)

Входные данные модели (Input) передаются на первый слой свёртки (Convolution). Этот слой изображает прямоугольник красного цвета. Выходные данные этого слоя передаются на следующий за ним слой подвыборки (Pooling). Затем пара слоёв свёртки и подвыборки повторяется. Обратите внимание, что параметры этих слоёв отличаются от предыдущих.

Все слои свёртки и подвыборки выполняют одну задачу: [**извлечение признаков**](https://ru.wikipedia.org/wiki/Выделение_признаков) (feature extraction) из исходного изображения. На иллюстрации 3-42 эти слои объединяются в один блок розового цвета для наглядности.

После извлечения признаков модель решает задачу **классификации** (classification). За неё отвечают слои, объединённые в зеленый блок. Первый из них слой сглаживания (Flatten) получает данные из последнего слоя подвыборки. Свой результат он передаёт в соединённые последовательно линейный полносвязные слои (Dense), которые и выполняют классификацию. Результат их работы и является результатом (Output), который выдаёт модель.

#### 3.6.3.1 Операция свёртки

Мы рассмотрели применение фильтра к изображению. Эта операция называется [**свёрткой**](https://proglib.io/p/convolution) (convolution). Именно из-за этой операции архитектура CNN и получила своё название.

Рассмотрим подробнее, как работает свёртка. Для простоты допустим, что у нас есть чёрно-белое изображение размером 5x5 пикселей. У него есть один канал серого цвета. Таким образом, каждому пикселю соответствует значение от 0 (белый цвет) до 255 (чёрный цвет).

Компьютер представляет наше изображение в виде двумерного массива пикселей размером 5x5. Пусть он выглядит следующим образом:
{width: "40%"}
![](images/Chess/convolution-source-image-5x5.png)

В CNN сетях перед работой с изображением все значения его пикселей **нормализуют**. Нормализация — это приведение числа к некоторому заданному диапазону. В случае изображений, значения его пикселей приводят к диапазону от 0 до 1. Для простоты нашего примера, мы не будем выполнять нормализацию.

Применим к этому изображению тот же фильтр, который мы использовали для обработки иллюстрации 3-40. Представим его в виде двумерного массива размером 3x3:
{width: "25%"}
![](images/Chess/convolution-kernel-3x3.png)

Эта матрица содержит веса признаков и называется **ядром** (convolution kernel). Она проходит по матрице исходного изображения с заданным шагом, начиная с левого верхнего угла. На каждом шаге выполняется поэлементное умножение: число из ячейки исходного изображения умножается на число из перекрывшей её ячейки ядра. Обратите внимание, что это не [**умножение матриц**](https://ru.wikipedia.org/wiki/Умножение_матриц), а [**произведение Адамара**](https://ru.wikipedia.org/wiki/Произведение_Адамара). В литературе обе эти операции могут обозначаться одинаково: *, &#0120; или &#0215;. Результаты перемножения ячеек суммируются в однин элемент выходного массива.

Рассмотрим подробнее поэлементное умножение. На первом шаге ядро перекрывает ячейки исходного изображения, выделенные зелёным цветом:
{width: "40%"}
![](images/Chess/convolution-source-image-5x5-highlight-first.png)

Перемножаем перекрытые ячейки изображения и ядра, складываем результаты и получаем следующее:
{line-numbers: false, format: text}
```
19 *  1 + 10 *  2 + 39  *  1 +
30 *  0 + 68 *  0 + 238 *  0 +
45 * -1 + 6  * -2 + 59  * -1 = -38
```

Мы получили значение -38 первого элемента в первом ряду карты признаков. Обратите внимание, что карта признаков — это не изображение, а его элементы — не цвета пикселей. В примере с фотографией Лены Сёдерберг мы для наглядности визуализировали карту признаков, полученную после свёртки. Для такой визуализации необоходимо сопоставить значения из карты признаков с некоторыми цветами. Есть разные методы такого споставления. Например, функция [`imshow`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html) из библиотеки Python Matplotlib предлагает целый ряд [**цветовых карт**](https://matplotlib.org/stable/users/explain/colors/colormaps.html) (colormap).

Допустим, что мы выбрали шаг равный единице. Тогда ядро смещается на один столбец вправо. Оно перекроет следующие ячейки исходного изображения:
{width: "40%"}
![](images/Chess/convolution-source-image-5x5-highlight-second.png)

Снова выполняем произведение Адамара. Затем суммируем получившиеся числа:
{line-numbers: false, format: text}
```
10 *  1 + 39  *  2 + 48  *  1 +
68 *  0 + 238 *  0 + 73  *  0 +
6  * -1 + 59  * -2 + 202 * -1 = -190
```

Второй элемент в первом ряду карты признаков равен -190.

Когда ядро достигает правого края изображения, оно смещается вниз на один ряд. После этого ядро движется снова с левого края изображения направо с шагом в один столбец. На последнем шаге ядро перекроет следующую часть изображения:
{width: "40%"}
![](images/Chess/convolution-source-image-5x5-highlight-last.png)

После того, как ядро пройдёт по всему исходному изображению, на выходе мы получим следующую карту признаков:
{width: "25%"}
![](images/Chess/convolution-result-image-3x3.png)

Python скрипт для расчёта карты признаков для нашего примера описан в приложении 5.2.2. Он также доступен на [Github](https://github.com/ellysh/ai-in-strategy-games/blob/master/manuscript/resources/code/Chess/apply-filter-manually.py).

Вы заметили, что размер карты признаков уменьшился до 3x3 по сравнению с размером исходного изображения 5x5? Это связано с потерей информации о пикселях на границах. Если эти пиксели важны, мы можем расширить исходное изображение. Для этого дополним его по краям занулёнными пикселями. Эта операция называется **выравниванием**. 

После выравнивания наше исходное изображение станет таким:
{width: "40%"}
![](images/Chess/convolution-source-image-aligned-7x7.png)

Синим цветом отмечены занулённые пиксели, которые мы добавили. Теперь если мы выполним свёртку для выровненного изображения, на выходе получится карта признаков размером 5x5. Информация о пикселях на границах не будет потеряна.

Конечная и начальная позиции ядра при обработке выровненного изображения выглядят так:
{width: "80%"}
![](images/Chess/convolution-source-image-aligned-7x7-highlight.png)

Как операция свёртки решает проблему проклятия размерности? В нашем примере исходное изображение имеет размер 5x5 пикселей. Это означает, что входной слой модели имеет `5 * 5 = 25` входных признаков. Карта признаков на выходе имеет размер 3x3. Следовательно, число выходных признаков равно `3 * 3 = 9`. Если для обработки изображения применить полносвязный слой нейронов, в нём должно быть `25 * 9 = 225` весовых параметров. Если же применить операцию свёртки, число весовых параметров будет равно размеру ядра 3x3. Это значит, что весовых параметров только `3 * 3 = 9`. Увеличение размера исходного изображение никак не повлияет на размер ядра.

Для простоты примера мы применили вертикальный фильтр оператора Собеля к входному изображению. Его ядро имеет константные весовые параметры. На практике параметры ядра подбираются в процессе обучения модели.

Другая проблема многослойного перцептрона — работа с пространственной структурой данных. Операция свёртки позволяет модели учесть эту структуру. Один и тот же набор весовых параметров ядра применяется к разным областям изображения. Таким образом модель находит одни и те же признаки (например, линии), независимо от их расположения на изображении. На следующем скрытом слое сети обнаруженные признаки обобщаются до шаблонов. Далее шаблоны складываются в объекты. Так свёртка естественным образом восстанавливает иерархию абстракций элементов изображения.

Мы рассмотрели свёртку на примере чёрно-белого изображения. У него есть только один канал серого цвета. Если исходное изображение цветное с тремя каналами (RGB), то говорят об операции **свёртки по объёму** (convolutions over volume). В этом случае для каждого канала нужно отдельное ядро. Все три ядра вместе образуют один фильтр. То есть **фильтр** — это набор ядер, каждое из которых соответствует одному каналу цвета. При этом сами ядра могут между собой различаться.

Допустим, что размер исходного изображения равен `N x N x C`, где `N` — число пикселей, а `C` — число каналов цвета. Тогда для его обработки нужен фильтр размером `F x F x C`, где `F` — число элементов, а `C` — число ядер. После свёртки по объёму  с шагом 1 карта признаков на выходе будет иметь размер `(N - F + 1) x (N - F + 1)`. Обратите внимание, что третье измерение для цветовых каналов пропадает.

Интуитивно мы воспринимаем исходное цветное изображение, как двумерное. Но фактически оно является трёхмерным, если учесть каналы цвета. Именно из-за этого третьего измерения возникает понятие **объёма** (volume). Для его обработки нужны трёхмерные или **3D фильтры**, состоящие из трёх ядер.

В CNN моделях к исходному изображению часто применяют несколько фильтров. Например, мы хотим распознавать не только вертикальные границы объектов, но и горизонтальные. В этом случае нам нужны два отдельных фильтра. Если изображение цветное, то у каждого из них будет по три ядра. Каждый фильтр извлекает разные признаки из исходного изображения и даёт свою карту признаков на выходе. Эти карты мы складываем в **стэк** (stack). Таким образом у нас снова появляется третье измерение. На этот раз оно связано не с числом цветовых каналов, а с числом применяемых фильтров.

Стэк карт признаков передаётся на следующий слой CNN сети. Он обрабатывается следующим трёхмерным фильтром. Этот фильтр распознаёт признаки более высокого уровня абстракции, чем фильтры предыдущего слоя.

I> Если фильтр состоит из нескольких ядер, то выполняется свёртка по объёму. Результат комбинируется в одну карту признаков. Если применяются несколько фильтров, каждый из которых имеет только одно ядро, выполняется обычная свёртка. В результате получается стек карт признаков. Их число равно числу применённых фильтров.

#### 3.6.3.2 Применение функции активации

Мы уже познакомились с понятием функции активации нейронов слоя сети, когда рассматривали архитектуру NNUE. Это одна из двух важных характеристик слоя нейронов.

Функцию активации каждый нейрон выполняет над своими входными данными, чтобы получить данные на выходе. Нейроны свёрточного слоя CNN сети — не исключение и делают то же самое.

Разберёмся, зачем функция активации нейронов нужна свёрточному слою. На это есть несколько причин:

1. Добавление нелинейности в модель. Благодаря функция активации, сеть может усваивать сложные закономерности во входных данных.

2. Выбор признаков. Функция активации позволяет модели концентрироваться на наиболее важных и отличительных признаках данных. Для этого сеть выборочно увеличивает или уменьшает определённые элементы карты признаков.

3. Улучшение обобщающей способности модели. Сеть эффективно работает с новыми изображениями, которые не входили в обучающие наборы.

В свёрточных слоях обычно используется функция активации **ReLU**. Её формула выглядит так:
{width: "50%"}
![](images/Chess/cnn-relu-formula.png)

В ней используются следующие обозначения:

* x — входное значение нейрона
* y — выходное значение нейрона.

Иллюстрация 3-43 демонстрирует график функции ReLU.

{caption: "Иллюстрация 3-43. Функция активации ReLU", height: "40%"}
![Функция активации ReLU](images/Chess/cnn-relu-graph.png)

ReL преобразут входное значение x в результат y от 0 до положительной бесконечности. Если входное значение меньше или равно нулю, то ReLU выдаёт ноль. В противном случае — входное значение.

Рассмотрим, что означает применение функции активации ReLU с точки зрения обработки исходного изображения. Для простоты предположим, что оно чёрно-белое.

Функция активации удаляет все чёрные пиксели и оставляет только те, которые имеют положительное значение (серый и белый цвета). В результате этого цвета на изображении меняются более резко. Нет больше их постепенного изменения. Это указывает на то, что мы избавились от линейности.

#### 3.6.3.3 Операция подвыборки

Начнём с операции подвыборки. Она считается второй фундаментальной концепцией CNN сетей и выполняется после свёртки. Подвыборка решает следующие задачи:

1. Уменьшает пространственные размеры (dimensionality reduction) карты признаков. Это позволяет контролировать вычислительную сложность сети. Меньше признаков означает меньше параметров модели и необходимых вычислений. Небольшие карты признаков быстрее обрабатываются в следующих слоях сети.

2. Инвариантность представлений (translation invariance) признаков. Благодаря операции подвыборки, модель становится менее чувствительной к пространственному расположению объектов на исходном изображении.

3. Агрегация локальных признаков (local feature aggregation). Операция подвыборки выделяет наиболее важные признаки в каждой области входных данных. Это помогает модели отделить важную входную информацию от шума.

4. Уменьшает вероятность переобучения (overfitting) модели. Благодаря упрощению карты признаков, операция подвыборки снижает риск подгонки модели под шум во входных данных.

Операция подвыборки уменьшает размеры карты признаков, полученной после свёртки. Для этого задаётся некоторое **окно подвыборки** (pooling window). Это примерно то же самое, что и ядро в операции свёртки. Отличие в том, что окно подвыборки имеет размер, но не имеет весов. Его единственная задача — проходить по входному массиву и выделять его элементы. На каждом шаге к выделенным элементам применяется операция подвыборки. Обычно это одна из следующих:

1. **Max pooling** (подвыборка по максимальному значению).

2. **Average pooling** (подвыборка по среднему значению).

Подвыборка по максимальному значению выбирает из всех выделенных элементов наибольший. Именно он записывается в выходной массив, а остальные отбрасываются.

Рассмотрим пример. Допустим, что у нас есть карта признаков размером 5x5. Для её обработки мы выбираем окно подвыборки размером 3x3. Выберем шаг прохода по входному массиву равный двум. Тогда на первом шаге, окно перекроет следующую часть карты признаков:
{width: "40%"}
![](images/Chess/pooling-source-image-5x5-highlight-first.png)

В перекрытой области максимальное значение ранво 251. Это будет значением первого элемента в выходной матрице операции подвыборки. 

На втором шаге окно подвыборки перекроет такую часть карты признаков:
{width: "40%"}
![](images/Chess/pooling-source-image-5x5-highlight-second.png)

В этой области максимальное значение снова равно 238. Это второй элемент в выходной матрице.

Теперь окно подвыборки возвращается в левый край карты признаков и смещается вниз на два ряда, потому что мы выбрали шаг равный двум. Мы повторяем операцию ещё два раза и получаем следующую выходную матрицу:
{width: "25%"}
![](images/Chess/max-pooling-result-image-2x2.png)

Подвыборка по среднему значению рассчитывает среднее значение всех выделенных элементов. Вернёмся к первому шагу нашего примера:
{width: "40%"}
![](images/Chess/pooling-source-image-5x5-highlight-first.png)

Среднее значение элементов в перекрытой области карты признаков рассчитывается так:
{line-numbers: false, format: text}
```
(19 + 10 + 39 + 30 + 251 + 238 + 45 + 6 + 59) / 9 = 77.4444
```

Полученный результат округляем до целого и получаем 77. Это и будет значением первого элемента в выходной матрице операции подвыборки. Повторим операцию для оставшихся трёх шагов и получим такой результат:
{width: "25%"}
![](images/Chess/average-pooling-result-image-2x2.png)

#### 3.6.3.4 Операция сглаживания

Операция сглаживания выполняется после подвыборки. Её главная задача преобразовать пространственную информацию из карты признаков в формат, который может обработать полносвязный слой сети. В терминологии CNN такой слой также называется **плотным** (dense).

Операция сглаживания важна по следующим причинам:

1. Переход к полносвязному слою сети, который выполняет высокоуровневое прогнозирование, классификацию или распознавание объектов. Такой слой требует одномерных входных данных. Операция сглаживания как раз предоставляет такой формат на выходе.

2. Сохранение пространственной информации. Слои свёртки фиксируют пространственную иерархию признаков: края объектов, их текстуры и части. Эта информация отображается на карте признаков. Операция сглаживания сохраняет обнаруженные при свёртке признаки, но отбрасывает пространственную информацию. Другими словами, одномерные выходные данные сглаживания сохраняют семантическую информацию карты признаков.

Рассмотрим операцию сглаживания на примере. Допустим, что мы получили такую карту признаков после подвыборка по максимальному значению:
{width: "25%"}
![](images/Chess/max-pooling-result-image-2x2.png)

Операция сглаживания развернёт эту матрицу размером 2x2 в одномерный массив из четырёх элементов:
{width: "40%"}
![](images/Chess/flattening-result-image-4x1.png)

Если операция подвыборки возвращает стек карт признаков, то сглаживание развернёт все эти карты в один одномерный массив. Только такой формат данных сможет обработать последующий полносвязный слой сети.

#### 3.6.3.5 Пакетная нормализация

При обучении deep learning моделей часто встречается проблема [**ковариантного сдвига**](https://habr.com/ru/articles/422185/) (covariance shift). Рассмотрим её на наглядном примере.

В разделе "2.6.2.1.1 Классификация" мы классифицировали ирисы из набора данных Фишера. Допустим, что теперь мы хотим обучить более сложную модель. Она получает на вход фотографию некоторого цветка. По нему модель отвечает на вопрос: "относится ли цветок к роду ирисов?".

Разные виды ирисов имеют разные цвета. Иллюстрация 3-44 демонстрирует их вариативность.

{caption: "Иллюстрация 3-44. Фотография ирисов разных цветов", width: "75%"}
![Ирисы разных цветов](images/Chess/iris-flowers.jpg)

Теперь предположим, что в обучающий набор данных (training set) попали только цветки ирисов фиолетового цвета и другие растения. В тестовом наборе (test set) данных помимо других растений есть ирисы разных цветов (фиолетовый, белый, желтый). В этом случае точность работы модели будет неудовлетворительной. Она не сможет распознавать ирисы не фиолетового цвета.

Причина низкой точности модели в том, что обучающий и тестовый наборы содержат фотографии ирисов разных цветов в разных пропорциях. Наша модель обучена отображать исходное множество X (цветки) в целевое множество Y (ирисы). Если пропорция элементов в X меняется, то необходимо переобучить модель заново. Таким образом мы выравниваем пропорции элементов в обоих множествах X и Y. Именно это изменение пропорций элементов и называется ковариантным сдвигом.

Мы рассмотрели пример ковариантного сдвига во входных данных модели. Эта проблема решается случайным перемешиванием данных из обучающего набора перед созданием **пакетов** (batch). Что такое пакет?

Алгоритм градиентного спуска имеет [три возможные реализации](https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a):

1. **Стохастический градиентный спуск** (stochastic gradient descent) — на каждой итерации алгоритма из обучающего набора данных случайно выбирается только один элемент.

2. **Пакетный градиентный спуск** (batch gradient descent) — на каждой итерации алгоритм просматривает обучающий набор данных целиком и только после этого меняет веса модели.

3. Промежуточное решени — на каждой итерации алгоритм просматривает некоторое подмножество обучающего набора данных фиксированного размера (batch size). Эти подмножества называются **мини-пакетами** (mini-batch).

На практике чаще всего применяется третья реализации алгоримта градиентного спуска. Поэтому для простоты мини-пакеты называют просто пакетами.

Ковариантный сдвиг встречается не только во входных данных модели. Он происходит и в данных, которые передаются между скрытыми слоями сети. В этом случае пропорции элементов входного массива слоя изменяются каждый раз, когда меняются параметры в предыдущем слое. Эта проблема называется **внутренним ковариантным сдвигом** (internal covariate shift). Её невозможно решить перемешиванием входных данных. Именно для неё был разработан метод [**пакетной нормализации**](https://en.wikipedia.org/wiki/Batch_normalization).

Метод пакетной реализации предложили сотрудники Google Сергей Иоффе и Кристиан Сегеди в 2015 году. Они подробно описали новый подход в своей статье "Пакетная нормализация: Ускорение глубокого обучения за счёт уменьшения внутреннего ковариатного сдвига" (["Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"](https://arxiv.org/pdf/1502.03167.pdf)).

Разберёмся, как работает пакетная нормализация. Её алгоритм выглядит следующим образом:

1. Вычислить [**среднее арифметическое**](https://en.wikipedia.org/wiki/Mean) (mean) всех элементов пакета по формуле:
{height: "15%"}
![](images/Chess/mean-formula.png)

В этой формуле следующие обозначения:

* x~i~ — элементы выборки под номером i
* m — размер пакета.

2. Вычислить [**дисперсию**](https://en.wikipedia.org/wiki/Variance) (variance) для элементов пакета по формуле:
{width: "40%"}
![](images/Chess/variance-formula.png)

3. Для простоты допустим, что пакет представляет собой одномерный массив. Тогда нормализованное значение каждого его элемента рассчитывается по формуле:
{width: "50%"}
![](images/Chess/normalized-value-formula.png)

В этой формуле следующие обозначения:

* ϵ — некоторая малая константа.

В результате такой нормализации мы получим распределение значений с центром в 0 и дисперсией равной 1.

I> **Дисперсия случайной величины** — это ожидаемое значение [**квадрата отклонения от среднего значения**](https://en.wikipedia.org/wiki/Squared_deviations_from_the_mean) случайной величины.

Нормализация входа слоя может изменить представление данных в слое. Чтобы этого избежать, вводятся два дополнительных параметра модели: сжатие (γ) и сдвиг (β)нормализованной величины. Они действуют следующим образом:
{width: "50%"}
![](images/Chess/scale-and-shift-formula.png)

Параметры γ и β настраиваются в процессе обучения вместе с остальными параметрами модели.

Предположим, что мы хотим применить пакетную нормализацию к скрытому слою B сети. Этот слой полчает данные от предыдущего слоя A. В этом случае пакетная нормализация выполняется после преобразования данных при их передачи из слоя A в слой B. Только после этого применяется функция активации слоя.

Использование пакетной нормализации в моделе даёт следующие преимущества:

1. Устраняет внутренний ковариантный сдвиг, возникающий во время обучения. Это повышает стабильность процесса обучения и упрощает оптимизацию модели.

2. Улучшает обобщающую способность модели и уменьшает вероятность переобучения.

3. Снижает чувствительность модели к начальным весам, которые задаются на этапе инициализации. Это упрощает обучение модели.

4. Повышает скорость обучения, поскольку позволяет использовать более высокий [**темп обучения**](https://en.wikipedia.org/wiki/Learning_rate) (learning rate). Часто увеличение темпа обучения приводит к взрыву или исчезновению градиента, но пакетная нормализация предотвращает эти проблемы.

I> Темп обучения — настраиваемый параметр алгоритма обучения, который определяет размер шага на каждой итерации при движении к минимуму **функции потерь** (loss function). Функция потерь отображает стоимость неправильного принятия решений на основе наблюдаемых данных.

Метод пакетной нормализации применим к различным архитектурам нейронных сетей, которые ориентированы на разные задачи. Среди этих задач: классификация изображений, обнаружение объектов, обработка естественного языка и многое другое. Благодаря своей универсальности, метод стал широко распространён в различных областях машинного обучения.

### 3.6.4 Нейронная сеть AlphaZero

Мы познакомились со свёрточными нейронными сетями. У читателя может возникнуть вопрос: как сеть, оптимизированная для распознавания изображений, может оценивать шахматные позиции?

Начнём с того, что позицию фигур на доске можно представиь в виде матрицы размером 8x8. Свёрточные слои хорошо подходят для обработки входных данных подобной структуры. Она напоминает структуру типичного изображения.

В разделе "3.3.1.2 Теория блоков" мы познакомились с теорией блоков Герберта Саймона. Согласно ей, профессиональные шахматисты иерархически упаковывают информацию о позиции. Это позволяет им рассматривать не отдельные фигуры, а их сочетания.

Свёрточная нейронная сеть делает примерно то же самое, что и шахматисты. Она работает с визуальной структурой, когда оценивает позицию на шахматной доске. Сеть может извлекать такие признаки как:

* открытые вертикали
* расположение коней
* открытые диагонали слонов
* пешечная структура

Сложно ввыяснить, какие именно признаки оценивает обученная готовая модель. Эти признаки обобщаются в высокоуровневые шаблоны наподобие тех, которые распознают профессиональные игроки. Таким образом поведение модели больше напоминает действия шахматистов, чем алгоритмы традиционнх движков наподобие Stockfish.

Вот общие принципы работы модели AlphaZero:

* Нейронная сеть получает на вход текущюу позицию фигур на доске.

* Нейронная сеть на выходе даёт два значения. Первое — **общая оценка позиции**. Она указывает вероятность победы белых, чёрных или ничьи. Второе значение — вероятность каждого возможного в текущей позиции хода. По-сути это оценка ходов: насколько хорош каждый из них.

* Для поиска лучшего хода в данной позиции модель AlphaZero использует поиск методом Монте-Карло. Процессом поиска управляет нейронная сеть. Она определяет, какой ход в дереве поиска выбрать на первом шаге selection для исследования. Из всех возможных ходов выбираются только ходы с максимальными вероятностями. Далее шаг simulation в AlphaZero отличается от обычного алгоритма MCTS. Обычно MCTS проигрывает партию до конца, выбирая случайные ходы. Вместо этого алгоритм AlphaZero узнаёт вероятный результат партии из общей оценки позиции. Её выдаёт нейронная сеть после анализа хода, выбранного на шаге selection.

* Алгоритм MCTS применяется также во время обучения нейронной сети, когда AlphaZero играет сам с собой. Для каждого хода в партии выполняется поиск MCTS. Им управляет текущая версия нейронной сети. В результате получаются наборы позиций и конечные результаты партий, к которым они приводят. Алгоритм обучения получает эти данные на вход.

#### 3.6.4.1 Входные данные сети

Традиционные шахматные дивжки наподобие Stockfish используют битборды для представляения позиций фигур. Такой подход идеально подходит для выполнения битовых операци на 64-разрядных процессорах. В случае нейронной сети, работающей на GPU, такая оптимизации ненужна.

Нейронная сеть AlphaZero получает на вход набор матриц (plane) размера 8x8. Эти матрицы кодируют как позицию фигур на доске, так и состояние партии (например, какой игрок сейчас ходит). Рассмотрим эти матрицы подробнее:

* Шесть матриц кодируют положение белых фигур на доске. На фигуры каждого типа отводится своя матрица: пешки, кони, слоны, ладьи, ферзь, король. Если фигура стоит на каком то поле, то элемент матрицы с этим индексом равен 1. В противном случае этот элемент равен 0. Например, белая пешка стоит на поле b2. Тогда соответствующий ей элемент с индексом (2, 2) равен 1.

* Шесть матриц кодируют положение чёрных фигур на доске. Они работают точно так же, как для белых фигур.

* Две матрицы используются для подсчёта повторяющихся ходов. Все элементы одной из них заполняется единицами, если текущая позиция повторялась ранее единожды. Элементы второй матрицы выставляются в единицы, если текущая позиция повторялась дважды. Это нужно, чтобы отслеживать ничью при троекратном повторении позиции (правило трёх ходов).

* Чтобы отслеживать правило трёх ходов, во входных данных сети запоминаются последние восемь позиций. В начале партии они занулены. Далее по ходу партии они отражают реальные позиции. Чтобы хранить эту историю позиций, нужно `(6 + 6) * 8 = 96` матриц.

* Одна матрица хранит цвет фигур игрока, который делает ход. Все её элементы равны 1, если ходят белые. При ходе чёрных она зануляется.

* Четыре матрицы кодируют правила рокировки: белые короткая (на королевском фланге), белые длинная (на ферзевом фланге), чёрная короткая и чёрная длинная. Если соответствующая матрице рокировка возможна, все её элементы выставляются в 1. В противном случае все матрицы элементы выставляются в 0.

* Одна матрица используется как счётчик ходов с начала партии. Её элементы, начиная с первого (1,1), интерпретируются как разряды целого числа.

* Одна матрица кодирует счётчик для отслеживания правила 50-ти ходов. Согласно ему, если в течении 50-ти ходов не было взятий и ходов пешками, то объявляется ничья.

Мы рассмотрели 117 входных матриц размера 8x8 сети AlphaZero. Комнда DeepMind опубликовала статью [Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm](https://arxiv.org/pdf/1712.01815.pdf) с описанием архитектуры AlphaZero. В ней сказано, что сеть имеет 119 входов. При этом назначаение двух оставшихся входов не уточняется.

#### 3.6.4.2 Выходные данные сети

Нейронная сеть AlphaZero на выходе даёт два значение. Первое — **общая оценка позиции** (value). Это грубая оценка наиболее вероятного результата партии, учитывая текущую позицию. Она нужна, чтобы заменить проигрывание партии с выполнением случайных ходов на шаге simulation алгоритма MCTS. Общая оценка позиции всё равно оказывается точнее, чем подобное проигрывание партии.

Общая оценка позиции — это вещественное число в диапазоне [-1, 1]. Его зачения -1, 0 и 1 имеют следующий смысл:

* -1 — проигрывает игрок, выполняющий текущий ход.
* 0 — ничья.
* 1 — выигрывает игрок, выполняющий текущий ход.

Промежуточные значения означают степень вероятности. Например, число 0,5 говорит о победе игрока, выполняющего текущий ход, в половине случаев.

Второй вывод сети — **вероятность каждого возможного хода** в текущий позиции (policy). Его формат достаточно сложный. Проблема заключается в том, что количество допустимых ходов меняется в зависимости от позиции фигур на доске. С другой стороны число выходов нейронной сети должно быть постоянным. Рассмотрим, как команда DeepMind решила это затруднение.

Идея разработчиков AlphaZero заключается в том, чтобы для любой позиции на доске рассматривать некоторый одинаковый шаблон возможных ходов. В этот шаблон входят все поля на доске в качестве точек старта. В них начинает свой ход некоторая фигура. Далее для каждого исходного поля перечисляются все возможные ходы "суперфигуры". Она может ходить как ферзь и конь. Каждому возможному ходу "суперфигуры" из каждого начального поля доски соответствует один выход нейронной сети.

Такой формат выходных данных избыточен. Нейронная сеть будет оценивать вероятность некоторых ходов как ненулевую. Но эти ходы будут запрещены по правилам шахмат. Чтобы решить эту проблему, достаточно занулять вероятности запрещённых ходов. После этого необходимо пересчитать вероятности оставшихся разрешённых ходов так, чтобы их сумма снова стала равна единице.

Представим для наглядности шаблоны возможных ходов в виде набора таблиц. Таблица 3-1 демонстрирует все ходы "суперфигуры" как ферзя из всех начальных полей доски. Первый столбец указывает поле, из которого фигура начинает ход. Второй столбец — это направление, в котором она двигается. Третий столбец — число полей, на которое передвигается фигура. Каждая строка таблицы соответствует одному выходу нейронной сетри AlphaZero.

{caption: "Таблица 3-1. Все возможные ходы ферзём", width: "80%"}
| Начальное поле | Направление | Число полей |
| --- | --- | --- |
| a1 | Вверх | 1 |
| a1 | Вверх | 2 |
| ... | ... | ... |
| a1 | Вверх | 6 |
| a1 | Вверх | 7 |
| a1 | Вверх-вправо | 1 |
| ... | ... | ... |
| a1 | Вверх-вправо | 7 |
| a1 | Вправо | 1 |
| ... | ... | ... |
| a1 | Вправо | 7 |
| a1 | Вниз | 1 |
| ... | ... | ... |
| a1 | Вниз | 7 |
| ... | ... | ... |
| ... | ... | ... |
| h8 | Влево | 7 |

Первая строка таблицы соответствует ходу любой фигуры, стоящей на a1, вверх на одно поле. Таким образом этот выход нейронной сети определяет вероятность хода a1-a2. Следующая строка таблицы и соответствующий выход сети описывают ход фигуры a1-a3. Продолжая аналогично, описываются все возможные ходы ферзём со всех полей доски.

Обратите внимание, что таблица 3-1 учитывает в том числе и ходы за пределы доски. Например, ходы из поля a1 в направлениях: вниз-вправо, вниз, вниз-влево, влево, вверх-влево. Такое решение упрощает формат входных данных. Ходы за пределы доски запрещены правилами шахмат, поэтому их вероятность всегда зануляется.

Ходы ферзём покрывают следующие типы фигур: пешка, слон, ладья, ферзь, король. Остаются ходы конём, которые не учитывает таблица 3-1. Составим для коней аналогичную таблицу 3-2.

{caption: "Таблица 3-2. Все возможные ходы конём", width: "80%"}
| Начальное поле | Направление |
| --- | --- |
| a1 | Два вверх, одно вправо |
| a1 | Два вправо, одно вверх |
| a1 | Два вправо, одно вниз |
| a1 | Два вниз, одно вправо |
| a1 | Два вниз, одно влево |
| a1 | Два влево, одно вниз |
| a1 | Два влево, одно вверх |
| a1 | Два вверх, одно влево |
| a2 | Два вверх, одно вправо |
| ... | ... |
| a2 | Два вверх, одно влево |
| ... | ... |
| ... | ... |
| h8 | Два вверх, одно влево |

Первая строка таблицы описывает ход коня из поля a1 на поле b3. Она соответствует выходу нейронной сети, который определяет вероятность хода a1-b3. Продолжая аналогично, мы получили выходы сети для всех возможных ходов конём. Некоторые из них выходят за пределы доски и запрещены правилами игры точно так же, как ходы ферзём из таблицы 3-1.

Последний тип ходов, которые не учитывают таблицы 3-1 и 3-2 — это превращение пешки. По правилам пешка, достигшая последней горизонтали (8-ой для белых и 1-ой для чёрных), превращается в любую фигуру кроме короля. Пешка может достигнуть последней диагонали в результате одного из следующи ходов:

* Движение вперёд на одно поле.
* Взятие вражеской фигуры на соседнем поле по диагонали слева.
* Взятие фигуры на соседнем поле по диагонали справа.

Чтобы учесть эти ходы, составим отдельную таблицу 4.3.

{caption: "Таблица 3-3. Все ходы с превращением пешки", width: "80%"}
| Начальное поле | Тип хода | Превращение в фигуру |
| --- | --- | --- |
| a1 | Вперёд | Конь |
| a1 | Вперёд | Слон |
| a1 | Вперёд | Ладья |
| a1 | Взятие влево | Конь |
| ... | ... | ... |
| a1 | Взятие влево | Ладья |
| a1 | Взятие вправо | Конь |
| ... | ... | ... |
| a1 | Взятие вправо | Ладья |
| a2 | Вперёд | Конь |
| ... | ... | ... |
| a2 | Взятие вправо | Ладья |
| ... | ... | ... |
| h8 | Взятие вправо | Ладья |

Мы не рассматриваем ходы пешки с превращением в ферзя, потому что они покрываются таблицей 3-1.

Из таблицы 3-3 очевидно, что только малая часть перечисленных в ней ходов допустима правилами шахмат. Например, ходы из поля a1 не достигают последней горизонтали (8-ой или 1-ой). Поэтому их вероятность всегда зануляется. Тем не менее каждая строка таблицы 3-3 соответствует одному выходу нейронной сети.

Рассчитаем сумму всех выходов нейронной сети. Таблица 3-1 содержит следующее число строк:
{line-numbers: false, format: text}
```
8 * 7 * 64 = 3584
```
В этой формуле: 

* 8 — число направлений для хода ферзём.
* 7 — число полей при движении в каждом направлении.
* 64 — число полей на доске, с которых может ходить ферзь.

Таблица 3-2 содержит такое число строк:
{line-numbers: false, format: text}
```
8 * 64 = 512
```
В этой формуле: 

* 8 — число направлений для хода конём.
* 64 — число полей на доске, с которых может ходить конь.

В таблице 3-3 строк столько:
{line-numbers: false, format: text}
```
3 * 3 * 64 = 576
```
В этой формуле: 

* 3 — число направлений для хода пешкой.
* 3 — число фигур для превращения пешки.
* 64 — число полей на доске, с которых может ходить пешка.

Сложим полученные результаты:
{line-numbers: false, format: text}
```
3584 + 512 + 576 = 4672
```

Таким образом у нейронной сети AlphaZero всего 4672 выхода. Эти выходы относятся только ко второму выводу сети, который оценивает вероятности возможных ходов. Они никак не связаны с первым выводом, который по текущей позиции оценивает вероятный результат партии.

Для обучения сети с таким большим количеством выходов нужны значительные вычислительные ресурсы. Почему команда DeepMind выбрала такой подход? На самом деле разработчики экспериментировали с альтернативным кодированием выходов. В этом случае перечислялись все допустимые ходы: a1-a2, a1-a3, a1-a4 и т.д. Таким образом полчаются все возможные комбинации пар: исходное поле, целевое поле. При этом исключаются запрещённые правилами ходы за пределы шахматной доски. К этому списку нужно добавить допустимые правилами ходы на превращение пешки. В итоге получится менее 2000 выходов сети.

На практике оказалось, что нейронная сеть с меньшим числом выходов обучается дольше. Причина в том, что подход с 4672 выходами сфокусирован на исходном поле фигуры. Это позволяет модели разделить решение о выборе хода на две части. Сначала грубо решить, какая именно фигура должна ходить в текущей позиции. Чтобы обучится этому, модели достаточно совсем немного итераций алгоритма обучения. Затем после дополнительных итераций, сеть учится более точно определять, куда именно должна пойти выбранная фигура.

#### 3.6.4.3 Архитектура сети

Архитектура нейронной сети AlphaZero очень похожа на сеть AlphaGo Zero. Эта архитектура подробно описана в статье [Mastering Go without Human Knowledge](https://discovery.ucl.ac.uk/10045895/1/agz_unformatted_nature.pdf).

Здесь мы рассмотрим наглядную блок-схему нейронной сети AlphaZero. Её демонстрирует иллюстрация 3-45.

{caption: "Иллюстрация 3-45. Блок схема нейронной сети AlphaZero", width: "100%"}
![Нейронная сеть AlphaZero](images/Chess/alphazero-network-architecture.png)

Как мы уже выяснили, входные данные сети (Input) представляют собой стек bp 119 матриц размером 8x8. Его последовательно обрабатывают несколько блоков. Познакомимся с ними в порядке следования.

Сначала входные данные поступают на первый **блок свёртки** (Convolutional block). Этот блок последовательно выполняет следующие операции:

1. **Свёртка типа А** (Convolution A). Этот свёрточный слой состоит из стека 256 фильтров. Каждый фильтр имеет одно ядро размером 3x3. Операция свёртки выполняется с шагом 1.

2. **Пакетная нормализация** (Batch normalization).

3. **Функция активации ReLU** (Activation function ReLU).

Первый блок вызывает несколько вопросов. Во-первых, если во входном стеке только 119 матриц, то как их обрабатывают 256 фильтров свёрточного слоя? В этом случае нельзя применить свёртку по объёму. При такой операции число слоёв исходного изображения совпадает с числом ядер. В результате все слои исходного изображения сводятся в один слой карты признаков. В случае AlphaZero это решение неприемлемо. Каждый из 119 слоёв несёт важную информацию и их данные не должны смешиваться между собой.

Вместо свёртки по объему происходит обычная свёртка. При этом каждое из 256 ядер применяется к каждой из 119 входных матриц. В результате получается 119 наборов по 256 карт признаков в каждом. Затем каждый набор из 256 карт признаков сворачивается в одну карту признаков. Для этого можно, например, просто просуммировать их элементы. То есть элемент (1,1) результирующей карты признаков равен сумме всех элементов (1,1) из набора 256 карт признаков. Так на выходе из свёрточного слоя мы получим стек из 119 карт признаков.

Второй вопрос — какой размер имеет каждая из 119 карт признаков после операции свёртки? Если применять фильтры без выравнивания, то это приведёт к потере данных. Информация об элементах на границах (левая, верхняя, правая, нижняя) входных матриц 8x8 потеряется. Это допустимо при обработке изображений, но совершенно неприемлемо для матриц с расположением фигур на доске. Поэтому перед обработкой каждой входной матрицы выполняется выравнивание. В результате все 119 выходных карт признаков имеют тот же размер 8x8.

Выходные данные первого блока свёртки (Convolutional block) передаются на первый **остаточный блок** (Residual block). Этот блок последовательно выполняет следующие операции:

1. **Свёртка типа А** (Convolution A).

2. **Пакетная нормализация** (Batch normalization).

3. **Функция активации ReLU** (Activation function ReLU).

4. **Свёртка типа А** (Convolution A).

5. **Пакетная нормализация** (Batch normalization).

6. **Остаточное соединение** (Residual connection).

7. **Функция активации ReLU** (Activation function ReLU).

Остаточнй блок нейронной сети можно представить, как комбинацию двух блоков свёртки. В нём выполняются точно такие же операции только дважды. Принципальное отличие блока — это **остаточное соединение** (Residual connection).

Остаточный блок работает следующим образом:

1. **Первый поток данных**: входные данные блока последовательно проходят через слои свёртки, пакетную нормализацию и функции активации.

2. **Второй поток данных**: остаточное соединение передаёт входные данные блока как есть напрямую к выходу блока, минуя все его слои.

3. **Остаточная операция** (residual operation): первый и второй потоки данных суммируются.

4. **Выходные данные блока** представляют собой результат остаточной операции. Они вычисляются по следующей формуле:
{line-numbers: false, format: text}
```
output = layer(input) + input
```
В этой формуле:

* `output` — выходные данные остаточного блока.
* `layer(input)` — результат прохода входных данных блока через все его слои (первый поток данных).
* `input` — входные данные блока как есть (второй поток данных).

Остаточные блоки позволили команде DeepMind избежать проблемы исчезающего градиента при обучении такой глубокой нейронной сети как у AlphaZero.

Всего в сети 19 остаточных блоков. Они следуют друг за другом. На выходе из них мы получаем 119 карт признаков размера 8x8. Они отличаются от выходных карт признаков блока свёртки тем, что содержат шаблоны более высокого уровня абстрации. Как и в случае сетей CNN для распознавания изображений, выяснить суть этих шаблонов крайне сложно.

Выходные данные последовательности остаточных блоков передаются по двум разным направляениям. В терминологии сети AlphaZero каждое направление идёт в блоки под названем **головы** (head).

Первая **голова вычисляет вероятности возможных ходов** (policy head). Она последовательно выполняет следующие операции:

1. **Свёртка типа B** (Convolution B). Этот свёрточный слой состоит из стека 2-х фильтров. Каждый фильтр имеет одно ядро размером 1x1. Операция свёртки выполняется с шагом 1.

2. **Пакетная нормализация** (Batch normalization).

3. **Функция активации ReLU** (Activation function ReLU).

4. **Операция сглаживания** (Flatten). После свёртки типа B получается 119 карт признаков размером 8x8. Операция сглаживания приводит их в формат одномерного массива, который нужен следующему далее линейному слою.

5. **Линейный полносвязный слой типа A** (Dense A). По входным данным вычисляет значения для каждого из 4672 выходов нейронной сети, которые оценивают вероятности всех возможных ходов в текущей позиции.

Вторая **голова вычисляет общую оценку позиции** (value head). Она последовательно выполняет следующие операции:

1. **Свёртка типа C** (Convolution C). Этот свёрточный слой состоит из одного фильтра, который имеет одно ядро размером 1x1. Операция свёртки выполняется с шагом 1.

2. **Пакетная нормализация** (Batch normalization).

3. **Функция активации ReLU** (Activation function ReLU).

4. **Операция сглаживания** (Flatten). После свёртки типа C получается 119 карт признаков размером 8x8. Операция сглаживания приводит их в формат одномерного массива, который нужен следующему далее линейному слою.

5. **Линейный полносвязный слой типа B** (Dense B). Сводит входные данные в выходной одномерный массив из 256 элементов. Это входной формат следующего далее линейного слоя.

6. **Функция активации ReLU** (Activation function ReLU).

7. **Линейный полносвязный слой типа C** (Dense C). Сводит массив входных данные к одному [**вещественному числу**](https://ru.wikipedia.org/wiki/Вещественное_число) (real number).

8. **Функция активации TanH** (Activation function TanH). Приводит вещественное число, полученное на выходе из линейного слоя типа C, к диапазону значений [-1, 1]. Результат определяет наиболее вероятный исход партии с учётом текущей позиции на доске.

Название функция TanH — это сокращение от [**hyperbolic tangent**](https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/) (гиперболический тангенс). Её формула выглядит так:
{height: "10%"}
![](images/Chess/tanh-formula.png)

В ней используются следующие обозначения:

* x — входное значение нейрона
* y — выходное значение нейрона
* e — константа [основание натурального логарифма](https://ru.wikipedia.org/wiki/E_(число)).

Иллюстрация 3-46 демонстрирует график функции TanH.

{caption: "Иллюстрация 3-46. Функция активации Tanh", height: "40%"}
![Функция активации Tanh](images/Chess/tanh-graph.png)

Из графика видно, что чем больше входное значение x, тем ближе результат функции y к значению 1. Аналогично, чем меньше значение x, тем ближе y к -1.

### 3.6.5 Результаты AlphaZero

В системе AlphaZero команда DeepMind реализовала принципиально новый подход к разработке шахматных движков. В эту систему не было заложено никаких знаний об игре в шахматы (например, дебюты и шахматные окончания). Только некоторые правила игры повлияли на структуру входов и выходов нейронной сети, а также порядок поиска методом Монте-Карло.

Принципиальное отличие AlphaZero от традиционных шахматных движков — это характер поиска. Движки, основанные на алгоритме минимакс с альфа-бета отсечением, большую часть времени тратят на перебор возможных ходов. Их функция оценки позиции работает чрезвычайно быстро.

С другой стороны AlphaZero большую часть времени тратит на оценку позиций, а не на поиск. Причина в том, что каждое обращение к нейронной сети требует времени и огромных вычислительных ресурсов. Из-за этого AlphaZero анализирует только малую часть возможных ходов в отличие от движков на альфа-бета поиске. Таким образом AlphaZero делает ставку на качество анализа, а не на количественные показатели вроде числа просмотренных ходов.

Достижения AlphaZero повлияли на развитие шахматной теории. В результате самостоятельного обучения с подкреплением система нашла новые варианты в дебютах, которое ранее считались малоперспективными. Это заставило шахматное сообщество пересмотреть устоявшуюся дебютную теорию. Также новый стиль игры, продемонстрированный AlphaZero, дал новые идеи для теории шахматной стратегии и тактики.

AlphaZero представляет собой программную платформу (фреймворк) для решения целого класса игр для двух участников. Эти игры должны иметь следующие признаки:

1. Последовательный порядок ходов.
2. Совершенная информация об игровых событиях.
3. Детерминированная игра.
4. Фиксированные правила игры.
4. Некооперативная игра.
5. Дискретная.

Наследником идей AlphaZero считается шахматный движок Leela Chess Zero. Его модель оптимизирована только для игры в шахматы. Кроме этого в него добавили дебютную книгу и информацию об эндшпилях. Благодаря этим улучшениям, Leela Chess Zero смогла стать достойным конкурентом для традиционных шахматных движков наподобие Stockfish. Её успехи заставили разработчиков всех шахматных программ обратить внимание на технологию нейронных сетей.

{pagebreak}
