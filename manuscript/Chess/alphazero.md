## 3.6 AlphaZero

5 декабря 2017 года компания [DeepMind](https://en.wikipedia.org/wiki/Google_DeepMind) опубликовала статью о своей новой системе. В статье исследователи рассказали о принципах работы и результатах [AlphaZero](https://en.wikipedia.org/wiki/AlphaZero).

AlphaZero 24 часа обучалась шахматам, играя сама с собой. После этого она сыграла матч из 100 партий со Stockfish версии 8. Контроль времени был укороченный: каждая сторона имела в среднем 1 минуту на ход. Результаты матча были следующими:

* AlphaZero выиграла 28 партии.
* Ничьей закончили 72 партии.
* Stockfish 8 выиграл 0 партий.

AlphaZero работала по совершенно иному принципу, чем типичный шахматный движок. В ней не было ни минимаксного поиска, ни вручную запрограммированной функции оценки позиции. Именно новый подход исследователей DeepMind вызвал сенсацию в шахматном мире.

AlphaZero стала прототипом для нового типа современных шахматных движков. Самый известный и успешный из них — [Leela Chess Zero](https://ru.wikipedia.org/wiki/Leela_Chess_Zero) (Lc0). Он распространяется с открытым исходным кодом, так же как и Stockfish.

### 3.6.1 История AlphaZero

Перед тем как рассмотреть внутреннее устройство системы AlphaZero, познакомимся с его историей. Она поможет нам лучше понять почему он работает именно так, а не иначе.

#### 3.6.1.1 AlphaGo

Исследователи DeepMind создали свою революционную систему AlphaZero не с нуля. Её архитектура основана на идеях двух более ранних систем AlphaGo и AlphaGo Zero. Разработчики следовали путём проб и ошибок. Так они пришли к техническим решениям, которые воплотились в AlphaZero.

Команда DeepMind уже занималась какое-то время проектами для настольных игр и классических аркад для приставок [Atari](https://ru.wikipedia.org/wiki/Atari). Эти игры были достаточно простыми. На них исследователи изучали возможности обучения с подкреплением.

Проект [AlphaGo](https://en.wikipedia.org/wiki/AlphaGo) начался в 2014 году. Он преследовал несколько целей. Во-первых, AlphaGo была не просто системой для решения конкретной практической задачи. Она должна была стать платформой для тестирования передовых методов ИИ. В ней объединялись несколько сетей deep learning с различными архитектурами и алгоритм поиска по дереву методом Монте-Карло.

Во-вторых, исследователи хотели проверить свои наработки в сложной задаче. Обучение с подкреплением хорошо показало себя в простых аркадных играх. Но оставался открытым вопрос: способна ли модель справиться с очень сложной игрой? Именно поэтому команда выбрала [го](https://ru.wikipedia.org/wiki/Го). Считается, что это самая сложная настольная игра как для человека, так и для системы ИИ.

К началу 2010-х годов программы для го сильно уступали профессиональным игрокам. Они играли на среднем любительском уровне. Лучшие результаты показали две программы. В 2012 году программа [Zen](https://en.wikipedia.org/wiki/Zen_(software)) дважды выиграла у профессионального игрока Масаки Такемия (9 дан). Сначала это была победа с [форой](https://ru.wikipedia.org/wiki/Го#Фора) в пять камней, а затем — с форой в четыре камня. В 2013 году программа [Crazy Stone](https://en.wikipedia.org/wiki/Crazy_Stone_(software)) выиграла у Ёсио Исида (9 дан) с форой в четыре камня. Несмотря на эти победы, силы сторон в каждой партии сильно различались. Фора на высоком уровне игры даёт огромное преимущество.

Для решения задачи игры в го исследователи DeepMind экспериментировали с разными идеями. Они спроектировали пять нейронных сетей. У каждой из них была своя архитектура. Некоторые модели настраивал алгоритм обучения с учителем. В этом случае обучающим набором данных были позиции из турнирных партий и их оценка. Другие модели обучались с подкреплением. Каждая из них играла сама с собой и запоминала удачные ходы.

Разработчики объединили все пять нейронных сетей в единую систему AlphaGo. Она работала как ансамбль моделей. При этом поиск по дереву игры выполнялся методом Монте-Карло. Мы рассмотрим этот метод далее.

I> [**Ансамбль моделей**](https://wiki.loginom.ru/articles/ensemble-models.html) (ensemble of models) — это объединение нескольких отдельных моделей для повышения общей производительности и точности прогнозов. Ансамбль может исправлять ошибки отдельных моделей и компенсировать проблему переобучения (overfitting).

AlphaGo превзошла все разработанные ранее программы для игры в го. Лучшая из них CrazyStone имела уровень Эло порядка 2000. Когда AlphaGo начала участвовать в официальных матчах, оказалось что она играет на уровне 3000 Эло. Разница в 1000 Эло — это совершенно другой уровень. Для примера, 1000 Эло разделяют любителя и гроссмейстера в шахматах.

Вот список громких побед AlphaGo, которые вызвали сенсацию и ажиотаж вокруг неё:

* В октябре 2015 года AlphaGo обыграла чемпиона Европы [Фань Хуэя](https://ru.wikipedia.org/wiki/Фань_Хуэй) со счётом 5-0.

* В марте 2016 года AlphaGo победила одного из лучших игроков в мире [Ли Седоля](https://ru.wikipedia.org/wiki/Ли_Седоль) из Южной Кореи со счётом 4-1.

* В декабре 2016 года AlphaGo одержала 60 побед в онлайн играх с профессионалами и лучшими игроками го.

* В мае 2017 AlphaGo обыграла лучшего игрока в го [Кэ Цзе](https://ru.wikipedia.org/wiki/Кэ_Цзе) из Китая со счётом 3-0.

После победы над Кэ Цзе [Китайская Ассоциация Вэйци](https://ru.wikipedia.org/wiki/Китайская_ассоциация_вэйци) официально присвоила AlphaGo самый высокий профессиональный дан — 9.

#### 3.6.1.2 AlphaGo Zero

Проект AlphaGo продемонстрировал, что комбинация deep learning моделей и поиска по дереву методом Монте-Карло может решать сложные зачади. Этот подход начали перенимать исследовательские и коммерческие проекты.

Несмотря на выдающиеся результаты, у системы AlphaGo были серьёзные недостатки:

1. Чрезмерно сложная архитектура. Все нейронные сети AlphaGo обучались отдельно. Для каждой применялся свой алгоритм обучения. Некоторые из алгоритмов требовали на вход большие обучающие наборы данных. Из-за этого изменять и улучшать систему было слишком трудоёмко.

2. Некоторые алгоритмы обучения требовали ручной [**настройки гиперпараметров**](https://ru.wikipedia.org/wiki/Оптимизация_гиперпараметров) (hyperparameter optimization). **Гиперпараметр** — некоторое значение, которое управляет процессом обучения. Настройка была трудоёмкой и усложняла изменение системы.

3. Система требовала много вычислительных ресурсов для обучения и работы. Это было следствием её высокой сложности.

Команда DeepMind прекратила развивать систему AlphaGo. Работая над ней, исследователи получили хороший опыт. Они поняли, какие технические решения работают, а с какими возникают проблемы.

Разработчики DeepMind учли свои ошибки и начали новый проект. Он получил название [AlphaGo Zero](https://en.wikipedia.org/wiki/AlphaGo_Zero). Компания DeepMind впервые объявила о нём 19 октября 2017 года в статье журнала Nature. Цели проекта были следующими:

1. Создать полностью самообучающуюся систему. Её алгоритм обучения должен был работать без каких-либо данных по партиям в го между людьми.

2. Упростить процесс обучения и изменения системы. Для этого исследователи заменили пять нейронных сетей AlphaGo на одну.

3. Повысить эффективность использования вычислительных ресурсов.

В отличие от предшественника, система AlphaGo Zero училась играть в го только на своих собственных играх. Слово "Zero" (ноль) в её названии означает, что при разработке не использовались знания экспертов в предметной области. То есть система не изучала позиции по партиям профессиональных игроков.

Через три дня самостоятельного обучения AlphaGo Zero достигла уровня игры одной из первых версий AlphaGo. Через 21 день — AlphaGo Zero поднялась до уровня версии AlphaGo, которая обыграла 60 профессионалов онлайн. Через 40 дней — AlphaGo Zero превзошла все предыдущие версии AlphaGo.

Успехи AlphaGo Zero объяснялись несколькими причинами:

1. Простая и эффективная архитектура.

2. Эффективное использование вычислительных ресурсов.

3. Преимущество обучения с подкреплением по сравнению с обучением с учителем.

Последний пункт вызывал сомнения у команды DeepMind. Чтобы его проверить, исследователи разработали тестовую модель. Её архитектура была очень похожа на AlphaGo Zero. Главное отличие заключалось в алгоритме обучения. Тестовая модель обучалась с учителем на примерах позиций из профессиональных игр.

Для чистоты эксперимента алгоритм обучения запустили с нуля для обеих моделей. Результаты сравнения их производительности оказались неожиданными.

В первом эксперименте системы играли против друг друга на каждой стадии обучения. В самом начале обучения AlphaGo Zero стабильно проигрывала тестовой модели. Но после 20 часов обучения игра AlphaGo Zero стала сильнее. Через 40 часов уровень игры тестовой модели достиг примерно 3500 пунктов Эло и больше не рос. В отличие от неё, AlphaGo Zero продолжила совершенствоваться и превысила уровень 4000 пунктов Эло.

Во втором эксперименте обе системы оценивали заданную позицию на доске. По ней они предсказывали исход партии. Позиции для оценки исследователи брали из базы данных профессиональных игр. Сначала тестовая модель давала более точные предсказания. Но через 15 часов обучения прогнозы AlphaGo Zero стали лучшие.

Третий эксперимент проводили, когда обе системы полностью закончили своё обучение. Им снова показывали позицию на доске. Теперь модели должны были предсказать, какой ход в такой ситуации сделает профессиональный игрок. Снова позиции были из базы данных профессиональных партий. Тестовая модель давала более точные предсказания, чем AlphaGo Zero.

Из сравнения двух систем исследователи сделали вывод, что обучение с подкреплением даёт лучшие результаты для игры в го. При этом его результат сильно зависит от времени обучения. На короткой дистанции обучение с учителем эффективнее. Но оно имеет определённый предел, достигнув который система перестаёт улучшаться.

Также команда DeepMind пришла к выводу, что сила игры AlphaGo Zero превзошла уровень профессионалов. Это показал третий эксперимент. Во время обучения модель обнаружила закономерности в игре го, которые люди ещё не понимают. Когда системе показывали позиции из профессиональных партий, она выбирала самый сильный ход с точки зрения своего сверхчеловеческого уровня игры. Реальные ходы профессионалов в этой позиции оказывались слабее. Поэтому система их не предлагала.

Проект AlphaGo Zero показал, что машинное обучение может решать задачи, для которых у людей ещё нет экспертных знаний. Подобная модель способна обучаться с нуля самостоятельно и находить сложные зависимости в проблемной среде. После обучения её навыки могут значительно превосходить способности специалистов в этой области.

После успеха AlphaGo Zero компания DeepMind запустила несколько проектов в [области биохимии](https://ru.wikipedia.org/wiki/AlphaFold). Это стало проверкой, насколько перспективны подобные системы для решения научных задач.

#### 3.6.1.3 AlphaZero

Следующий проект DeepMind для настольных игр получил название AlphaZero. Целью проекта была проверка, насколько универсален подход, реализованный в системе AlphaGo Zero.

Названия проектов DeepMind для настольных игр похожи. В них легко запутаться. Их будет легче различать, если рассматривать проекты как три стадии одного исследования:

1. AlphaGo — это экспериментальная платформа с набором разных моделей. Она помогла исследователям доказать, что методы глубокого обучения могут в принципе решить задачу игры в го.

2. AlphaGo Zero — из экспериментальной платформы удалили все неэффективные модели. В ней осталась только одна перспективная нейронная сеть, которая обучается алгоритмом с подкреплением. Слово Zero в названии означает, что нет обучения с учителем. В этом проекте разработчики убедились, что нашли подходящую архитектуру.

3. AlphaZero — модель для проверки возможностей найденной архитектуры. Из названия ушло слово Go — это означает, что система универсальна. Она успешно решила задачи, похожие на игру в го: шахматы и сёги.

Работая над проектом AlphaZero, команда DeepMind подготовила три экземпляра одной модели. Их параметры были одинаковыми: архитектура нейронной сети, алгоритм поиска, алгоритм обучения и его гиперпараметры. Каждый экземпляр учился только одной из трёх игр: го, шахматы и сёги. Если модель училась го, то она могла играть только в го. Полученные ей знания не подходили для игры в шахматы и сёги. То же самое справедливо для других двух моделей.

7 декабря 2017 года команда DeepMind опубликовала [статью](https://arxiv.org/abs/1712.01815) в журнале Science. Статья объясняет принцип работы новой модели и приводит её результаты. После 24 часов обучения каждый экземпляр AlphaZero превзошёл уровень профессиональных игроков. Также модели смогли обыграть сильнейшие на 2017 год программы:

* Модель для шахмат обыграла Stockfish 8.

* Модель для сёги обыграла движок [Elmo](https://en.wikipedia.org/wiki/Elmo_(shogi_engine)) в сёги. Сила движка превосходила 2500 пунктов Эло.

* Модель для го обыграла AlphaGo Zero.

Нейронная сеть AlphaZero и алгоритм её обучения были немного изменены по сравнению с AlphaGo Zero. Это было необходимо, чтобы модель стала более универсальной и могла научиться игре в сёги и шахматы. Основные отличия были следующими.

Во-первых, нейронная сеть AlphaGo Zero оценивает вероятность результата партии, предполагая только два возможных исхода в го: победа или поражение. В шахматах и сёги партия может закончиться вничью. Сеть AlphaZero учитывала этот результат.

Во-вторых, у доски для игры в го есть 4 плоскости симметрии, а все фигуры равнозначны. Поэтому для каждой позиции есть ещё 7 симметричных позиций. Они создаются разворотом доски вокруг каждой плоскости симметрии. Алгоритм обучения AlphaGo Zero случайным образом заменял реальную позицию на одну из симметричных. Это повышало качество обучения. В шахматах и сёги симметрии позиций нет. Поэтому алгоритм обучения AlphaZero не использовал подобную технику.

В-третьих, алгоритм обучения AlphaGo Zero использовал разные экземпляры модели. Они различались матрицами весов нейронной сети. На текущей итерации алгоритма один экземпляр играл сам с собой. По результатам игр алгоритм обучения настраивал веса его сети. Затем получившаяся модель играла с самым сильным экземпляром из прошлой итерации. Если новая модель одерживала в среднем 55% побед, она становилась самым сильным экземпляром и играла сама с собой на следующей итерации. Алгоритм обучения AlphaZero всегда работал только с одним экземпляром модели. Он играл сам с собой, и по результатам этих игр алгоритм настраивал веса его сети.

В-четвёртых, алгоритм обучения AlphaGo Zero в процессе работы подбирал оптимальные гиперпараметры обучения. В случае AlphaZero гиперпараметры не менялись на протяжении всего процесса обучения.

Система AlphaZero доказала универсальность подхода, найденного командой DeepMind. Она научилась игре в разные настольные игры на высоком уровне. Сам процесс обучение проходил быстрее и эффективнее, чем для предыдущих систем AlphaGo. При этом сила игры AlphaZero оказалась выше.

Теперь поговорим о внутреннем устройстве системы AlphaZero и принципах, на которых она строится.

### 3.6.2 Поиск по дереву методом Монте-Карло

Система AlphaZero использует поиск по дереву методом Монте-Карло. Рассмотрим, как он работает.

[**Методом Монте-Карло**](https://en.wikipedia.org/wiki/Monte_Carlo_method) называется широкий класс вычислительных алгоритмов. Для получения численных результатов в них применяется повторяющаяся случайная выборка и статистический анализ.

Физик Энрико Ферми был первым, кто экспериментировал с ранним вариантом метода Монте-Карло. С его помощью учёный изучал процессы ядерной физики в 1930-е годы. Свою работу по этой теме он не опубликовал.

Современную версию метода Монте-Карло разработал польский математик [Станислав Улам](https://ru.wikipedia.org/wiki/Улам,_Станислав) в конце 1940-х годов. Вместе с Энрико Ферми он участвовал в Манхэттенском проекте по разработке ядерного оружия. В таком оружии ключевую роль играю нейтроны: они поддерживают и распространяют цепную ядерную реакцию. **Диффузией нейтронов** называется прохождение частиц через какую-то среду. На тот момент существовала модель диффузии нейтронов, построенная на детерминированных математических методах. Она работала плохо и её предсказания были неточными. Новая версия метода Монте-Карло позволила учёным создать более точную модель.

Метод Монте-Карло хорошо объясняет рассказа Станислава Улама. Учёный пришел к идее метода случайно. В 1946 году он выздоравливал после болезни и, чтобы скоротать время, раскладывал пасьянс. Станислав Улам задался вопросом: какова вероятность того, что пасьянс из 52 карт сложится успешно? Для решения задачи учёный попробовал применить комбинаторные методы, но это оказалось слишком трудоёмко. Тогда он решил, что практичнее будет разложить пасьянс, например, сто раз и подсчитать долю успешных игр. Именно в этом заключается суть метода Монте-Карло: повторить эксперимент многократно и оценить статистику полученных результатов.

Метод Монте-Карло нашел применение в поиске по дереву. В 1987 году Брюс Абрамсон из Колумбийского университета Нью-Йорка защищал докторскую диссертацию. В ней он предложил заменить функцию оценки в минимаксном поиске на специальную модель. Она предсказывала ожидаемый результат игры, исходя из текущего состояния. Для этого модель начинала с некоторого хода, допустимого в заданной позиции, и проигрывала партию до конца. Во время такой игры наперёд каждая сторона делала случайные ходы, разрешённые правилами.

Брюс Абрамсон назвал свой подход **моделью ожидаемого результата** (expected-outcome model). Он применил её для шахмат и крестиков-ноликов. Учёный утверждал, что в обоих случаях модель дала точные и легко вычислимые результаты. Это означает, что подход универсален и не зависит от предметной области. Предложенная Брюсом Абрамсоном модель стала прототипом [**поиска по дереву методом Монте-Карло**](https://habr.com/ru/articles/282522/) (Monte Carlo tree search или MCTS).

Может возникнуть вопрос: как статистический поиск по дереву может быть так же точен, как строго детерминированный минимаксный поиск? Дело в том, что нельзя недооценивать статистические данные. Рассмотрим пример.

Когда профессиональный шахматист готовит дебюты, он в равной степени полагается на собственный анализ и статистику. В интернете доступны сайты с перечнем дебютов, которые часто играются на турнирах. Например, сайт [365chess](https://www.365chess.com/opening.php) приводит следующую статистику:

1. Первый ход белых e4 был сыгран в официальных партиях 1833019 раз. Он привел к следующим результатам:

* 38% партий закончились победой белых
* 30,3% партий — ничья
* 31,6% партий — победа чёрных.

2. Первый ход белых f3 был сыгран в официальных партиях 90 раз. Он привел к следующим результатам:

* 31,1% партий — победа белых
* 25,6% партий — ничья
* 43,3% партий — победа чёрных.

Теперь представьте, что вы играете турнирную партию и должны сделать первый ход за белых. Какой из двух ходов e4 или f3 вы предпочтёте? Профессиональные игроки предпочитают вариант e4, а не f3. Он даёт белым больше шансов на победу: 38% против 31,1%. Поэтому из двух предложенных вариантов вам тоже лучше выбрать именно его.

Поговорим о применении поиска методом Монте-Карло для дерева игры шахмат. Он перебирает возможные ходы в текущей позиции на доске. Для каждого из них метод предсказывает исход партии. Но есть одна проблема. Готовой базы данных с результатами партий для каждого рассматриваемого хода не существует. Эту проблему решает высокая вычислительная мощность современных компьютеров. Благодаря ей, программа может составить нечто похожее на такую базу данных.

Пример поможет лучше понять, как именно поиск методом Монте-Карло предсказывает исходы партий. Представьте, что вы играете со своим другом в шахматы. Он допускает ошибку и зевает ладью. На своём ходу вы можете её взять. Упростим пример и допустим, что перед вами только два возможных варианта: брать или не брать ладью.

Допустим, что вы начинающий игрок и знаете только правила шахмат. Ценность фигур вам неизвестна. Чтобы сделать правильный выбор, вам нужна статистика. Точнее вас интересует вопрос: какой процент партий заканчивается победой, если взять ладью? Готовой статистики по партиям у вас нет, поэтому сначала нужно её собрать. Для этого вы берёте другую шахматную доску и расставляете на ней позицию после взятия ладьи. Далее из этой позиции вы сами с собой играете партию до конца 1000 раз. Чтобы сэкономить время и силы, вы не задумываетесь над ходами. Каждая сторона делает случайные ходы, допустимые правилами игры. Это даст вам процент побед для варианта взять ладью.

Далее вы повторяете эксперимент для позиции без взятия ладьи. Вы точно так же проигрываете 1000 партий, выполняя случайные ходы за каждую сторону. Теперь достаточно сравнить, какой из двух вариантов приводит к потенциально большему числу побед. Допустим, что при взятии ладьи победой закончились 754 из 1000 партий. Если не брать ладью, победой закончились только 501 из 1000 партий. Из этой статистики можно сделать вывод, что взятие ладьи увеличивает ваш шанс на победу примерно на 25%. Следовательно, этот ход однозначно лучше, чем альтернатива.

Теперь вы знаете, какой из двух ходов статистически лучше. Вы возвращаетесь к доске, за которой вас ждёт друг, и забираете ладью. Именно так поиск по дереву методом Монте-Карло выбирает лучший ход.

#### 3.6.2.1 Алгоритм MCTS

Рассмотрим алгоритм поиска методом Монте-Карло. Он состоит из четырёх шагов, которые повторяются циклически:

1. **Selection** (выбор)

2. **Expansion** (расширение)

3. **Simulation** (симуляция)

4. **Backpropagation** (обратное распространение)

Рассмотрим иллюстрацию 3-35. Перед вами дерево игры в шахматы, которое построил алгоритм поиска после нескольких итераций. Допустим, что алгоритм исполняет некоторая программа. Зелёные узлы — это позиции фигур, в которых ходит программа. Красные узлы — позиции, в которых ходит оппонент.

Оппонент сделал ход, который привёл к текущей позиции на доске A. Теперь программа рассматривает свои варианты. Для простоты допустим, что у неё есть только три возможных хода. Они приводят к трём новым позициям: B, C и D. Вопрос — какой ход выбрать программе?

Программа применяет алгоритм MCTS. На первом шаге selection она выбирает, какой из узлов дерева исследовать. У этого узла должен быть минимум один дочерний узел, для которого ещё не выполнялся шаг simulation (симуляция). Допустим, что B, C и D удовлетворяют этому условию.

Чтобы выбрать узел для исследования, программе нужна некоторая стратегия. Она должна отличать перспективные ходы от остальных. Допустим, что стратегия есть. Она рассмотрела всю имеющуюся информацию и выбрала ход, который приводит к узлу B. Он считается самым перспективным на данный момент. На иллюстрации 3-35 этот ход отмечен стрелкой.

{caption: "Иллюстрация 3-35. Первый шаг selection в поиске по дереву методом Монте-Карло", height: "30%"}
![Шаг selection в MCTS](images/Chess/monte-carlo-search-selection.png)

Второй шаг алгоритма expansion (расширение) демонстрирует иллюстрация 3-36. Программа выбрала ход, ведущий к узлу B. Теперь в узле B ходит оппонент. Один из его возможных ходов ведёт в узел E. Его программа уже рассмотрела на прошлой итерации поиска. Другой ход оппонента ведёт в узел F. На иллюстрации этот ход отмечен стрелкой. Программа добавляет узел F в дерево игры.

Чтобы выбрать потенциальные ответы оппонента на шаге expansion, программе снова нужна некоторая стратегия. Это также может быть случайный узел, который ещё не был исследован.

{caption: "Иллюстрация 3-36. Второй шаг expansion в поиске по дереву методом Монте-Карло", height: "30%"}
![Шаг expansion в MCTS](images/Chess/monte-carlo-search-expansion.png)

Третий шаг алгоритма simulation (симуляция) демонстрирует иллюстрация 3-37. На этом шаге программа оценивает новый узел дерева F. Для этого она запускает вычисления, которые собирают статистическую информацию о новом узле. Суть вычислений в том, чтобы сыграть определённое количество партий, начиная с позиции в проверяемом узле. Ходы в такой игре выбираются случайным образом, пока партия не завершится. Одна такая сыгранная партия называется **playout** (воспроизведение).

На иллюстрации 3-37 мы видим, что программа выполнила один playout. Он обозначен пунктирной стрелкой. В результате симуляции программа проиграла.

{caption: "Иллюстрация 3-37. Третий шаг simulation в поиске по дереву методом Монте-Карло", height: "50%"}
![Шаг simulation в MCTS](images/Chess/monte-carlo-search-simulation.png)

Четвёртый шаг алгоритма backpropagation (обратное распространение) демонстрирует иллюстрация 3-38. На этом шаге программа передаёт статистическую информацию, полученную в результате simulation, для узла F вверх по дереву игры. Таким образом обновляются состояния всех родительских узлов: B и A. Стрелки обозначают направление передачи информации.

{caption: "Иллюстрация 3-38. Четвёртый шаг backpropagation в поиске по дереву методом Монте-Карло", height: "30%"}
![Шаг backpropagation в MCTS](images/Chess/monte-carlo-search-backpropagation.png)

Ещё раз кратко повторим шаги алгоритма поиска:

1. Программа начинает с текущей позиции на доске в узле A. Она применяет стратегию для выбора перспективного хода. Стратегия указывает на ход, ведущий к узлу B. Поэтому на первом шаге selection программа выбирает этот ход.

2. Для узла B программа должна выбрать, какой из возможных следующих ходов анализировать. Для этого она снова применяет некоторую стратегию. Стратегия указывает на ход, ведущий к позиции в узле F. Этот ход ещё не рассматривался, поэтому программа выполняет шаг expansion и добавляет в дерево новый узел F.

3. Программа должна собрать статистику, чтобы оценить новый узел F. Для этого она выполняет симуляцию (simulation). Количество игр в ней зависит от параметров алгоритма, которые задаёт разработчик.

4. Программа распространяет результат симуляции вверх по дереву игры. Так она обновляет состояния всех родительских узлов. Это шаг backpropagation.

Алгоритм MCTS работает до выполнения условия остановки. Оно зависит от решаемой задачи. В шахматах алгоритм останавливается при достижении определённой глубины дерева игры. Также он досрочно прекращается, если у программы закончилось время, отведённое на один ход.

Теперь поговорим о состояниях узлов дерева игры. Для каждого узла алгоритм хранит следующую информацию:

1. Число побед игрока, делающего ход в этом узле. Это сумма его побед во всех playout, выполненных для всех дочерних узлов.

2. Общее число playout, выполненных для всех дочерних узлов.

Вернёмся к иллюстрации 3-37. Она показывает, как выглядит дерево до симуляции в узле F. Рассмотрим узел B. Состояние узла равно 1/3. Второе число 3 означает, сколько playout было выполнено для всех дочерних узлов. Первое число 1 означает, сколько раз в этих playout выиграл оппонент. Именно он делает ход в узле B.

Для узла B было выполнено три playout. В одном из них выиграл оппонент программы. Посмотрите внимательно на дочерние узлы E и F. Очевидно, что все playout выполнялись для узла E, потому что для узла F ещё не было ни одной симуляции. Состояние узла E равно 2/3. Это означает, что в двух из трёх playout для него выиграла программа. В сумме число побед в узлах B и E равно трём, то есть числу playout. Это равенство выполняется всегда.

Иллюстрация 3-38 показывает, как выглядит дерево игры после симуляции в узле F. Новое состояние этого узла равно 0/1. Оно означает, что в симуляции был один playout, который завершился поражением программы. С учётом этой информации алгоритм обновляет состояние узла B до 2/4. Это означает, что в двух его дочерних узлах E и F в сумме было выполнено четыре playout. В двух из них победил оппонент, а в двух — программа.

После обновления состояния узла B, программа переходит к узлу A. Это текущая позиция на доске. На иллюстрации 3-37 состояние A равно 9/12. Программа обновляет это значение до 9/13. Это означает, что для всех дочерних узлов A выполнилось 13 playout. В 9 из них выиграла программа.

Допустим, что у программы закончилось время на поиск. Теперь она должна выбрать свой ход в узле A, опираясь на текущую информацию. Вот что говорит собранная на данный момент статистика:

* Ход в узел B приводит к выигрышу программы в 50% случаев.

* Ход в узел C приводит к выигрышу программы в 100% случаев. Ни один из playout, запущенных из этого узла не привёл к победе оппонента.

* Ход в узел D приводит к выигрышу программы в 66% случаев. Из шести playout, запущенных в его дочерних узлах, только два привели к победе оппонента.

Из этого следует, что ход в узел C лучший для программы. Он с большей вероятностью приведёт к победе в партии.

Для простоты нашего примера мы проигнорировали случаи, когда playout завершается вничью. Если это учесть, возможные результаты playout будут такими:

* 1 — победа
* 0.5 — ничья
* 0 — поражение.

С учётом ничьи состояния узлов на шаге backpropagation рассчитываются точно так же, как для целочисленных результатов. Рассмотрим пример.

Иллюстрация 3-39 демонстрирует состояние дерева игры, когда симуляция одного playout в узле F закончилась ничьей. Этот узел получил состояние 0.5/1.

{caption: "Иллюстрация 3-39. Результат backpropagation для дробного результата simulation", height: "30%"}
![Backpropagation для дробного результата simulation](images/Chess/monte-carlo-search-backpropagation-fraction.png)

Теперь программа выполняет шаг backpropagation. Состояние родительского узла B считается по следующему алгоритму:

1. Посчитать число playout для всех дочерних узлов B. Это узлы E и F. Сумма равна: `3 + 1 = 4`. Записать 4 как второе число в состоянии узла B.

2. Посчитать, сколько playout закончились победами игрока, делающего ход в узлах E и F. Сумма побед равна: `2 + 0.5 = 2.5`.

3. Посчитать число побед у игрока, делающего ход в узле B. Для этого вычесть из общего числа playout в узле B число побед игрока, делающего ход в дочерних узлах E и F. Получим `4 - 2.5 = 1.5`. Запишем 1.5 как первое число в состоянии узла B.

Так мы получили новое состояние узла B 1.5/4.

Состояние корневого узла A считается по тому же алгоритму:

1. Найти число playout в его дочерних узлах B, C и D: `4 + 3 + 6 = 13`.

2. Найти число побед игрока, делающего ход в этих узлах: `1.5 + 0 + 2 = 3.5`.

3. Вычесть из числа playout число побед в узлах B, C и D: `13 - 3.5 = 9.5`.

Так мы получили новое состояние узла A 9.5/13.

Мы подробно рассмотрели все шаги алгоритма поиска по дереву методом Монте-Карло.

#### 3.6.2.2 Формула UCT

На первом шаге selection алгоритм MCTS использует стратегию для выбора хода. Она называется политикой дерева. **Политика дерева** (tree policy) — это набор правил для навигации по дереву от его корня до листьев. Хорошая политика гарантирует обход дерева в таком порядке, который с наибольшей вероятностью приведёт к оптимальному решению.

Стратегии могут быть разными. Некоторые зависят от условий задачи и особенностей проблемной среды. Есть универсальные стратегии, которые рассматривают только структуру самого дерева. Они используются чаще, потому что не зависят от предметной области, как и сам алгоритм MCTS.

Рассмотрим универсальные политики дерева. Есть две базовые стратегии выбора узлов, которые учитывают только структуру дерева:

1. **Exploration** (исследование) — выбирать узлы, которые ещё ни разу не посещались или были исследованы на небольшую глубину. Это позволит обнаружить ходы, которые более перспективны чем уже известные.

2. **Exploitation** (эксплуатация) — глубже исследовать уже известные узлы, которые на данный момент выглядят самыми перспективными. Это позволит выбрать из известных ходов наилучший.

Следовать только одной из этих стратегий не оптимально. Чистое исследование (exploration) не даст глубоко просчитать перспективные ходы. Если поиск ведётся по дереву игры в шахматы, то у лучшего найденного хода может оказаться опровержение. Алгоритм поиска его не найдёт. В итоге он выберет посредственный или плохой ход.

I> **Опровержение** в шахматах — это ход или серия ходов, доказывающих несостоятельность плана действий.

Чистая эксплуатация (exploitation) оставляет вне поля зрения лучшие ходы. Руководствуясь только ей, алгоритм поиска обнаружит несколько посредственных ходов. После этого он просчитает каждый из них вглубь. В результате алгоритм выберет из первых попавшихся ходов наилучший.

Мы уже обсуждали компромисс между стратегиями исследования и эксплуатации в разделе 2.6.2.3. В нём речь шла о проблеме многорукого бандита. В случае поиска по дереву методом Монте-Карло задача та же самая. Выражаясь в терминах теории игр, надо найти оптимальную смешанную стратегию.

В 2006 году венгерские учёные в области информатики [Левенте Кочиш](https://www.chessprogramming.org/Levente_Kocsis) и [Чаба Сепешвари](https://www.chessprogramming.org/Csaba_Szepesvári) предложили формулу под названием [**Upper Confidence bounds applied to Trees**](https://www.chessprogramming.org/UCT) или UCT (верхние границы уверенности, применяемые к деревьям).

Идея формулы в следующем. Предположим, что алгоритм поиска находится в некотором узле дерева i. Формула UCT вычисляет оценку для его каждого дочернего узла j:
{width: "50%"}
![](images/Chess/uct-formula.png)

Обозначения в формуле:

* X~j~ — коэффициент выигрыша для дочернего узла j. Рассчитывается как w~j~/n~j~. То есть отношение числа побед w~j~ к количеству посещений узла n~j~.

* n~i~ — количество посещений родительского узла i.

* n~j~ — количество посещений дочернего узла j.

* C — константа, задающая соотношение между шириной и глубиной поиска. Чем она больше, тем глубже будет поиск.

Левая часть формулы X~j~ соответствует стратегии эксплуатации. Это значение больше для дочерних узлов с высоким коэффициентом выигрыша. Правая часть формулы соответствует стратегии исследования. Её значение выше для дочерних узлов, у которых мало посещений и для которых выполнено мало playout.

Алгоритм MCTS использует формулу UCT в двух случаях:

1. На шаге selection для текущего узла i алгоритм выбирает дочерний узел j с максимальной оценкой UCT~j~. Это даёт хороший компромисс между стратегиями исследования и эксплуатации.

2. Когда условие завершения алгоритма выполнено, надо выбрать ход в текущей позиции. Если в качестве политики дерева применялась формула UCT, то лучшим будет ход с наибольшим числом посещений.

Второй случай требует пояснения. Снова обратимся к иллюстрации 3-38. Если учитывать только собранную статистику, лучший ход ведёт в узел C. Потому что программа выиграла во всех playout, запущенных из этого узла.

Теперь допустим, что алгоритм MCTS использовал формулу UCT на каждом шаге selection. Формула гарантирует, что узел с наибольшим числом посещений будет самым перспективным с точки зрения смешенной стратегии исследования и эксплуатации. В этом случае лучшим ходом на иллюстрации 3-38 будет ход D. Он посещался чаще всего и для него было выполнено больше playout.

Формула UCT — это не единственная возможная политика дерева. Хорошей альтернативой является нейронная сеть. Она должна отличать перспективные ходы в текущей позиции. Нейронная сеть может направлять алгоритм MCTS не только на шаге selection, но и на шаге expansion. В отличие от сети формула UCT не может помочь на шаге expansion. Причина в том, что коэффициент выигрыша для неисследованного узла неизвестен, а количество его посещений равно нулю.

#### 3.6.2.3 Преимущества и недостатки MCTS

В общих чертах мы познакомились с поиском по дереву методом Монте-Карло. Теперь поговорим о его сильных и слабых сторонах. Нас особенно интересуют возможности этого алгоритма применительно к настольным играм типа шахмат.

Начнём с **преимуществ**. Во-первых, метод Монте-Карло — это универсальный инструмент, который основан на теоремах статистики. Применение метода для поиска по дереву дало настолько же универсальный алгоритм. В этом его главная сильная сторона. Рассмотрим несколько примеров.

Брюс Абрамсон особенно подчёркивал независимость алгоритма от предметной области. Учёный применял его для разных игр с двумя участниками. Позднее алгоритм MCTS успешно применялся для разного рода задач. В 1989 году австрийские учёные Вольфганг Эртель, Иоганн Шуман и Кристиан Саттнер использовали его в системе для автоматического доказательства теорем.

В 1993 году немецкий учёный Бернд Брюгманн из института физики общества Макса Планка впервые применил поиск методом Монте-Карло в программе для игры в го. В своей [статье](http://www.ideanest.com/vegos/MonteCarloGo.pdf) учёный утверждает, что программа играла на начальном любительском уровне 25 кю на уменьшенной доске 9x9 линий. При этом в неё не закладывалось никаких знаний об игре го кроме правил.

Программы для игры в го продолжали развиваться на протяжении 1990-х и 2000-х годов. Самые сильные из них применяли тот или иной вариант поиска методом Монте-Карло. В 2012 году программа Zen впервые выиграла матч у любителя 2-го дана на стандартной доске 19x19 линий. Она не использовала какие-либо модели машинного обучения. В основе программы лежал алгоритм MCTS с политикой дерева и некоторыми эвристиками.

Второе преимущество алгоритма MCTS в том, что он хорошо работает в играх и задачах с очень высоким фактором ветвления. **Фактор ветвления** (branching factor) — это среднее количество вариантов действий у игроков на каждом ходу. В шахматах он равен 35, а в го — 250. Именно из-за высокого фактора ветвления минимаксный поиск с альфа-бета отсечением плохо справляется с деревом игры го. С другой стороны, для игр с низким фактором ветвления поиск минимакс работает быстрее и даёт более точный результат чем MCTS.

В-третьих, алгоритм MCTS не зависит от функции оценки и эвристик, специфичных для предметной области. Функция оценки играет ключевую роль в поиске минимакс. Если она низкого качества, то минимакс вместо лучших ходов выберет посредственные. Алгоритму MCTS важно количество симуляций. Другими словами, качество его результатов определяется доступной вычислительной мощностью и памятью компьютера. Таким образом MCTS применим в играх и задачах, для которых ещё не разработана подходящая теория.

У алгоритма MCTS есть и **недостатки**. Во-первых, он требует значительно большего объёма вычислений, чем минимакс. Ему нужна высокая вычислительная мощность для выполнения многократных симуляций. Также нужно больше памяти, чтобы хранить состояние узлов дерева поиска. Эти требования мешают применять MCTS в условиях ограниченных ресурсов. Также алгоритм плохо справляется в ситуациях, когда решения нужно принимать очень быстро. Его вычисления требуют времени.

Во-вторых, метод Монте-Карло — это инструмент статистики. Изначально он ориентирован на стохастические среды. На состояния таких сред влияют какие-то случайные события. На практике это предположение не всегда верно. Алгоритм MCTS применяют для поиска по дереву игр с нулевой суммой. Такая игра представляет собой конкурентную среду, которая иногда ещё и детерминирована (например, шахматы). В ней есть оппонент, который активно противодействует. Поскольку метод Монте-Карло не рассчитан на такие среды, алгоритму MCTS сложнее находить в них оптимальные действия.

Третий недостаток алгоритма MCTS следует из второго. Алгоритм ошибается при оценке ходов, у которых есть опровержение. Если опровержение состоит из последовательности лучших ходов, то MCTS может его не заметить. В стохастической среде очень маловероятно, что такая последовательность ходов случайно произойдёт. Но в условиях активного противодействия оппонент целенаправленно ищет такие последовательности и исполняет их.

В-четвёртых, алгоритм MCTS чувствителен к параметрам настройки. Качество его результатов сильно зависит от глубины поиска, количества симуляций для каждого узла, соотношения стратегий исследования и эксплуатации. Подобрать оптимальные значения этих параметров для конкретной задачи бывает сложно и трудоёмко.

Алгоритм MCTS может конкурировать с поиском минимакс, но только в специфичных задачах и условиях. Прежде всего ему нужно предоставить вычислительные ресурсы, адекватные решаемой задаче. Высокая скорость принятия решений не должна быть критичным требованием. Тогда результаты алгоритма MCTS будут сопоставимы по качеству с поиском минимакс.

### 3.6.3 Свёрточная нейронная сеть

Система AlphaZero использует одну нейронную сеть. Она имеет несколько свёрточных слоёв (convolution layer). Впервые слои этого типа появились в специальной архитектуре под названием свёрточная нейронная сеть (CNN). Для начала рассмотрим эту архитектуру и типичные задачи, которые она решает.

До начала 1980-х годов для распознавания изображений применялись модели с архитектурой [многослойного перцептрона](https://en.wikipedia.org/wiki/Multilayer_perceptron). Эта архитектура универсальна. Она также способна решать задачи классификации, регрессии и аппроксимации функций.

Многослойный перцептрон может распознавать изображения, но при этом возникает несколько проблем. Главная из них связана с большим количество связей между нейронами. В сети перцептрона все слои линейные и, следовательно, полносвязные. Каждый нейрон следующего слоя связан с каждым нейроном предыдущего. Это приводит к проблеме, известной как проклятие размерности.

Проблема [**проклятия размерности**](https://en.wikipedia.org/wiki/Curse_of_dimensionality#Machine_learning) (curse of dimensionality) применительно к распознаванию изображений заключается в следующем. Если увеличивать разрешение изображения на входе модели, число связей между нейронами сети тоже будет расти. Это ожидаемо. Но характер увеличения числа связей может быть разным. Если происходит взрывной рост, это называют проклятием размерности. Рассмотрим пример.

Предположим, что модель многослойного перцептрона обрабатывает [RGB изображение](https://ru.wikipedia.org/wiki/RGB) с тремя цветовыми каналами: красный, зелёный и синий. Его размер 32x32 пикселя. Тогда во входном слое модели должно быть по три нейрона на каждый пиксель. То есть список входов выглядит примерно так:

* пиксель с координатой (1, 1) канал красного цвета
* пиксель с координатой (1, 1) канал зелёного цвета
* пиксель с координатой (1, 1) канал синего цвета
* пиксель с координатой (2, 1) канал красного цвета
* пиксель с координатой (2, 1) канал зелёного цвета
* пиксель с координатой (2, 1) канал синего цвета
* пиксель с координатой (3, 1) канал красного цвета
* пиксель с координатой (3, 1) канал зелёного цвета
* пиксель с координатой (3, 1) канал синего цвета
* ...
* пиксель с координатой (32, 32) канал синего цвета.

В случае 8-битной кодировки цветов каждый из этих нейронов принимает на вход целое число от 0 до 255. 0 соответствует минимальной интенсивности цвета у данного пикселя, а 255 — максимальной. Всего входных нейронов у модели будет столько: `32 * 32 * 3 = 3072`.

Перейдём к первому скрытому слою. Каждый нейрон этого слоя соединяется с каждым нейроном входного слоя. Другими словами, у каждого нейрона первого слоя есть 3072 соединений. Допустим для примера, что первый скрытый слой состоит из 512 нейронов. Тогда мы получим `3072 * 512 = 1572864` соединений между слоями. У каждого из них есть вес, который надо корректировать в процессе обучения.

Теперь увеличим разрешение входного RGB изображения. Пусть оно станет 200x200 пикселей. В этом случае входной слой вырастет до `200 * 200 * 3 = 120000` нейронов. Число соединений между входным и первым скрытым слоем вырастет до `120000 * 512 = 61440000`. Именно столько весов надо подобрать при обучении модели.

Мы увеличили размер изображения в 6.25 раза. В результате число соединений между слоями выросло примерно в 39 раз. Из-за такого взрывного роста числа соединений многослойный перцептрон не может обрабатывать изображения с высоким разрешением. Это вычислительно неразрешимая задача.

У многослойного перцептрона есть и другие проблемы. Полносвязные слои не учитывают пространственную структуру данных. Что это означает? Некоторые пиксели изображения расположены рядом друг с другом. Другие находятся на разных концах картинки. Перцептрон обрабатывает их одинаково. Такой подход малоэффективен. Причина в том, что изображения состоят из наборов близко расположенных элементов (например, линий). Они складываются в определённые шаблоны (например, контуры деревьев). Эти отношения между соседними элементами очень важны, но перцептрон не способен их учесть.

**Свёрточная нейронная сеть** (CNN) — это специальная архитектура для распознавания изображений. Она решает проблемы многослойного перцептрона.

В основе архитектуры CNN лежит идея применения фильтров (filter) для извлечения признаков (features) изображённого объекта. Это очень похоже на то, как мозг человека распознаёт объекты. Например, чтобы различать лица, мы ориентируемся на следующие признаки:

* контур всего лица
* линии носа, ушей и глаз
* оттенок кожи
* цвет волос и глаз.

Фильтры выделяют из исходного изображения ключевые признаки и отбрасывают несущественные детали. Таким образом, уменьшается объём информации для обработки на следующем этапе.

Рассмотрим на примере, как именно работают фильтры. В качестве исходного изображения возьмём известный фрагмент [фотографии Лены Сёдерберг](https://ru.wikipedia.org/wiki/Лена_(изображение)) (см. иллюстрацию 3-40).

{caption: "Иллюстрация 3-40. Фрагмент фотографии Лены Сёдерберг", height: "30%"}
![Фрагмент фотографии Лены Сёдерберг](images/Chess/lena.png)

Применим к этой фотографии фильтр, который [выделяет границы объектов](https://ru.wikipedia.org/wiki/Выделение_границ). После наложения фильтра мы ожидаем, что линии лица и шляпы станут хорошо различимы. Сам фильтр представляет собой следующую матрицу размера 3x3:
{width: "25%"}
![](images/Chess/sobel-vertical-kernel.png)

Это вертикальный фильтр [**оператора Собеля**](https://ru.wikipedia.org/wiki/Оператор_Собеля). Оператор состоит из двух действий:

* Наложение вертикального фильтра на исходное изображение.

* Наложение горизонтального фильтра на исходное изображение.

Каждый фильтр представляет собой матрицу размера 3x3. Результаты двух наложений комбинируются в одно выходное изображение. Оно называется **картой признаков** (feature map). В общем смысле этот термин означает результат наложения фильтров.

Иллюстрация 3-41 демонстрирует карту признаков после применения вертикального фильтра оператора Собеля.

{caption: "Иллюстрация 3-41. Карта признаков после применения фильтра", height: "30%"}
![Карта признаков после применения фильтра](images/Chess/lena-edge-detection-filter.png)

Карта признаков не очень чёткая. Ей не хватает результата наложения горизонтального фильтра. Тем не менее линии границ некоторых объектов стали отчётливыми. Они хорошо выделяются на чёрном фоне.

Python скрипт для применения вертикального фильтра Собеля описан в приложении 5.2.1 и доступен на [Github](https://github.com/ellysh/ai-in-strategy-games/blob/master/manuscript/resources/code/Chess/sobel-vertical-filter.py).

Более сложный пример использования оператора Собеля описан в следующей [статье](https://www.adeveloperdiary.com/data-science/computer-vision/how-to-implement-sobel-edge-detection-using-python-from-scratch/). В ней автор применяет горизонтальный и вертикальный фильтры. Затем он комбинирует их результаты. Исходный код этого примера доступен на [Github](https://github.com/adeveloperdiary/blog/tree/master/Computer_Vision/Sobel_Edge_Detection).

Типичная CNN сеть последовательно выполняет следующие операции над входными данными:

1. **Свёртка**

2. **Пакетная нормализация** (batch normalization)

3. Применение **функции активации** (activation function)

4. **Подвыборка** (pooling)

5. **Сглаживание** (flattening)

Некоторые из этих операций могут повторяться несколько раз.

Иллюстрация 3-42 демонстрирует архитектуру типичной CNN сети для распознавания изображений. Подобные сети после обучения решают одну из следующих задач:

1. **Классификация изображений**. Например, определить вид животного на картинке: собака или кошка.

2. **Обнаружение объектов**. Например, найти на изображении лица людей.

3. **Сегментация изображений** — разделение изображения на значимые фрагменты.

4. **Перенос стиля** — стиль одного изображения применяется к содержимому другого.

5. **Анализ лиц** — идентификация людей и анализ выражений лица.

{caption: "Иллюстрация 3-42. Архитектура типичной CNN сети", height: "60%"}
![Архитектура CNN сети](images/Chess/typical-cnn-architecture.png)

Рассмотрим в общих чертах, как работает CNN сеть. Допустим, что она решает задачу классификации. Входные данные (Input) передаются на первый слой свёртки (Convolution). На иллюстрации этот слой изображает прямоугольник красного цвета. Выходные данные этого слоя передаются на следующий за ним слой подвыборки (Pooling). Затем пара слоёв свёртки и подвыборки повторяется. Обратите внимание, что параметры слоёв этой пары отличаются от предыдущей.

Все слои свёртки и подвыборки выполняют одну задачу — [**извлекают признаки**](https://ru.wikipedia.org/wiki/Выделение_признаков) (feature extraction) из исходного изображения. На иллюстрации 3-42 эти слои объединены в блок розового цвета для наглядности.

После извлечения признаков модель решает задачу классификации. **Классификация** (classification) в данном случае означает назначение меток на полученные карты признаков. За эту операцию отвечают слои, объединённые в зелёный блок. Первый из них — слой сглаживания (Flatten). Он получает данные из последнего слоя подвыборки. Свой результат он передаёт в соединённые последовательно линейные полносвязные слои (Dense). Они и выполняют классификацию. Результат последнего полносвязного слоя передаётся на выход модели (Output).

#### 3.6.3.1 Операция свёртки

Мы научились применять фильтр к изображению и извлекать признаки. Эта операция называется [**свёрткой**](https://proglib.io/p/convolution) (convolution). Именно на ней строится вся архитектура CNN.

Рассмотрим на примере, как именно работает свёртка. Допустим, что мы обрабатываем чёрно-белое изображение размером 5x5 пикселей. У него есть один канал серого цвета. Поэтому каждому пикселю изображения соответствует значение от 0 (белый цвет) до 255 (чёрный цвет).

Компьютер представляет изображение в виде двумерного массива пикселей размером 5x5. Пусть массив выглядит следующим образом:
{width: "40%"}
![](images/Chess/convolution-source-image-5x5.png)

Перед тем как передать изображение на вход CNN сети, его нормализуют. **Нормализация** (normalization) — это преобразование входных данных таким образом, чтобы они имели одинаковые динамические диапазоны. Для изображений это означает деление пикселей на 255. Таким образом они приводятся к диапазону от 0 до 1. Для простоты примера мы не будем выполнять нормализацию.

Применим к нашему изображению тот же фильтр, который мы использовали для обработки иллюстрации 3-40. Представим фильтр в виде следующего двумерного массива размером 3x3:
{width: "25%"}
![](images/Chess/convolution-kernel-3x3.png)

Эта матрица содержит веса признаков и называется **ядром** (convolution kernel). Она проходит по матрице исходного изображения с заданным **шагом** (convolution step), начиная с левого верхнего угла. На каждом шаге выполняется поэлементное умножение: число из ячейки исходного изображения умножается на число из перекрывшей её ячейки ядра. Обратите внимание, что это не [**умножение матриц**](https://ru.wikipedia.org/wiki/Умножение_матриц), а [**произведение Адамара**](https://ru.wikipedia.org/wiki/Произведение_Адамара). В литературе обе операции могут обозначаться одинаково: *, &#0120; или &#0215;. Результаты перемножения ячеек суммируются в один элемент выходного массива.

Поэлементное умножение происходит следующим образом. На первом шаге ядро перекрывает ячейки исходного изображения, выделенные зелёным цветом:
{width: "40%"}
![](images/Chess/convolution-source-image-5x5-highlight-first.png)

Перемножаем перекрытые ячейки изображения и ядра, складываем результаты и получаем следующее:
{line-numbers: false, format: text}
```
19 *  1 + 10 *  2 + 39  *  1 +
30 *  0 + 68 *  0 + 238 *  0 +
45 * -1 + 6  * -2 + 59  * -1 = -38
```

Мы получили значение -38 для первого элемента в первом ряду карты признаков. Обратите внимание, что карта признаков — это не изображение, а его элементы — не цвета пикселей. В примере с фотографией Лены Сёдерберг мы для наглядности визуализировали карту признаков, полученную после свёртки. Для такой визуализации необходимо сопоставить значения из карты признаков с некоторыми цветами. Есть разные методы такого сопоставления. Например, функция [`imshow`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html) из библиотеки Python Matplotlib предлагает целый ряд [**цветовых карт**](https://matplotlib.org/stable/users/explain/colors/colormaps.html) (colormap).

Допустим, что мы выбрали шаг, равный единице. Тогда ядро смещается на один столбец вправо. Оно перекроет следующие ячейки исходного изображения:
{width: "40%"}
![](images/Chess/convolution-source-image-5x5-highlight-second.png)

Снова выполняем произведение Адамара. Затем суммируем получившиеся числа. Получается следующий результат:
{line-numbers: false, format: text}
```
10 *  1 + 39  *  2 + 48  *  1 +
68 *  0 + 238 *  0 + 73  *  0 +
6  * -1 + 59  * -2 + 202 * -1 = -190
```

Второй элемент в первом ряду карты признаков равен -190.

Когда ядро достигает правого края изображения, оно смещается вниз на один ряд. После этого ядро движется снова с левого края изображения направо с шагом в один столбец. На последнем шаге ядро перекроет следующую часть изображения:
{width: "40%"}
![](images/Chess/convolution-source-image-5x5-highlight-last.png)

Когда ядро пройдёт по всему исходному изображению, мы получим следующую карту признаков:
{width: "25%"}
![](images/Chess/convolution-result-image-3x3.png)

Приложение 5.2.2 подробно описывает Python скрипт для расчёта карты признаков нашего примера. Скрипт также доступен на [Github странице проекта](https://github.com/ellysh/ai-in-strategy-games/blob/master/manuscript/resources/code/Chess/apply-filter-manually.py).

Вы заметили, что размер карты признаков уменьшился до 3x3 по сравнению с размером исходного изображения 5x5? Причина в том, что информация о пикселях на его границах потерялась. Если эти пиксели важны, мы можем расширить исходное изображение. Для этого дополним его по краям зануленными пикселями. Эта операция называется **выравниванием** (padding).

После выравнивания наше исходное изображение станет таким:
{width: "50%"}
![](images/Chess/convolution-source-image-aligned-7x7.png)

Синим цветом отмечены зануленные пиксели, которые мы добавили. Теперь если выполнить свёртку для выровненного изображения, на выходе получится карта признаков размером 5x5. Информация о пикселях на границах не будет потеряна.

Конечная и начальная позиции ядра при обработке выровненного изображения выглядят так:
{width: "100%"}
![](images/Chess/convolution-source-image-aligned-7x7-highlight.png)

Операция свёртки решает проблему проклятия размерности, которая характерна для многослойного перцептрона. Как это происходит? В нашем примере исходное изображение имеет размер 5x5 пикселей. Это означает, что входной слой нейронной сети имеет `5 * 5 = 25` входных признаков. Карта признаков на выходе имеет размер 3x3. Следовательно, число выходных признаков равно `3 * 3 = 9`. Если для обработки изображения применить полносвязный слой нейронов, в нём должно быть `25 * 9 = 225` весов. Если же применить операцию свёртки, число весов будет равно размеру ядра 3x3. Это значит, что их только `3 * 3 = 9`. Увеличение размера исходного изображение никак не повлияет на размер ядра.

Для простоты примера мы применили вертикальный фильтр оператора Собеля к входному изображению 5x5. Ядро оператора имеет константные весовые параметры. На практике параметры ядра подбираются в процессе обучения CNN сети.

Другая проблема многослойного перцептрона — работа с пространственной структурой данных. Операция свёртки учитывает эту структуру. Один и тот же набор весовых параметров ядра применяется к разным областям изображения. Таким образом модель находит одни и те же признаки (например, линии), независимо от их расположения на изображении. На следующем скрытом слое сети обнаруженные признаки обобщаются в шаблоны. Далее шаблоны складываются в объекты. Так свёртка естественным образом восстанавливает иерархию абстракций элементов изображения.

Мы рассмотрели свёртку на примере чёрно-белого изображения. У него есть только один канал серого цвета. Если исходное изображение цветное с тремя каналами (RGB), то говорят об операции **свёртки по объёму** (convolutions over volume). В этом случае для каждого канала нужно отдельное ядро. Все три ядра вместе образуют один фильтр. То есть **фильтр** (filter) — это набор ядер, каждое из которых соответствует одному каналу цвета. При этом сами ядра могут между собой различаться.

Допустим, что размер исходного изображения равен `N x N x C`, где `N` — число пикселей, а `C` — число каналов цвета. Тогда для его обработки нужен фильтр размером `F x F x C`, где `F` — число элементов, а `C` — число ядер. После свёртки по объёму с шагом 1 карта признаков на выходе будет иметь размер `(N - F + 1) x (N - F + 1)`. Обратите внимание, что третье измерение для цветовых каналов пропадает.

Интуитивно мы воспринимаем исходное цветное изображение, как двумерное. Но фактически оно является трёхмерным, если учесть каналы цвета. Именно из-за третьего измерения возникает понятие **объёма** (volume). Для его обработки нужны трёхмерные или **3D фильтры**, состоящие из трёх ядер.

В CNN моделях к исходному изображению часто применяют несколько фильтров в одной операции свёртки. Например, мы хотим распознать не только вертикальные границы объектов, но и горизонтальные. В этом случае нам нужны два отдельных фильтра. Если изображение цветное, то у каждого из них будет по три ядра. Каждый фильтр извлекает разные признаки из исходного изображения и даёт свою карту признаков на выходе. Эти карты складываются в **стек** (stack). Таким образом, у нас появляется третье измерение в выходной карте признаков. На оно связано не с числом цветовых каналов, а с числом применяемых фильтров.

Стек карт признаков передаётся на следующий слой CNN сети. Его обрабатывает трёхмерный фильтр следующего свёрточного слоя. Он распознаёт признаки более высокого уровня абстракции, чем фильтр предыдущего слоя.

Подведём итог. Если фильтр состоит из нескольких ядер, то выполняется свёртка по объёму. Результат комбинируется в одну карту признаков. Если применяются несколько фильтров, каждый из которых имеет только одно ядро, выполняется обычная свёртка. В результате получается стек из нескольких карт признаков. Их число равно числу применённых фильтров.

#### 3.6.3.2 Применение функции активации

Мы познакомились с понятием функции активации, когда рассматривали архитектуру NNUE. Это одна из двух главных характеристик слоя нейронной сети.

Каждый нейрон выполняет функцию активации над своими входными данными. Так он получает выходные данные, которые передаются по сети дальше. Нейроны свёрточного слоя CNN сети — не исключение. Они делают то же самое.

Разберёмся, зачем нейронам свёрточного слоя нужна функция активации. Она решает следующие задачи:

1. Добавляет нелинейность (non-linearity) в модель. Благодаря функции активации, сеть может усваивать сложные закономерности во входных данных.

2. Управляет диапазоном активации. **Диапазон активации** (activation range) — это значения, которые могут принимать выходные данные функции активации, а следовательно и нейрона. Управление диапазоном решает проблемы взрывающегося и исчезающего градиента во время обучения.

3. Повышает разряженность активации. **Разряженность активации** (activation sparsity) возникает, когда многие выходные сигналы нейронов внутренних слоёв сети равны нулю. Это ускоряет вычисления, поскольку операции с нулями можно пропускать. Также разряженность уменьшает шум в выходных данных нейронов. Тем самым улучшается обобщающая возможность модели.

4. Выбор признаков. Функция активации позволяет модели концентрироваться на наиболее важных и отличительных признаках во входных данных. Для она выборочно увеличивает или уменьшает определённые элементы карты признаков.

В свёрточных слоях обычно используют функцию активации **ReLU**. Её формула выглядит так:
{width: "50%"}
![](images/Chess/cnn-relu-formula.png)

В ней используются следующие обозначения:

* x — входное значение нейрона
* y — выходное значение нейрона.

Иллюстрация 3-43 демонстрирует график функции ReLU.

{caption: "Иллюстрация 3-43. Функция активации ReLU", height: "40%"}
![Функция активации ReLU](images/Chess/cnn-relu-graph.png)

ReLU преобразует входное значение x в результат y, который принимает значения от 0 до положительной бесконечности. Если входное значение меньше или равно нулю, то ReLU выдаёт ноль. В противном случае — входное значение.

Рассмотрим, что означает применение функции активации ReLU с точки зрения обработки исходного изображения.

Сначала происходит операция свёртки. В результате из исходного изображения получается карта признаков. Некоторые пиксели этой карты имеют отрицательные значения. Знак минус означает, что пиксель относится к некоторому шаблону. Этот шаблон противоположен тем, которые ищет фильтр. Другими словами, пиксели с отрицательными значениями менее информативны для задачи, которую решает CNN сеть. Например, фильтр может искать светлые области на изображении. Тогда тёмные области получат отрицательные значения на карте признаков.

После операции свёртки применяется функция активации ReLU. Она зануляет все пиксели с отрицательными значениями. Таким образом они подавляются и исключаются из дальнейших вычислений. Это позволяет сети концентрироваться только на самых важных признаках. Менее существенные признаки она просто игнорирует.

#### 3.6.3.3 Операция подвыборки

Мы рассмотрели операцию свёртки. Теперь поговорим об операции **подвыборки** (pooling). Её выполняет отдельный слой CNN сети, который обычно идёт сразу за слоем свёртки. Наличие слоя подвыборки не обязательно и зависит от решаемой задачи. Есть архитектуры CNN сетей, в которых он не применяется.
 
Операция подвыборки решает следующие задачи:

1. Уменьшает **пространственные размеры** (spatial dimensions) карты признаков. Это уменьшает размеры ядер в последующих свёрточных слоях. Меньшие ядра означают меньше параметров, которые надо подобрать в процессе обучения. В результате работа с сетью требует меньших вычислений.

2. Обеспечивает инвариантность. **Инвариантность** (invariance) — это способность сети распознавать признаки независимо от преобразований исходного изображения. Примеры преобразований: перемещение и вращение.

3. Уменьшает вероятность переобучения (overfitting) модели. Благодаря упрощению карты признаков, операция подвыборки снижает риск подгонки сети под обучающие наборы входных данных.

4. Агрегация **локальных признаков** (local features). Операция подвыборки выделяет наиболее значимые признаки в каждой области входных данных. Это помогает модели отделить важную входную информацию от шума.

Операция подвыборки уменьшает размер карты признаков, полученной после свёртки. Для этого задаётся некоторое **окно подвыборки** (pooling window). Оно отдалённо напоминает 
ядро для операции свёртки. Отличие в том, что окно подвыборки имеет размер, но не имеет весов. Его задача — проходить по входному массиву и последовательно выделять группы элементов. На каждом шаге к выделенным элементам применяется операция подвыборки.

Есть несколько типов операции подвыборки. Самые распространённые из них следующие:

1. **Max pooling** — подвыборка по максимальному значению.

2. **Average pooling** — подвыборка по среднему значению.

Подвыборка по максимальному значению выбирает из всех выделенных элементов наибольший. Он записывается в выходной массив, а остальные выделенные элементы отбрасываются.

Рассмотрим пример. Допустим, что у нас есть карта признаков размером 5x5. Мы применяем к ней операцию подвыборки по максимальному значению с окном размера 3x3. Пусть шаг прохода по входному массиву равен двум. Тогда на первом шаге, окно перекроет следующую часть карты признаков:
{width: "40%"}
![](images/Chess/pooling-source-image-5x5-highlight-first.png)

Максимальный из перекрытых элементов массива равен 251. Записываем его в качестве первого элемента выходной матрицы операции подвыборки.

На втором шаге окно подвыборки перекроет следующую часть карты признаков:
{width: "40%"}
![](images/Chess/pooling-source-image-5x5-highlight-second.png)

В этой области максимальный элемент равен 238. Это второй элемент в выходной матрице.

На третьем шаге окно подвыборки возвращается в левый край карты признаков и смещается вниз на два ряда. Мы выбрали шаг равный двум, поэтому все горизонтальные и вертикальные смещения равны двум.

На четвертом шаге окно подвыборки окажется в правом нижнем углу карты признаков. После выбора максимального элемента в этой области мы получим следующую выходную матрицу:
{width: "25%"}
![](images/Chess/max-pooling-result-image-2x2.png)

Подвыборка по среднему значению выполняется аналогично. Она рассчитывает среднее значение всех элементов карты признаков, которые перекрыло окно подвыборки на текущем шаге.

Выполним операцию подвыборки по среднему значению для карты признаков из нашего пример. На первом шаге окно подвыборки перекроет следующую его часть:
{width: "40%"}
![](images/Chess/pooling-source-image-5x5-highlight-first.png)

Среднее значение элементов в перекрытой области карты признаков рассчитывается так:
{line-numbers: false, format: text}
```
(19 + 10 + 39 + 30 + 251 + 238 + 45 + 6 + 59) / 9 = 77.4444
```

Результат округляем до целого и получаем 77. Это значение первого элемента в выходной матрице. Повторим операцию подвыборки для оставшихся трёх шагов. Получим следующую выходную матрицу:
{width: "25%"}
![](images/Chess/average-pooling-result-image-2x2.png)

Выбор между операциями max pooling и average pooling зависит от решаемой задачи. Подвыборка по максимальному значению эффективна для обнаружения резко выделяющихся элементов изображения (например, края объектов). При этом она отбрасывает менее заметные элементы. Подвыборка по среднему значению применяется, когда важно сохранить всю информацию исходного изображении (например, текстуру фона).

#### 3.6.3.4 Операция сглаживания

Операция **сглаживания** (flatten) выполняется после подвыборки. Она преобразовывает пространственную информацию карты признаков в формат, который может обработать полносвязный слой сети. В терминологии архитектуры CNN такой слой называется **плотным** (dense). Операция сглаживания служит мостом между слоями свёртки и подвыборки с одной стороны и плотными слоями с другой.

Операция сглаживания решает следующие задачи:

1. Преобразует карту признаков в одномерный вектор. Например, если слой подвыборки выдал стек карт признаков размером 12x12x4, то операция сглаживания преобразует его в одномерный массив размером: `12 * 12 * 4 = 576`.

2. Упрощает формат данных. Свёрточные слои фиксируют пространственную иерархию признаков: края объектов, их текстуры и части. Эта информация отображается на карте признаков. Операция сглаживания сохраняет обнаруженные при свёртке признаки, но отбрасывает пространственную информацию.

Рассмотрим операцию сглаживания на примере. Допустим, что подвыборка по максимальному значению дала следующую карту признаков размером 2x2:
{width: "25%"}
![](images/Chess/max-pooling-result-image-2x2.png)

Операция сглаживания развернёт эту матрицу в одномерный вектор из четырёх элементов:
{width: "40%"}
![](images/Chess/flattening-result-image-4x1.png)

Если операция подвыборки возвращает стек карт признаков, то сглаживание развернёт все эти карты в один одномерный массив. Только такой формат данных сможет обработать последующий плотный слой сети.

Плотные слои сети принимают конечное решение на основе всех признаков, извлечённых из входного изображения. Для примера допустим, что сеть решает задачу классификации. Тогда именно её плотные слои определяют, что за объект на изображении: животное или машина.

#### 3.6.3.5 Пакетная нормализация

При обучении моделей часто возникает проблема ковариантного сдвига. Чтобы лучше понять её суть, обратимся к примеру.

В разделе 2.6.2.1.1 мы классифицировали ирисы из набора данных Фишера. Допустим, что теперь мы обучаем более сложную модель. На вход она получает фотографию некоторого цветка. Модель должна ответить на вопрос: относится ли он к роду ирисов?

В этой задаче есть одна тонкость. Дело в том, что разные виды ирисов различаются по цвету. Это различие демонстрирует иллюстрация 3-44.

{caption: "Иллюстрация 3-44. Фотография разных видов ирисов", width: "75%"}
![Ирисы разных видов](images/Chess/iris-flowers.jpg)

Теперь предположим, что в обучающий набор данных (training set) попали только ирисы фиолетового цвета и другие растения не ирисы. В тестовом наборе данных (test set) есть ирисы трёх цветов: фиолетового, белого и жёлтого. В этом случае точность модели будет неудовлетворительной. Она не сможет распознать ирисы с цветом, который отличается от фиолетового и его оттенков.

Почему точность модели оказалось низкой? Алгоритм обучения настроил веса нейронов сети так, чтобы она распознавала признаки, характерные для ирисов фиолетового цвета. Модель реагирует на то, что однозначно отличает этот вид ирисов от других растений. Наверняка сложно сказать, каким именно признакам обучилась модель. Но наиболее вероятный признак — это фиолетовый цветок.

Мы проверяем обученную модель на тестовом наборе данных. В нём встречаются жёлтые и белые ирисы. Это новые цветовые характеристики, которых не было в обучающем наборе. Алгоритм обучения не сконфигурировал модель для обнаружения этих признаков. Поэтому сеть ошибочно отнесёт жёлтые и белые ирисы к другим растениям.

Причина низкой точности модели в том, что обучающий и тестовый наборы содержат фиолетовые, жёлтые и белые ирисы в разных пропорциях. Модель учится отображать исходное множество X (цветки) в целевое множество Y (ирисы разных видов). Целевое множество состоит из подмножеств A (фиолетовые ирисы), B (жёлтые ирисы) и C (белые ирисы). Если пропорции элементов из подмножеств A, B и C меняются в исходном множестве X, то модель необходимо переобучить заново. В нашем примере произошло именно это. Обучающий набор — это исходное множество X. В нём есть только элементы A. Тестовый набор — это изменённое множество X с элементами A, B, C. Пропорции элементов в X изменились — следовательно, модель надо переобучить. Такое изменение пропорций в данных называется [**ковариантным сдвигом**](https://neerc.ifmo.ru/wiki/index.php?title=Batch-normalization) (covariate shift).

Мы рассмотрели пример ковариантного сдвига во входных данных модели. Чтобы его скомпенсировать, надо случайным образом перемешать все имеющиеся размеченные данные. Затем составить из них обучающий и тестовый наборы. Элементы обучающего набора надо скомпоновать в подмножества, которые называются **пакетами** (batch). Эти действия обеспечат одинаковые пропорции элементов в обучающем и тестовом наборах.

Какие преимущества даёт объединение элементов обучающего набора в пакеты? Алгоритм градиентного спуска имеет [три возможные реализации](https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a):

1. **Стохастический градиентный спуск** (stochastic gradient descent или SGD) — на каждой итерации алгоритм случайно выбирает один элемент из обучающего набора данных. Затем он корректирует веса модели, чтобы её вывод совпал с ожидаемым. Такая реализация вычислительно эффективна, но имеет низкую скорость сходимости.

2. **Пакетный градиентный спуск** (batch gradient descent) — на каждой итерации алгоритм обрабатывает весь обучающий набор данных целиком и только после этого меняет веса модели. Это медленный вариант обучения, но с быстрой сходимостью.

3. **Мини-пакетный градиентный спуск** (mini-batch gradient descent) — на каждой итерации алгоритм обрабатывает некоторое подмножество обучающего набора данных фиксированного размера (batch size). Эти подмножества называются **мини-пакетами** (mini-batch). Затем алгоритм меняет веса модели. Это промежуточное решение с приемлемой вычислительной эффективностью и сходимостью.

На практике чаще всего применяется третья реализация алгоритма градиентного спуска. Она даёт компромиссное решение с хорошими характеристиками. Из-за популярности этого подхода мини-пакеты для простоты называют пакетами.

Ковариантный сдвиг встречается не только во входных данных модели. Он может произойти в данных, которые передаются между скрытыми слоями сети. Такая проблема возникает только в deep learning моделях. Её суть заключается в следующем. Алгоритм обучения меняет веса и смещения (bias) для нейронов сети. Это меняет статистические свойства выходных данных каждого слоя. Следовательно, входные данные следующего слоя тоже меняются. Из-за этого слоям постоянно приходится адаптироваться к новым свойствам входных данных. Это замедляет обучение и ухудшает его сходимость.

Такая проблема называется **внутренним ковариантным сдвигом** (internal covariate shift). Её невозможно решить случайным перемешиванием входных данных. Вместо этого надо применить специальный метод под названием [**пакетная нормализация**](https://en.wikipedia.org/wiki/Batch_normalization) (batch normalization).

Метод пакетной нормализации предложили сотрудники Google Сергей Иоффе и Кристиан Сегеди в 2015 году. Они подробно описали свою идею и способы её применения в статье "Пакетная нормализация: Ускорение глубокого обучения за счёт уменьшения внутреннего ковариантного сдвига" (["Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"](https://arxiv.org/pdf/1502.03167.pdf)).

Пакетная нормализация применяется к каждому слою сети и корректирует его входные данные. Это поддерживает в них стабильное среднее значение и дисперсию. Таким образом пакетная нормализация гарантирует, что входные данные слоёв будут согласованы на протяжении всего обучения. 

Слово "пакет" в названии метода означает мини-пакеты, которыми оперирует третья реализация алгоритма градиентного спуска. Метод корректирует данные на уровне отдельных мини-пакетов.

Чтобы понять пакетную нормализацию, нам понадобятся следующие термины из теории вероятностей:

* [**Среднее**](https://en.wikipedia.org/wiki/Mean) или среднее арифметическое (mean) рассчитывается сложением всех значений в наборе данных и делением этой суммы на количество значений.

* **Отклонение от среднего** (deviation from the mean) — это разница между конкретным значением из набора и средним этого набора.

* [**Квадрат отклонения от среднего**](https://en.wikipedia.org/wiki/Squared_deviations_from_the_mean) (squared deviation from the mean) — умножение отклонения от среднего на самого себя.

* [**Дисперсия**](https://en.wikipedia.org/wiki/Variance) (variance) — это усреднённое значение квадратов отклонения от среднего. Дисперсия показывает, насколько разбросаны числа в наборе данных. Другими словами, она показывает насколько значения в наборе отличаются от среднего арифметического (mean).

Теперь подробно рассмотрим алгоритм пакетной нормализации:

1. Вычислить среднее арифметическое (mean) всех элементов пакета по формуле:
{height: "10%"}
![](images/Chess/mean-formula.png)

В этой формуле следующие обозначения:

* x~i~ — элементы выборки под номером i
* m — размер пакета.

2. Вычислить дисперсию (variance) для элементов пакета по формуле:
{height: "10%"}
![](images/Chess/variance-formula.png)

3. Для простоты допустим, что пакет представляет собой одномерный массив. Тогда нормализованное значение каждого его элемента рассчитывается по формуле:
{height: "15%"}
![](images/Chess/normalized-value-formula.png)

В этой формуле следующие обозначения:

* ϵ — некоторая малая константа.

В результате пакетной нормализации мы получим распределение значений входных данных слоя с центром в 0 и дисперсией равной 1.

Пакетная нормализация может изменить представление данных в слое. Чтобы этого избежать, вводятся два дополнительных параметра модели: сжатие (γ) и сдвиг (β) нормализованной величины. Они действуют следующим образом:
{width: "50%"}
![](images/Chess/scale-and-shift-formula.png)

Параметры γ и β настраиваются в процессе обучения вместе с остальными параметрами модели.

Алгоритм пакетной нормализации работает как на этапе обучения модели, так и на этапе вывода. **Выводом** (inference) называется нормальный режим работы нейронной сети, когда она решает поставленную задачу (например, классифицирует изображения).

В какой именно точке потока данных в нейронной сети происходит пакетная нормализация? Допустим, что мы применяем её к скрытому слою B. Он получает данные от предыдущего слоя A. При передаче данных из слоя A в слой B они преобразуются в зависимости от типа слоя B. Другими словами, связи между нейронами слоёв определяют преобразование данных. Сразу после этого применяется пакетная нормализация. Затем выполняется функция активации слоя B. Такой подход считается стандартным.

Пакетная нормализация устраняет последствия внутреннего ковариантного сдвига. В этом её главный эффект. Если рассуждать с точки зрения параметров обучения и модели, то преимущества пакетной нормализации следующие:

1. Ускоряет сходимость алгоритма обучения сети.

2. Смягчает исчезающие и взрывающиеся градиенты.

3. Улучшает обобщающую способность модели и уменьшает вероятность переобучения.

4. Снижает чувствительность модели к начальным весам, которые задаются на этапе инициализации. Это упрощает обучение модели.

5. Повышает скорость обучения, поскольку позволяет увеличить темп обучения. Часто рост темпа приводит к взрыву или исчезновению градиента, но пакетная нормализация предотвращает эти проблемы.

I> [**Темп обучения**](https://en.wikipedia.org/wiki/Learning_rate) (learning rate) — это настраиваемый параметр алгоритма обучения, который определяет размер шага на каждой итерации при движении к минимуму функции потерь. **Функция потерь** (loss function) определяет стоимость неправильного принятия решений на основе наблюдаемых данных.

Метод пакетной нормализации оказался очень универсальным. Его применяют к разным архитектурам нейронных сетей, которые ориентированы на разные задачи и предметные области. Например, этот метод используют в CNN сетях для классификации изображений и моделях для обработки естественного языка.

### 3.6.4 Нейронная сеть AlphaZero

Только что мы познакомились со всеми компонентами нейронной сети AlphaZero. Она строится на свёрточных слоях с функцией активации ReLU. Поскольку модель относится к классу deep learning, в ней используется пакетная нормализация. Это предотвращает внутренний ковариантный сдвиг. За вывод модели отвечают плотные слои. Данные для них подготавливает операция сглаживания.

Перед тем как познакомиться с детальным устройством AlphaZero, подумаем над следующим вопросом. Как архитектура сети, оптимизированная для распознавания изображений, может оценивать шахматные позиции?

Позицию фигур на доске можно представить в виде матрицы размером 8x8. Она напоминает структуру типичного изображения 8x8 пикселей. Как мы выяснили, свёрточные слои хорошо подходят для обработки подобных структур.

В разделе 3.3.3 мы познакомились с моделью шаблонов Герберта Саймона. Согласно ей, профессиональные шахматисты иерархически упаковывают информацию о позиции. Таким образом они анализируют не отдельные фигуры, а их сочетания.

Свёрточная нейронная сеть AlphaZero делает примерно то же самое, что и шахматисты. Она оценивает шахматную позицию, исходя из её визуальной структуры. Таким образом, сеть может извлекать, например, следующие признаки:

* открытые вертикали
* расположение коней
* открытые диагонали слонов
* пешечная структура.

На какие именно признаки ориентируется обученная модель AlphaZero, сказать невозможно. Они обобщаются в высокоуровневые шаблоны. Похожие шаблоны распознают профессиональные игроки. Это сходство делает поведение AlphaZero похожим на действия опытных шахматистов. Поведение системы намного ближе к человеческому, чем у традиционных движков наподобие Stockfish.

Рассмотрим общие принципы работы модели AlphaZero. Начнём с нейронной сети. Она получает на вход текущую позицию фигур на доске. На выходе сеть даёт два значения:

1. **Вероятность каждого ход** (policy или move probability), возможного в заданной позиции. Этот вывод совмещает в себе генератор ходов и функцию оценки традиционного шахматного движка. Вероятность хода является по сути его оценкой: чем она выше, тем лучше ход.

2. **Результат партии** (value) указывает вероятность победы белых, чёрных или ничьи, если проиграть партию до конца из заданной позиции.

Теперь поговорим об алгоритме поиска. Для выбора хода в текущей позиции AlphaZero использует поиск методом Монте-Карло. При этом нейронная сеть определяет направление, в котором развёртываются узлы дерева игры. Она применяется на следующих шагах алгоритма MCTS:

1. На шаге selection нейронная сеть определяет, какой узел дерева выбрать для исследования. Лучший узел соответствует позиции после хода, который имеет наибольшую вероятность. Эту информацию даёт первый вывод (move probability) нейронной сети.

2. На шаге simulation алгоритм MCTS не проигрывает партию до конца, выбирая случайные ходы. Вместо этого AlphaZero узнаёт наиболее вероятный результат партии из второго вывода (value) нейронной сети.

Алгоритм MCTS применяется не только для вывода модели AlphaZero, но и для её обучения. Во время обучения с подкреплением модель играет сама с собой. Каждый ход в этой игре выбирает алгоритм MCTS, которым управляет текущая версия нейронной сети. В результате накапливаются следующие данные:

1. Все позиции фигур в сыгранной партии.

2. Результат партии, который предсказала нейронная сеть в каждой проверенной позиции.

3. Вероятности ходов, которые вернула сеть в каждой проверенной позиции.

4. Реальный результат партии.

Алгоритм обучения использует эти данные, чтобы скорректировать параметры нейронной сети AlphaZero. Эта корректировка происходит после каждой партии, которую модель сыграла сама с собой. Таким образом, алгоритм обучения исправляет ошибки в действиях модели.

#### 3.6.4.1 Входные данные сети

В традиционных шахматных движках данные о ходах и позициях хранятся в битбордах. Их можно считать внутренним форматом данных программы. Движок использует битборды практически для всех операций:

* генерации ходов
* оценки позиций
* оценки состояния игры (шах, мат, пат).

Битборды дают выигрыш производительности, когда движок работает на 64-разрядном процессоре (CPU). Но такая оптимизация не нужна для нейронной сети, которая выполняется на графическом процессоре (GPU). Рассмотрим, с какими форматами данных работает AlphaZero.

Нейронная сеть AlphaZero получает на вход набор матриц (plane) размера 8x8. Эти матрицы кодируют всю информацию о партии: позиции фигур, какой игрок ходит, история ходов и т.д. Вот полный список эти матриц:

1. Шесть матриц кодируют положение белых фигур на доске. Для фигур каждого типа отводится своя матрица: пешки, кони, слоны, ладьи, ферзь, король. Каждому полю соответствует элемент матрицы с определённым индексом. Если фигура занимает поле, элемент матрицы равен 1. В противном случае он равен 0. Например, если белая пешка на поле b2, то элемент матрицы белых пешек с индексом (2, 2) равен 1.

2. Шесть матриц кодируют положение чёрных фигур на доске. Они устроены точно так же, как шесть матриц для белых фигур.

3. Две матрицы используются для подсчёта повторяющихся ходов. Когда текущая позиция повторяется единожды, все элементы первой матрицы заполняются единицами. Если позиция повторяется дважды, то же самое происходит со второй матрицей. Так AlphaZero отслеживает ничью при троекратном повторении позиции (правило трёх ходов).

4. Чтобы отслеживать правило трёх ходов, во входных данных сети сохраняются последние восемь позиций на доске. В начале партии соответствующие матрицы занулены. Далее по ходу партии они отражают реальные позиции. Для этого отводится `(6 + 6) * 8 = 96` входных матриц.

5. Одна матрица хранит цвет фигур игрока, который делает ход. Все её элементы равны 1, если ходят белые. При ходе чёрных она зануляется.

6. Четыре матрицы кодируют правила рокировки: белые короткая (на королевском фланге), белые длинная (на ферзевом фланге), чёрная короткая и чёрная длинная. Если рокировка возможна, все элементы соответствующей матрицы выставляются в 1. В противном случае матрица зануляется.

7. Одна матрица используется как счётчик ходов с начала партии. Её элементы, начиная с первого (1,1), интерпретируются как разряды 64-битного целого числа.

8. Одна матрица кодирует счётчик для отслеживания правила 50-ти ходов: если в течении 50-ти ходов пешки не двигались и не было взятий, то объявляется ничья. Кодирование работает так же, как для матрицы со счётчиком ходов от начала партии.

9. Две матрицы используются для правила взятия пешки на проходе. Если взятие возможно на каком-то поле, соответствующий ему элемент матрицы выставляется в 1. Есть отдельная матрица для белых и для чёрных.

Команда DeepMind описала свою систему в статье "Освоение шахмат и сёги посредством самостоятельной игры с помощью общего алгоритма обучения с подкреплением" (["Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm"](https://arxiv.org/pdf/1712.01815.pdf)). В ней сказано, что нейронная сеть имеет 119 входов. Все эти входы мы рассмотрели.

#### 3.6.4.2 Выходные данные сети

Нейронная сеть AlphaZero на выходе даёт два значения. Первое — ожидаемый **результат партии** (value), если проиграть её до конца из заданной позиции. Это очень грубая оценка. Её использует алгоритм MCTS на шаге simulation вместо проигрывания серии партий со случайными ходами. Вывод нейронной сети оказывается всё равно точнее, чем такая статистика.

Результат партии — это вещественное число в диапазоне [-1, 1]. Его значения имеют следующий смысл:

* -1 — проигрывает игрок, выполняющий текущий ход.
* 0 — ничья.
* 1 — выигрывает игрок, выполняющий текущий ход.

Промежуточные значения соответствуют степени вероятности. Например, число 0.5 означает, что делающий ход игрок выигрывает в половине случаев.

Второй выход сети — это **вероятность каждого хода** (policy), возможного в текущий позиции. Формат этого выхода достаточно сложный. Проблема в том, что количество допустимых ходов меняется в зависимости от позиции фигур на доске. С другой стороны, число выходов нейронной сети должно быть постоянным. Рассмотрим, как команда DeepMind решила это затруднение.

Разработчики AlphaZero предложили следующую идею. В любой позиции на доске надо рассматривать некоторый одинаковый шаблон возможных ходов. В шаблон входят:

1. Начальные поля, из которых некоторая фигура начинает свой ход.

2. Целевые поля, в которые эта фигура делает ход.

Начальные поля — это все 64 поля шахматной доски. Потенциально, фигура может пойти из любого из них. Для каждого исходного поля есть набор полей, которые может достигнуть фигура. Вопрос — о какой именно фигуре идёт речь? Разработчики предложили концепцию **суперфигуры**. Она ходит как ферзь и конь. Таким образом покрываются все возможные ходы любой настоящей шахматной фигуры. Каждому возможному ходу суперфигуры из каждого начального поля доски соответствует один выход нейронной сети.

Такой формат выходных данных избыточен. Например, нейронная сеть оценит вероятность некоторого хода как ненулевую. Но правила шахмат запрещают этот ход, потому что он открывает шах своему королю. Чтобы решить эту проблему, надо занулить вероятности запрещённых ходов. После этого вероятности оставшихся разрешённых ходов пересчитываются так, чтобы их сумма снова стала равна единице.

Представим для наглядности шаблоны возможных ходов в виде набора таблиц. Таблица 3-1 демонстрирует все ходы суперфигуры как ферзя из всех полей доски. Первый столбец указывает поле, из которого фигура начинает ход. Второй столбец — это направление, в котором она двигается. Третий столбец — число полей, на которое передвигается фигура. Каждая строка таблицы соответствует одному выходу нейронной сети AlphaZero.

{caption: "Таблица 3-1. Все возможные ходы ферзём из всех полей доски", width: "80%"}
| Начальное поле | Направление | Число полей |
| --- | --- | --- |
| a1 | Вверх | 1 |
| a1 | Вверх | 2 |
| ... | ... | ... |
| a1 | Вверх | 6 |
| a1 | Вверх | 7 |
| a1 | Вверх-вправо | 1 |
| ... | ... | ... |
| a1 | Вверх-вправо | 7 |
| a1 | Вправо | 1 |
| ... | ... | ... |
| a1 | Вправо | 7 |
| a1 | Вниз | 1 |
| ... | ... | ... |
| a1 | Вниз | 7 |
| ... | ... | ... |
| ... | ... | ... |
| h8 | Влево | 7 |

Первая строка таблицы соответствует ходу любой фигуры из a1 вверх на одно поле. Таким образом этот выход нейронной сети определяет вероятность хода a1-a2. Следующая строка таблицы и соответствующий выход сети — это ход фигуры a1-a3. Аналогично перечислены все возможные ходы ферзём со всех полей доски.

Обратите внимание, что таблица 3-1 учитывает, в том числе и ходы за пределы доски. Например, это ходы из поля a1 в следующих направлениях:

* вниз-вправо
* вниз
* вниз-влево
* влево
* вверх-влево.

Такое решение упрощает формат выходных данных. Ходы за пределы доски запрещены правилами шахмат, поэтому их вероятность всегда зануляется.

Ходы ферзём покрывают следующие типы фигур: пешка, слон, ладья, ферзь, король. Остаются ходы конём, которые не учитывает таблица 3-1. Составим для коней аналогичную таблицу 3-2.

{caption: "Таблица 3-2. Все возможные ходы конём из всех полей доски", width: "80%"}
| Начальное поле | Направление |
| --- | --- |
| a1 | Два вверх, одно вправо |
| a1 | Два вправо, одно вверх |
| a1 | Два вправо, одно вниз |
| a1 | Два вниз, одно вправо |
| a1 | Два вниз, одно влево |
| a1 | Два влево, одно вниз |
| a1 | Два влево, одно вверх |
| a1 | Два вверх, одно влево |
| a2 | Два вверх, одно вправо |
| ... | ... |
| a2 | Два вверх, одно влево |
| ... | ... |
| ... | ... |
| h8 | Два вверх, одно влево |

Первая строка таблицы описывает ход коня из поля a1 на поле b3. Она соответствует выходу нейронной сети, который определяет вероятность хода a1-b3. Продолжая аналогично, получаются выходы сети для всех возможных ходов конём. Некоторые из них выводят фигуру за пределы доски и запрещены правилами игры. Их вероятность всегда зануляется.

Последний тип ходов, которые не учитывают таблицы 3-1 и 3-2, — это превращения пешки. По правилам пешка, достигшая последней горизонтали (8-й для белых и 1-й для чёрных), превращается в любую фигуру кроме короля. Пешка может достигнуть последней диагонали в результате одного из следующи ходов:

* Движение вперёд на одно поле.
* Взятие вражеской фигуры на соседнем поле по диагонали слева.
* Взятие фигуры на соседнем поле по диагонали справа.

Чтобы учесть эти ходы, составим отдельную таблицу 3-3.

{caption: "Таблица 3-3. Все ходы с превращением пешки из всех полей доски", width: "80%"}
| Начальное поле | Тип хода | Превращение в фигуру |
| --- | --- | --- |
| a1 | Вперёд | Конь |
| a1 | Вперёд | Слон |
| a1 | Вперёд | Ладья |
| a1 | Взятие влево | Конь |
| ... | ... | ... |
| a1 | Взятие влево | Ладья |
| a1 | Взятие вправо | Конь |
| ... | ... | ... |
| a1 | Взятие вправо | Ладья |
| a2 | Вперёд | Конь |
| ... | ... | ... |
| a2 | Взятие вправо | Ладья |
| ... | ... | ... |
| h8 | Взятие вправо | Ладья |

Мы не рассматриваем ходы пешки с превращением в ферзя, потому что они покрываются таблицей 3-1.

Правила шахмат допускают лишь малую часть ходов из таблицы 3-3. Например, ходы из поля a1 не достигают последней горизонтали (8-й или 1-й). Поэтому их вероятность всегда зануляется. Тем не менее каждая строка таблицы 3-3 соответствует одному выходу нейронной сети.

Рассчитаем сумму всех выходов нейронной сети. Таблица 3-1 содержит следующее число строк:
{line-numbers: false, format: text}
```
8 * 7 * 64 = 3584
```
В этой формуле: 

* 8 — число направлений для хода ферзём.
* 7 — число полей при движении в каждом направлении.
* 64 — число полей на доске, с которых может ходить ферзь.

Таблица 3-2 содержит столько строк:
{line-numbers: false, format: text}
```
8 * 64 = 512
```
В этой формуле: 

* 8 — число направлений для хода конём.
* 64 — число полей на доске, с которых может ходить конь.

В таблице 3-3 строк столько:
{line-numbers: false, format: text}
```
3 * 3 * 64 = 576
```
В этой формуле: 

* 3 — число направлений для хода пешкой.
* 3 — число фигур для превращения пешки.
* 64 — число полей на доске, с которых может ходить пешка.

Сложим полученные результаты:
{line-numbers: false, format: text}
```
3584 + 512 + 576 = 4672
```

Таким образом, у нейронной сети AlphaZero всего 4672 выхода. Эти выходы относятся только ко второму выводу сети, который оценивает вероятности возможных ходов (policy). Они никак не связаны с первым выводом, который по текущей позиции оценивает ожидаемый результат партии (value).

Для обучения нейронной сети с таким большим количеством выходов нужны значительные вычислительные ресурсы. Почему команда DeepMind выбрала такой подход? На самом деле разработчики экспериментировали с альтернативным кодированием выходов. В этом случае перечислялись все допустимые ходы: a1-a2, a1-a3, a1-a4 и т.д. Таким образом получаются все возможные комбинации пар: исходное поле, целевое поле. При этом исключаются запрещённые правилами ходы за пределы шахматной доски. К этому списку нужно добавить допустимые правилами ходы на превращение пешки. В итоге получится менее 2000 выходов сети.

На практике оказалось, что нейронная сеть с меньшим числом выходов обучается дольше. Причина в том, что подход с 4672 выходами сфокусирован на исходном поле фигуры. Это позволяет модели разделить решение о выборе хода на две части. Сначала грубо решить, какая именно фигура должна ходить в текущей позиции. Чтобы обучиться этому, модели достаточно совсем немного итераций алгоритма обучения. Затем после дополнительных итераций, сеть учится более точно определять, куда именно должна пойти выбранная фигура. Такое разделение делает процесс обучения эффективнее.

#### 3.6.4.3 Архитектура сети

AlphaZero и AlphaGo Zero используют одну и ту же архитектуру нейронной сети. Команда DeepBlue подробно описала её в статье, посвящённой AlphaGo Zero. Статья называется "Освоение го без человеческих знаний" (["Mastering Go without Human Knowledge"](https://discovery.ucl.ac.uk/10045895/1/agz_unformatted_nature.pdf)).

Рассмотрим схему нейронной сети AlphaZero. Её демонстрирует иллюстрация 3-45.

{caption: "Иллюстрация 3-45. Схема нейронной сети AlphaZero", width: "100%"}
![Нейронная сеть AlphaZero](images/Chess/alphazero-network-architecture.png)

I> Входные данные сети (Input) представляют собой стек из 119 матриц размером 8x8. Их последовательно обрабатывают несколько блоков нейронной сети. Познакомимся с ними в порядке следования.

Входные данные сети (Input) представляют собой стек из 119 матриц размером 8x8. Их последовательно обрабатывают внутренние слои нейронной сети. Для удобства разделим эти слои на блоки. Один блок представляет собой логически законченный набор операций.

Входные данные модели поступают на первый **блок свёртки** (Convolutional block). Он последовательно выполняет следующие операции:

1. **Свёртка типа А** (Convolution A). Её выполняет свёрточный слой, который состоит из стека 256 фильтров. Каждый фильтр имеет одно ядро размером 3x3. Операция свёртки выполняется с шагом 1.

2. **Пакетная нормализация** (Batch normalization).

3. **Функция активации ReLU** (Activation function ReLU).

Остановимся на первом блоке свёртки (Convolutional block) и разберём принцип его работы. Из описания блока вытекает несколько вопросов.

Во-первых, как именно 256 фильтров свёрточного слоя (Convolution A) применяются к стеку из 119 входных матриц? Обратите внимание, что речью идёт об отдельных фильтрах, а не об одном фильтре с 256 ядрами. Это означает, что выполняется не свёртка по объёму, а обычная свёртка. В этом случае каждый из 256 фильтров применяется ко всему стеку из 119 входных матриц. Фактически это означает, что каждый фильтр имеет размер 3x3x119.

Для простоты сконцентрируемся только на одном фильтре. Он проходит с заданным шагом по всему стеку входных матриц. На каждом шаге рассчитывается соответствующий выходной элемент карты признаков. Для этого выполняются следующие действия:

1. Вычислить произведение Адамара для элементов ядра фильтра и перекрытых элементов каждой входной матрицы. Всего получится 3x3x119 отдельных операций умножения.

2. Сложить результаты всех 3x3x119 операций умножения. Эта сумма даст один элемент выходной карты признаков.

После этого фильтр перемещается по горизонтали на один элемент, потому что шаг свёртки равен единице. Для новой позиции ядра рассчитывается очередной элемент карты признаков. Когда фильтр доходит до правой границы матриц, он смещается вниз на один ряд и проходит его слева направо. Таким образом, фильтр проходит по всем элементам входных матриц.

Применение одного фильтра к стеку входных матриц даёт одну карту признаков. Если применить 256 фильтров к стеку входных матриц, получится 256 карт признаков. Они передаются в следующий слой нейронной сети.

Второй вопрос: какой размер имеет каждая из 256 карт признаков на выходе свёрточного слоя? Если применить фильтры без выравнивания, это приведёт к потере данных. Информация об элементах на границах каждой из 119 входных матриц потеряется. Это допустимо при обработке изображений, но неприемлемо для матриц с расположением фигур на доске. Каждая фигура важна для правильной оценки позиции. Поэтому перед обработкой стека матриц выполняется выравнивание (padding). По краям каждой из 119 матриц добавляется по одному столбцу и ряду занулённых элементов. Поэтому выходные карты признаков свёрточного слоя (Convolution A) имеют размер 8x8.

Выходные данные первого блока свёртки (Convolutional block) передаются на первый **остаточный блок** (Residual block). Этот блок последовательно выполняет следующие операции:

1. **Свёртка типа А** (Convolution A).

2. **Пакетная нормализация** (Batch normalization).

3. **Функция активации ReLU** (Activation function ReLU).

4. **Свёртка типа А** (Convolution A).

5. **Пакетная нормализация** (Batch normalization).

6. **Остаточное соединение** (Residual connection).

7. **Функция активации ReLU** (Activation function ReLU).

Остаточный блок нейронной сети можно представить, как комбинацию двух блоков свёртки (Convolutional block). В нём выполняются те же самые операции только дважды. Особенность блока заключается в **остаточном соединении** (Residual connection).

Остаточный блок (Residual block) работает следующим образом:

1. **Первый поток данных**. Входные данные блока последовательно проходят через два слоя свёртки, их пакетную нормализацию и функцию активации первого слоя.

2. **Второй поток данных**. Остаточное соединение передаёт входные данные блока на функцию активации после второго слоя свёртки. При этом все остальные операции блока игнорируются.

3. **Остаточная операция** (residual operation). Первый и второй потоки данных суммируются по следующей формуле: 
{line-numbers: false, format: text}
```
output = layer(input) + input
```
В этой формуле:

* `output` — выходные данные остаточного блока.
* `layer(input)` — результат прохода входных данных блока через все его слои (первый поток данных).
* `input` — входные данные блока как есть (второй поток данных).

4. **Выходные данные блока** представляют собой результат остаточной операции после его обработки функцией активации ReLU.

При обучении такой глубокой нейронной сети как у AlphaZero неизбежна проблема исчезающего градиента. Остаточные соединения решают эту проблему.

Всего в нейронной сети AlphaZero 19 остаточных блоков. Они следуют друг за другом. Последний из них выдаёт стек из 256 карт признаков размером 8x8. Они отличаются от выходных карт признаков блока свёртки (Convolutional block) тем, что содержат шаблоны самого высокого уровня абстракции. Именно эти шаблоны позволяют нейронной сети правильно оценивать шахматные позиции. Если бы их удалось узнать, это могло бы вывести шахматную теорию на новый уровень. К сожалению, выяснить суть этих шаблонов практически невозможно.

Выходные данные последнего остаточного блока передаются по двум направлениям. В терминологии сети AlphaZero каждое направление ведёт в блок под названием **голова** (head).

Первая голова вычисляет вероятности возможных ходов (Policy head). Она последовательно выполняет следующие операции:

1. **Свёртка типа B** (Convolution B). Её выполняет свёрточный слой, который состоит из стека двух фильтров. Каждый фильтр имеет одно ядро размером 1x1. Операция свёртки выполняется с шагом 1.

2. **Пакетная нормализация** (Batch normalization).

3. **Функция активации ReLU** (Activation function ReLU).

4. **Операция сглаживания** (Flatten). После свёртки типа B (Convolution B) получается стек из двух карт признаков размером 8x8. Операция сглаживания приводит их в формат одномерного массива размером 128. Он нужен следующему далее линейному слою.

5. **Линейный полносвязный слой типа A** (Dense A) вычисляет значение каждого из 4672 выходов нейронной сети. Эти выходы оценивают вероятности всех возможных ходов в текущей позиции фигур на доске.

Вторая голова вычисляет ожидаемый результат партии (Value head), сыгранной до конца из текущей позиции. Голова последовательно выполняет следующие операции:

1. **Свёртка типа C** (Convolution C). Её выполняет свёрточный слой, который состоит из одного фильтра. Он имеет одно ядро размером 1x1. Операция свёртки выполняется с шагом 1.

2. **Пакетная нормализация** (Batch normalization).

3. **Функция активации ReLU** (Activation function ReLU).

4. **Операция сглаживания** (Flatten). После свёртки типа C (Convolution C) получается одна карта признаков размером 8x8. Операция сглаживания приводит её в формат одномерного массива размером 64. Он нужен следующему далее линейному слою.

5. **Линейный полносвязный слой типа B** (Dense B) принимает на вход массив из 64 элементов. На выходе он выдаёт массив из 256 элементов. Это входной формат следующего далее линейного слоя.

6. **Функция активации ReLU** (Activation function ReLU).

7. **Линейный полносвязный слой типа C** (Dense C) сводит входной массив из 256 элементов к одному вещественному числу на выходе.

8. **Функция активации TanH** (Activation function TanH) приводит вещественное число, полученное на выходе из линейного слоя типа C (Dense C), к диапазону значений [-1, 1]. Результат определяет наиболее вероятный исход партии с учётом текущей позиции на доске.

Название функции активации TanH — это сокращение от [**hyperbolic tangent**](https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/) (гиперболический тангенс). Её формула выглядит так:
{height: "10%"}
![](images/Chess/tanh-formula.png)

Обозначения в формуле следующие:

* x — входное значение нейрона
* y — выходное значение нейрона
* e — константа [основание натурального логарифма](https://ru.wikipedia.org/wiki/E_(число)).

Иллюстрация 3-46 демонстрирует график функции TanH.

{caption: "Иллюстрация 3-46. Функция активации Tanh", height: "40%"}
![Функция активации Tanh](images/Chess/tanh-graph.png)

Из графика видно, что чем больше входное значение x, тем ближе результат функции y к единице. Аналогично, чем меньше значение x, тем ближе y к минус единице.

Мы рассмотрели схему нейронной сети AlphaZero и все операции над входными данными, которые она выполняет. Ещё раз подчеркну, что чрезвычайно сложно понять смысл данных, которые выдаёт каждый скрытый слой сети. Модель обучена таким образом, чтобы получать на вход данные в определённом формате и выдавать другие данные в другом формате. Чтобы интерпретировать данные между входом и выходом такой сложной модели, потребуются экспертные знания профессиональных шахматистов, специалистов по машинному обучению, значительное время и серьёзные вычислительные ресурсы.

### 3.6.5 Результаты AlphaZero

В системе AlphaZero команда DeepMind реализовала принципиально новый подход к разработке шахматных движков. В эту систему не было заложено никаких знаний об игре в шахматы (например, дебюты и шахматные окончания). Только некоторые правила игры повлияли на структуру входов и выходов нейронной сети, а также порядок поиска методом Монте-Карло.

Принципиальное отличие AlphaZero от традиционных шахматных движков — это характер поиска. Движки, основанные на алгоритме минимакс с альфа-бета отсечением, большую часть времени тратят на перебор возможных ходов. Их функция оценки позиции работает чрезвычайно быстро.

С другой стороны AlphaZero большую часть времени тратит на оценку позиций, а не на поиск. Причина в том, что каждое обращение к нейронной сети требует времени и огромных вычислительных ресурсов. Из-за этого AlphaZero анализирует только малую часть возможных ходов в отличие от движков на альфа-бета поиске. Таким образом AlphaZero делает ставку на качество анализа, а не на количественные показатели вроде числа просмотренных ходов.

Достижения AlphaZero повлияли на развитие шахматной теории. В результате самостоятельного обучения с подкреплением система нашла новые варианты в дебютах, которое ранее считались малоперспективными. Это заставило шахматное сообщество пересмотреть устоявшуюся дебютную теорию. Также новый стиль игры, продемонстрированный AlphaZero, дал новые идеи для теории шахматной стратегии и тактики.

AlphaZero представляет собой программную платформу (фреймворк) для решения целого класса игр для двух участников. Эти игры должны иметь следующие признаки:

1. Последовательный порядок ходов.
2. Совершенная информация об игровых событиях.
3. Детерминированная игра.
4. Фиксированные правила игры.
4. Некооперативная игра.
5. Дискретная.

Наследником идей AlphaZero считается шахматный движок Leela Chess Zero. Его модель оптимизирована только для игры в шахматы. Кроме этого в него добавили дебютную книгу и информацию об эндшпилях. Благодаря этим улучшениям, Leela Chess Zero смогла стать достойным конкурентом для традиционных шахматных движков наподобие Stockfish. Её успехи заставили разработчиков всех шахматных программ обратить внимание на технологию нейронных сетей.

{pagebreak}
